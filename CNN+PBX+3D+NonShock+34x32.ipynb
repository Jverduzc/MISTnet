{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1vHzMJfDhJ5I8it5pGiq4Y6W6BRhji6Pg",
      "authorship_tag": "ABX9TyM5p3pnZlht70tSoHKafns8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jverduzc/CNN_PBX_Model/blob/master/CNN%2BPBX%2B3D%2BNonShock%2B34x32.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Nd2jeT8r8uu"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import initializers\n",
        "from keras.layers import Input, Dropout, BatchNormalization, Conv3DTranspose, concatenate, Dense, Conv3D, Flatten, MaxPooling3D\n",
        "from keras.models import Sequential\n",
        "from sklearn.base import BaseEstimator\n",
        "import keras.backend as K\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "tf.keras.utils.set_random_seed(0)\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "import sys\n",
        "import os\n",
        "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
        "sys.path.insert(0, '../src/')\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TRAINING DATA\n",
        "\n",
        "paths = [\"PBX_PS14,7_Spherical_Oriented_Bottom.npy\", \"PBX_PS14,7_Spherical_Oriented_Top.npy\",\n",
        "         \"PBX_PS14,7_Spherical_Oriented_Bottom_Mirrored.npy\", \"PBX_PS14,7_Spherical_Oriented_Top_Mirrored.npy\",\n",
        "         \n",
        "         \"PBX_PS9,5_Spherical_Oriented_Bottom.npy\", \"PBX_PS9,5_Spherical_Oriented_Top.npy\",\n",
        "         \"PBX_PS9,5_Spherical_Oriented_Bottom_Mirrored.npy\", \"PBX_PS9,5_Spherical_Oriented_Top_Mirrored.npy\",\n",
        "\n",
        "         \"PBX_PS9,5_Spherical_Random_Bottom.npy\", \"PBX_PS9,5_Spherical_Random_Top.npy\",\n",
        "         \"PBX_PS9,5_Spherical_Random_Bottom_Mirrored.npy\", \"PBX_PS9,5_Spherical_Random_Top_Mirrored.npy\",\n",
        "\n",
        "         \"PBX_PS9,6_Faceted_Random_Bottom.npy\", \"PBX_PS9,6_Faceted_Random_Top.npy\",\n",
        "         \"PBX_PS9,6_Faceted_Random_Bottom_Mirrored.npy\", \"PBX_PS9,6_Faceted_Random_Top_Mirrored.npy\",\n",
        "\n",
        "         \"PBX_PS14,7_Spherical_Random_Bottom_Sim1.npy\", \"PBX_PS14,7_Spherical_Random_Top_Sim1.npy\",\n",
        "         \"PBX_PS14,7_Spherical_Random_Bottom_Mirrored_Sim1.npy\", \"PBX_PS14,7_Spherical_Random_Top_Mirrored_Sim1.npy\"\n",
        "]\n",
        "\n",
        "train_data = []\n",
        "train_labels = []\n",
        "\n",
        "for i in paths:\n",
        "  train_ex = np.load(\"inputs/Inputs_\" + i)\n",
        "  train_lb = np.load(\"temps/TemperatureMap_\" + i)\n",
        "  train_data.append(train_ex)\n",
        "  train_labels.append(train_lb)\n",
        "\n",
        "train_data = np.array(train_data)\n",
        "train_labels = np.array(train_labels)\n",
        "\n",
        "print(train_data.shape)\n",
        "print(train_labels.shape)"
      ],
      "metadata": {
        "id": "gCb5lRG1mDbI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# VALIDATION DATA\n",
        "\n",
        "validation_paths = [\n",
        "    \"PBX_PS14,7_Spherical_Random_Bottom_Sim2.npy\", \"PBX_PS14,7_Spherical_Random_Top_Sim2.npy\",\n",
        "    \"PBX_PS14,7_Spherical_Random_Bottom_Mirrored_Sim2.npy\", \"PBX_PS14,7_Spherical_Random_Top_Mirrored_Sim2.npy\"\n",
        "]\n",
        "\n",
        "validation_data = []\n",
        "validation_labels = []\n",
        "\n",
        "for i in validation_paths:\n",
        "  validation_ex= np.load(\"inputs/Inputs_\" + i)\n",
        "  validation_lb = np.load(\"temps/TemperatureMap_\" + i)\n",
        "  validation_data.append(validation_ex)\n",
        "  validation_labels.append(validation_lb)\n",
        "\n",
        "validation_data = np.array(validation_data)\n",
        "validation_labels = np.array(validation_labels)\n",
        "\n",
        "print(validation_data.shape)\n",
        "print(validation_labels.shape)"
      ],
      "metadata": {
        "id": "02ORxH_vZNOC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def periodic_padding_flexible(tensor, axis, padding=1):\n",
        "\n",
        "    if isinstance(axis,int):\n",
        "        axis = (axis,)\n",
        "    if isinstance(padding,int):\n",
        "        padding = (padding,)\n",
        "\n",
        "    ndim = len(tensor.shape)\n",
        "\n",
        "    for ax,p in zip(axis,padding):\n",
        "        # create a slice object that selects everything from all axes,\n",
        "        # except only 0:p for the specified for right, and -p: for left\n",
        "\n",
        "        ind_right = [slice(-p,None) if i == ax else slice(None) for i in range(ndim)]\n",
        "        ind_left = [slice(0, p) if i == ax else slice(None) for i in range(ndim)]\n",
        "        right = tensor[ind_right]\n",
        "        left = tensor[ind_left]\n",
        "        middle = tensor\n",
        "        tensor = tf.concat([right,middle,left], axis=ax)\n",
        "\n",
        "    return tensor\n"
      ],
      "metadata": {
        "id": "fWvLsqQXsWGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def DownConvBlock(inputs, n_filters=32, filter_size = 3, max_pooling=True, special_padding=False):\n",
        "\n",
        "  padding_size = int((filter_size-1)/2)\n",
        "  kernel_init =   tf.keras.initializers.GlorotUniform(seed=0)\n",
        "  bias_init = tf.keras.initializers.Zeros()\n",
        "\n",
        "\n",
        "  # PERIODIC PADDING\n",
        "\n",
        "  inputs = periodic_padding_flexible(inputs, axis=1,padding=padding_size)\n",
        "  if special_padding == False:\n",
        "    inputs = periodic_padding_flexible(inputs, axis=2,padding=padding_size)\n",
        "  inputs = periodic_padding_flexible(inputs, axis=3,padding=padding_size)\n",
        "\n",
        "  conv = Conv3D(n_filters, filter_size, activation='relu', padding='valid', kernel_initializer=kernel_init, bias_initializer=bias_init)(inputs)\n",
        "  print(conv.shape)\n",
        "  conv = periodic_padding_flexible(conv, axis=(1,2,3),padding=(padding_size,padding_size,padding_size))\n",
        "  conv = Conv3D(n_filters, filter_size, activation='relu', padding='valid', kernel_initializer=kernel_init, bias_initializer=bias_init)(conv)\n",
        "  print(conv.shape)\n",
        "  conv = BatchNormalization()(conv, training=False)\n",
        "      \n",
        "  if max_pooling:\n",
        "    next_layer = tf.keras.layers.MaxPooling3D(pool_size = (2,2,2))(conv)\n",
        "\n",
        "\n",
        "    # print(conv.shape)\n",
        "    # next_layer = tf.keras.layers.MaxPooling3D(pool_size = (2,2,2), strides=(1,1,1))(conv)\n",
        "    # print(next_layer.shape)\n",
        "    # for i in range(int(conv.shape[1]/2) - 1):\n",
        "    #   next_layer = tf.keras.layers.MaxPooling3D(pool_size = (2,2,2), strides=(1,1,1))(next_layer)\n",
        "    #   print(next_layer.shape)\n",
        "    # for i in range(int(conv.shape[2]/4)):\n",
        "    #   next_layer = tf.keras.layers.MaxPooling3D(pool_size = (1,2,2), strides=(1,1,1))(next_layer)\n",
        "    #   print(next_layer.shape)\n",
        "\n",
        "  else:\n",
        "    next_layer = conv\n",
        "  \n",
        "  skip_connection = conv   \n",
        "\n",
        "  print(\"end_of_block\") \n",
        "  return next_layer, skip_connection"
      ],
      "metadata": {
        "id": "rLhZ3NIocWJ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def UpConvBlock(prev_layer_input, skip_layer_input, filter_size = 3, n_filters=32):\n",
        "\n",
        "    padding_size = int((filter_size-1)/2)\n",
        "    kernel_init = tf.keras.initializers.GlorotUniform(seed=0)\n",
        "    bias_init = tf.keras.initializers.Zeros()   \n",
        "\n",
        "    up = Conv3DTranspose(n_filters, (filter_size,filter_size,filter_size),\n",
        "                         strides=(filter_size-1,filter_size-1,filter_size-1),\n",
        "                         padding='same', kernel_initializer=kernel_init, bias_initializer=bias_init)(prev_layer_input)\n",
        "\n",
        "    merge = concatenate([up, skip_layer_input], axis=4)\n",
        "    merge = periodic_padding_flexible(merge, axis=(1,2,3),padding=(padding_size,padding_size,padding_size))\n",
        "    conv = Conv3D(n_filters, filter_size, activation='relu', padding='valid', kernel_initializer=kernel_init,  bias_initializer=bias_init)(merge)\n",
        "    print(conv.shape)\n",
        "    conv = periodic_padding_flexible(conv, axis=(1,2,3),padding=(padding_size,padding_size,padding_size))\n",
        "    conv = Conv3D(n_filters, filter_size, activation='relu',padding='valid', kernel_initializer=kernel_init,  bias_initializer=bias_init)(conv)\n",
        "    print(conv.shape)\n",
        "\n",
        "    print(\"end_of_block\")\n",
        "    return conv"
      ],
      "metadata": {
        "id": "YvMNFlsHseOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def UNet3DModel(input_size=(16, 34, 32, 3), n_filters=32, filter_size=3, n_classes=1):\n",
        "  kernel_init =  tf.keras.initializers.GlorotUniform(seed=0)\n",
        "  bias_init = tf.keras.initializers.Zeros()  \n",
        "  \n",
        "  inputs = Input(input_size)\n",
        "  print(\"Inputs\", inputs.shape)\n",
        "\n",
        "  cblock0 = DownConvBlock(inputs,     n_filters = n_filters    , filter_size = filter_size, max_pooling=False, special_padding=True)\n",
        "  print(\"CB0\", cblock0[0].shape)\n",
        "\n",
        "  cblock1 = DownConvBlock(inputs,     n_filters = n_filters    , filter_size = filter_size, max_pooling=True, special_padding=True)\n",
        "  print(\"CB1\", cblock1[0].shape)\n",
        "\n",
        "  cblock2 = DownConvBlock(cblock1[0], n_filters = n_filters*2  , filter_size = filter_size, max_pooling=True, special_padding=False)\n",
        "  print(\"CB2\", cblock2[0].shape)\n",
        "    \n",
        "  cblock3 = DownConvBlock(cblock2[0], n_filters = n_filters*4  , filter_size = filter_size, max_pooling=True, special_padding=False)\n",
        "  print(\"CB3\", cblock3[0].shape)\n",
        "  \n",
        "  cblock4 = DownConvBlock(cblock3[0], n_filters = n_filters*8  , filter_size = filter_size, max_pooling=False, special_padding=False)\n",
        "  print(\"CB4\", cblock4[0].shape)\n",
        "\n",
        "\n",
        "  print(\"------------------\")\n",
        "\n",
        "  ublock7 = UpConvBlock(cblock4[0]   , cblock3[1],  n_filters = n_filters * 4, filter_size = filter_size)\n",
        "  print(\"UB7\", ublock7.shape)\n",
        "  \n",
        "  ublock8 = UpConvBlock(ublock7   , cblock2[1],  n_filters = n_filters * 2, filter_size = filter_size)\n",
        "  print(\"UB8\", ublock8.shape)\n",
        "  \n",
        "  ublock9 = UpConvBlock(ublock8   , cblock1[1],  n_filters = n_filters, filter_size = filter_size)\n",
        "  print(\"UB9\", ublock9.shape)\n",
        "\n",
        "  ublock9 = periodic_padding_flexible(ublock9, axis=(1,2,3),padding=(1,1,1))\n",
        "  \n",
        "  conv9 = Conv3D(n_filters, 3, activation='relu', padding='valid', kernel_initializer=kernel_init,  bias_initializer=bias_init)(ublock9)\n",
        "  print(\"C9\", conv9.shape)\n",
        "  \n",
        "  conv10 = Conv3D(n_classes, 1, padding='same', kernel_initializer=kernel_init,  bias_initializer=bias_init)(conv9)\n",
        "  print(\"C10\", conv10.shape)\n",
        "\n",
        "  model = tf.keras.Model(inputs=inputs, outputs=conv10)  \n",
        "\n",
        "  return model\n"
      ],
      "metadata": {
        "id": "fe-fUh99sgKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def custom_mse(y_true,y_pred):\n",
        "    w_hot = 5.0\n",
        "    w_cold = 1.0\n",
        "    cutoff = 1.8\n",
        "    weightmat = tf.cast(tf.where(tf.greater(y_true, cutoff), w_hot, w_cold),float)\n",
        "    loss = tf.cast(K.square(y_pred - y_true),float)\n",
        "    loss = loss*weightmat\n",
        "    loss = K.mean(loss)\n",
        "    return loss"
      ],
      "metadata": {
        "id": "d2a5FVUYPHTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = UNet3DModel(input_size=(16, 34, 32, 3), n_filters=32, filter_size = 3, n_classes=1)\n",
        "optimizer = tf.optimizers.Adam(learning_rate = 0.0005)\n",
        "model.compile(loss=custom_mse, optimizer=optimizer, metrics=['mse'])"
      ],
      "metadata": {
        "id": "R3SJtgKps6tc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model.summary()"
      ],
      "metadata": {
        "id": "n06aSBP25jGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#early = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\",min_delta=0.001, patience=200, verbose=0, mode=\"auto\", baseline=None, restore_best_weights=True)\n",
        "\n",
        "early = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\",min_delta=0.0005, patience=200, verbose=0, mode=\"auto\", baseline=None, restore_best_weights=True)\n",
        "\n",
        "\n",
        "def scheduler(epoch, lr):\n",
        "  if epoch == 2000:\n",
        "    return lr /5\n",
        "  else:\n",
        "    return lr\n",
        "\n",
        "\n",
        "scheduler_cb = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
        "\n",
        "\n",
        "history = model.fit(train_data, train_labels, epochs=4000, validation_data=(validation_data, validation_labels), callbacks=[early, scheduler_cb])"
      ],
      "metadata": {
        "id": "baIEkfpntuOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(history.history.keys())\n",
        "# # summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "#plt.ylim([0,0.2])\n",
        "plt.legend(['train', 'valid'], loc='upper right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UgCp0oNUAfzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def area_hotspot(datapoint):\n",
        "  tinc = 5\n",
        "  tnum = int(4500/tinc)\n",
        "\n",
        "  tinc = 5\n",
        "  tnum = int(4500/tinc)\n",
        "\n",
        "  bins=np.zeros((tnum,2))\n",
        "  bins[:,0] = [ tinc*(0.5 + x) for x in list(range(tnum))]\n",
        "\n",
        "  for bin_index, temp in np.ndenumerate(datapoint):\n",
        "    theta = temp * 1000\n",
        "    tind=math.floor(theta/tinc)\n",
        "    bins[:tind, 1] += 4 # ~ Roughly 2*2*1 nm^3 (volume value from Chunyu)\n",
        "  \n",
        "  return bins\n",
        "\n",
        "def plot_prediction(data_to_predict, labels_for_data):\n",
        "\n",
        "  maxval = max(np.max(data_to_predict), np.max(labels_for_data))\n",
        "  colorscale = [[0, 'rgba(0,0,0,0)'], [0.5, 'rgba(0,0,0,0)'], [1, 'rgb(255,0,0)']]\n",
        "\n",
        "  fig = make_subplots(rows=2, cols=3, specs=[[{\"type\": \"scatter3d\"}, {\"type\": \"histogram\"}, {\"type\": \"histogram\"}],[{\"type\": \"scatter3d\"}, {\"type\": \"scatter\"},{\"type\": \"scatter\"}]],\n",
        "    subplot_titles=['Prediction (Temperature)',  'Difference', 'Temp Distribution',  'Labels (Temperature)', 'Parity', 'Volume Plot'], horizontal_spacing = 0.15, vertical_spacing = 0.15)\n",
        "  \n",
        "\n",
        "  # FIRST PLOT --> PREDICTIONS\n",
        "\n",
        "  X,Y,Z = np.mgrid[0:data_to_predict.shape[2], 0:data_to_predict.shape[1], 0:data_to_predict.shape[0]]\n",
        "  data_to_predict_xz = np.swapaxes(data_to_predict, 2, 0)\n",
        "\n",
        "  trace_1 = go.Scatter3d(x = X.flatten(), y = Y.flatten(), z=Z.flatten(), hovertemplate = 'Bin: (%{x},%{y},%{z})<br>Temperature: %{marker.color:.2f}<extra></extra>',\n",
        "                          mode='markers', marker=dict(colorscale=colorscale, symbol='square', color = data_to_predict_xz.flatten(), line=dict(width=0.5, color='rgba(0,0,0,0.025)'),\n",
        "                                                      cmin=0, cmax=maxval, colorbar=dict(thickness=20, len=0.3, x = 0.27, y= 0.8)), showlegend=False)\n",
        "  fig.add_trace(trace_1, row=1, col=1)\n",
        "  ####\n",
        "\n",
        "  # SECOND PLOT --> LABELS\n",
        "\n",
        "  X_t,Y_t,Z_t = np.mgrid[0:labels_for_data.shape[2], 0:labels_for_data.shape[1], 0:labels_for_data.shape[0]]\n",
        "  labels_for_data_xz = np.swapaxes(labels_for_data, 2, 0)\n",
        "  trace_2 = go.Scatter3d(x = X_t.flatten(), y = Y_t.flatten(), z=Z_t.flatten(), hovertemplate = 'Bin: (%{x},%{y},%{z})<br>Temperature: %{marker.color:.2f}<extra></extra>',\n",
        "                          mode='markers', marker=dict(colorscale=colorscale, symbol='square', color = labels_for_data_xz.flatten(), line=dict(width=0.5, color='rgba(0,0,0,0.025)'),\n",
        "                                                      cmin=0, cmax=maxval, colorbar=dict(thickness=20,  len=0.3, x = 0.27, y= 0.3)),showlegend=False)\n",
        "  fig.add_trace(trace_2, row=2, col=1)\n",
        "  \n",
        "  ####\n",
        "\n",
        "  # TRIRD PLOT\n",
        "  diff_xz = data_to_predict_xz - labels_for_data_xz\n",
        "  flat_diff_xz = diff_xz.flatten()\n",
        "\n",
        "  trace_3 = go.Histogram(x=flat_diff_xz, showlegend=False)\n",
        "  trace_line = go.Scatter(x=[0,0], y = [0,700], mode='lines', showlegend=False)\n",
        "\n",
        "  fig.add_trace(trace_3, row=1, col=2)\n",
        "  fig.add_trace(trace_line, row=1, col=2)\n",
        "\n",
        "\n",
        "  # FOURTH PLOT\n",
        "  trace_4 = go.Scatter(x=labels_for_data_xz.flatten(), y = data_to_predict_xz.flatten(), mode='markers', showlegend=False)\n",
        "  trace_math = go.Scatter(x=[0,maxval], y=[0, maxval], mode='lines', showlegend=False)\n",
        "  fig.add_trace(trace_4, row=2, col=2)\n",
        "  fig.add_trace(trace_math, row=2,col=2)\n",
        "  fig.update_xaxes(title=\"Scaled Temperature (K) - Labels\", row=2, col=2)\n",
        "  fig.update_yaxes(title=\"Scaled Temperature (K) - Predictions\", row=2, col=2)\n",
        "\n",
        "  # FIFTH PLOT\n",
        "  trace_5 = go.Histogram(x=data_to_predict_xz.flatten(), name='Predictions', marker=dict(color='red'), showlegend=True)\n",
        "  trace_6 = go.Histogram(x=labels_for_data_xz.flatten(), name='Truth', marker=dict(color='green'), showlegend=True)\n",
        "  fig.update_xaxes(title=\"Temperature (K) - Labels\", row=1, col=3)\n",
        "  fig.add_trace(trace_5, row=1, col=3)\n",
        "  fig.add_trace(trace_6, row=1, col=3) \n",
        "\n",
        "  # SIXTH PLOT\n",
        "  \n",
        "  bins_labels = area_hotspot(labels_for_data)\n",
        "  bins_predictions = area_hotspot(data_to_predict)\n",
        "  trace_7 = go.Scatter(x=bins_predictions[:,1], y=bins_predictions[:,0], mode='lines', marker=dict(color='red'), showlegend=False)\n",
        "  trace_8 = go.Scatter(x=bins_labels[:,1], y=bins_labels[:,0], mode='lines', marker=dict(color='green'), showlegend=False)\n",
        "\n",
        "\n",
        "  fig.add_trace(trace_7, row=2, col=3)\n",
        "  fig.add_trace(trace_8, row=2, col=3) \n",
        "  fig.update_xaxes(type=\"log\", row=2, col=3)\n",
        "  fig.update_xaxes(title=\"Hotspot Volume (nm^3)\", row=2, col=3)\n",
        "  fig.update_yaxes(title=\"Temperature (K)\", row=2, col=3)  \n",
        "\n",
        "  fig.update_layout(autosize=False, width=1200, height=800, margin=dict(l=75, r=75, b=75, t=75), legend=dict(x=0.85, y=0.25))      \n",
        "  fig.show()"
      ],
      "metadata": {
        "id": "iTwnIzqkErW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### TRAINING DATA\n",
        "\n",
        "pred_training_data = model.predict(train_data)\n",
        "#print(pred_training_data.shape)\n",
        "\n",
        "labels_training_data = train_labels.copy()\n",
        "#print(labels_training_data.shape)\n"
      ],
      "metadata": {
        "id": "VhI-HUEWwIRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for i in [9]:#pred_training_data.shape[0]):\n",
        "  training_cube = pred_training_data[i].squeeze()\n",
        "  training_label = labels_training_data[i].squeeze()\n",
        "  plot_prediction(training_cube, training_label)\n",
        "  print(\" \")\n"
      ],
      "metadata": {
        "id": "VtaBemKOQbWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### VALIDATION DATA\n",
        "\n",
        "pred_validation_data = model.predict(validation_data)\n",
        "#print(pred_validation_data.shape)\n",
        "\n",
        "labels_validation_data = validation_labels.copy()\n",
        "#print(labels_validation_data.shape)\n"
      ],
      "metadata": {
        "id": "HQ7JU8A7d6lw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in [0]:#pred_validation_data.shape[0]):\n",
        "\n",
        "  validation_cube = pred_validation_data[i].squeeze()\n",
        "  validation_label = labels_validation_data[i].squeeze()\n",
        "  plot_prediction(validation_cube, validation_label)"
      ],
      "metadata": {
        "id": "QYEvIqocyL7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TESTING DATA\n",
        "\n",
        "test_paths = ['/content/drive/MyDrive/PBX/PBXdatasets_small/testPBX6/btm',\n",
        "              '/content/drive/MyDrive/PBX/PBXdatasets_small/testPBX6/top']\n",
        "\n",
        "test_data = []\n",
        "test_labels = []\n",
        "\n",
        "for i in test_paths:\n",
        "  test_ex= data_loader_inputs(i)\n",
        "  test_data.append(test_ex)\n",
        "\n",
        "test_data = np.array(test_data)\n",
        "print(test_data.shape)"
      ],
      "metadata": {
        "id": "e-KhOBQZfoAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_only_prediction(data_to_predict, title):\n",
        "\n",
        "\n",
        "  maxval = max([np.max(data_to_predict)])\n",
        "\n",
        "  fig = make_subplots(rows=1, cols=1, specs=[[{\"type\": \"scatter3d\"}]], subplot_titles=[title], horizontal_spacing = 0.15, vertical_spacing = 0.15)\n",
        "  \n",
        "\n",
        "  # FIRST PLOT --> PREDICTIONS\n",
        "\n",
        "  X,Y,Z = np.mgrid[0:data_to_predict.shape[2], 0:data_to_predict.shape[1], 0:data_to_predict.shape[0]]\n",
        "  data_to_predict_xz = np.swapaxes(data_to_predict, 2, 0)\n",
        "\n",
        "  trace_1 = go.Scatter3d(x = X.flatten(), y = Y.flatten(), z=Z.flatten(),\n",
        "                          mode='markers', marker=dict(colorscale='Viridis', symbol='square', color = data_to_predict_xz.flatten(),\n",
        "                                                      cmin=0, cmax=maxval, colorbar=dict(thickness=20)), showlegend=False)\n",
        "  fig.add_trace(trace_1, row=1, col=1)\n",
        "  #### \n",
        "\n",
        "  fig.update_layout(autosize=False, width=800, height=600, legend=dict(x=0.85, y=0.25))      \n",
        "  fig.show()"
      ],
      "metadata": {
        "id": "GtjH1Pxf2-EI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_testing_data = model.predict(test_data)\n",
        "#print(pred_testing_data.shape)\n",
        "\n",
        "\n",
        "for i in [0]:#pred_testing_data.shape[0]):\n",
        "\n",
        "  testing_cube = pred_testing_data[i].squeeze()\n",
        "  plot_only_prediction(testing_cube, 'Prediction (Temperature)')"
      ],
      "metadata": {
        "id": "Px40Hid33dEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in [0]:#pred_testing_data.shape[0]):\n",
        "\n",
        "  input1_cube = test_data[i,:,:,:,0].squeeze()\n",
        "  plot_only_prediction(input1_cube, 'HE Density')\n",
        "\n",
        "  input2_cube = test_data[i,:,:,:,1].squeeze()\n",
        "  plot_only_prediction(input2_cube, 'Total Density')\n",
        "\n",
        "  input3_cube = test_data[i,:,:,:,2].squeeze()\n",
        "  plot_only_prediction(input3_cube, 'GB Interface')"
      ],
      "metadata": {
        "id": "-BR8XhSM4F3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KhHN27p35vWl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}