{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jverduzc/CNN_PBX_Model/blob/master/CNN_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convolutional Neural Networks (CNN) for prediction of hotspots on PBX\n",
        "\n",
        "In this notebook we create a CNN model with a U-Net architecture for the prediction of temperature for a plastically bonded explosive molecular dynamics simulation."
      ],
      "metadata": {
        "id": "4QfRMy9VFrcW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Libraries\n",
        "\n",
        "First, we need to set up libraries that are not part of the default environment in Google Colab. In our case, this is only the rendering library ```kaleido```.\n",
        "\n",
        "You will need to restart the runtime to update the kernel.\n",
        "- Menu Runtime -> Restart Runtime"
      ],
      "metadata": {
        "id": "C2UgThuCGG3F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaleido"
      ],
      "metadata": {
        "id": "M79B0dH7ftZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will then import the required libraries for our notebook to run. The following cells import:\n",
        "- Standard python libraries\n",
        "- Plotting libraries\n",
        "- Machine learning libraries (tensorflow / keras)"
      ],
      "metadata": {
        "id": "2ECMQghLGzUA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Nd2jeT8r8uu"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import copy\n",
        "import math\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import clear_output\n",
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()"
      ],
      "metadata": {
        "id": "qrgVqntVG6YP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V08TFba6G_nw"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import initializers\n",
        "from keras.layers import Input, Dropout, BatchNormalization, Conv3DTranspose, concatenate, Dense, Conv3D, Flatten, MaxPooling3D\n",
        "from keras.models import Sequential\n",
        "import keras.backend as K\n",
        "tf.keras.utils.set_random_seed(0)\n",
        "\n",
        "from sklearn.cluster import AgglomerativeClustering"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to verify that we are running an enviroment with a GPU in Colab. The next cell shows if you have a GPU front-end execution host allocated to the notebook. \n",
        "\n",
        "<font color='red'><b>Warning:</b></font> If you don't have one, you'll need to re-run the previous cells after going to:\n",
        "- Menu Runtime -> Change runtime type -> Hardware accelerator"
      ],
      "metadata": {
        "id": "Wh5PkIn3H9dD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KnH0b5QqG_ny"
      },
      "outputs": [],
      "source": [
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data\n",
        "\n",
        "After setting up the environment, we need the repository files to create the model and access the training/validation data. \n",
        "\n",
        "<font color='orange'><b>Attention:</b></font> This is not trivial in Colab, but to access a private repository on Github like this, you need to provide Colab with your Github Key. You can get that key here: https://github.com/settings/tokens\n",
        "\n",
        "After that, you need to execute a command with this syntax:\n",
        "```\n",
        "!git clone https://username:github_key@github.com/Jverduzc/CNN_PBX_Model.git\n",
        "```\n",
        "You can fill up the blanks in the following cell. We will also change the current working directory. You should see the a new directory in your directory tree (on your left) with the files on the repository.\n"
      ],
      "metadata": {
        "id": "hYseV3r9LkME"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone XXX\n",
        "os.chdir(\"CNN_PBX_Model\")"
      ],
      "metadata": {
        "id": "Cr1dF6TJHU2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training and validation data\n",
        "\n",
        "This notebook was designed to read everything in the ```/train/``` folder as training data and everything in the ```/validation/``` folder as validation data. These directories contain individual labeled subdirectories that represent each of our simulation systems (data points).\n",
        "\n",
        "In each of the simulation systems subdirectories there are two files as numpy arrays: ```input.npy``` and ```output.npy```.\n",
        "\n",
        "\n",
        "```input.npy``` contains a (16 x 34 x 34 x 3) array with the input mappings from our simulations. The first three numbers represent the dimensions (in bins) of our system. The last number represents the number of mappings for our inputs. For each of our 3D structures, we generate the following:\n",
        "- Total density\n",
        "- HE density\n",
        "- GB interface parameter\n",
        "\n",
        "```output.npy``` contains a (16 x 32 x 32 x 1) array with the output mapping (temperature) from our simulations. The first three numbers represent the dimensions (in bins) of our system, but note that they are different from the inputs due to periodic boundary conditions. The last number represents the temperature mapping. \n",
        "- Temperature\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "B2wbmJTjOpoe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please verify that after running the next cells you get the following results:\n",
        "\n",
        "<b>Training data:</b><br>\n",
        "\n",
        "(64, 16, 34, 34, 3) <br>\n",
        "(64, 16, 32, 32, 1) <br>\n",
        "\n",
        "<b>Validation data:</b><br>\n",
        "\n",
        "(28, 16, 34, 34, 3) <br>\n",
        "(28, 16, 32, 32, 1) <br>\n",
        "\n",
        "You can see that there is a new number in these arrays. It indicates the number of datapoints in each set. You can read this as having 64 points with (16 x 34 x 34 x 3) shape."
      ],
      "metadata": {
        "id": "ogxA0zxHRUIl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZxqA1mmG_nz"
      },
      "outputs": [],
      "source": [
        "# TRAINING DATA\n",
        "\n",
        "paths = [x[0] for x in os.walk('train/')][1:]\n",
        "\n",
        "train_data = []\n",
        "train_labels = []\n",
        "\n",
        "for i in paths:\n",
        "  train_ex = np.load(i + \"/input.npy\")\n",
        "\n",
        "  if train_ex.shape != (16,34,34,3):\n",
        "\n",
        "    first_axis_pad = int(0.5 * (34 - train_ex.shape[1]))\n",
        "    second_axis_pad = int(0.5 * (34 - train_ex.shape[2]))\n",
        "\n",
        "    train_ex = np.pad(train_ex, ((0, 0), (first_axis_pad, first_axis_pad), (second_axis_pad, second_axis_pad), (0,0)), 'wrap')\n",
        "\n",
        "  train_lb = np.load(i + \"/output.npy\")\n",
        "  train_data.append(train_ex)\n",
        "  train_labels.append(train_lb)\n",
        "\n",
        "train_data = np.array(train_data)\n",
        "train_labels = np.array(train_labels) / 1000\n",
        "\n",
        "print(train_data.shape)\n",
        "print(train_labels.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02ORxH_vZNOC"
      },
      "outputs": [],
      "source": [
        "# VALIDATION DATA\n",
        "\n",
        "validation_paths = [x[0] for x in os.walk('validation/')][1:]\n",
        "\n",
        "validation_data = []\n",
        "validation_labels = []\n",
        "\n",
        "for i in validation_paths:\n",
        "  validation_ex= np.load(i + \"/input.npy\")\n",
        "\n",
        "  if validation_ex.shape != (16,34,34,1):\n",
        "\n",
        "    first_axis_pad = int(0.5 * (34 - validation_ex.shape[1]))\n",
        "    second_axis_pad = int(0.5 * (34 - validation_ex.shape[2]))\n",
        "\n",
        "    validation_ex = np.pad(validation_ex, ((0, 0), (first_axis_pad, first_axis_pad), (second_axis_pad, second_axis_pad), (0,0)), 'wrap')\n",
        "\n",
        "\n",
        "  validation_lb = np.load(i + \"/output.npy\")\n",
        "  validation_data.append(validation_ex)\n",
        "  validation_labels.append(validation_lb)\n",
        "\n",
        "validation_data = np.array(validation_data)\n",
        "validation_labels = np.array(validation_labels) / 1000\n",
        "\n",
        "print(validation_data.shape)\n",
        "print(validation_labels.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model\n",
        "\n",
        "This notebook creates an architecture based on U-Net, a CNN algorithm for image segmentation. We will go a bit into the design of the architecture in the following cells.\n",
        "\n",
        "This first function addresses periodic padding for tensors in the model."
      ],
      "metadata": {
        "id": "xQrUeLx5R1mp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fWvLsqQXsWGy"
      },
      "outputs": [],
      "source": [
        "# Code taken from: https://stackoverflow.com/questions/39088489/tensorflow-periodic-padding\n",
        "\n",
        "def periodic_padding_flexible(tensor, axis, padding=1):\n",
        "\n",
        "    if isinstance(axis,int):\n",
        "        axis = (axis,)\n",
        "    if isinstance(padding,int):\n",
        "        padding = (padding,)\n",
        "\n",
        "    ndim = len(tensor.shape)\n",
        "\n",
        "    for ax,p in zip(axis,padding):\n",
        "        # create a slice object that selects everything from all axes,\n",
        "        # except only 0:p for the specified for right, and -p: for left\n",
        "\n",
        "        ind_right = [slice(-p,None) if i == ax else slice(None) for i in range(ndim)]\n",
        "        ind_left = [slice(0, p) if i == ax else slice(None) for i in range(ndim)]\n",
        "        right = tensor[ind_right]\n",
        "        left = tensor[ind_left]\n",
        "        middle = tensor\n",
        "        tensor = tf.concat([right,middle,left], axis=ax)\n",
        "\n",
        "    return tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since this architecture is going to be complicated, we will start by creating mini-blocks. This first function ```DownConvBlock``` executes the following operations sequentially:\n",
        "\n",
        "- (Optional) Periodic Padding\n",
        "- 3D Convolutional Layer\n",
        "- Periodic Padding\n",
        "- 3D Convolutional Layer\n",
        "- Batch normalization\n",
        "- (Optional) MaxPooling3D Layer"
      ],
      "metadata": {
        "id": "i2sWbazsagHg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rLhZ3NIocWJ9"
      },
      "outputs": [],
      "source": [
        "def DownConvBlock(inputs, n_filters=32, filter_size = 3, max_pooling=True, special_padding=False):\n",
        "\n",
        "  padding_size = int((filter_size-1)/2)\n",
        "  kernel_init =   tf.keras.initializers.GlorotUniform(seed=0)\n",
        "  bias_init = tf.keras.initializers.Zeros()\n",
        "\n",
        "\n",
        "  # PERIODIC PADDING\n",
        "\n",
        "  inputs = periodic_padding_flexible(inputs, axis=1,padding=padding_size)\n",
        "  if special_padding == False:\n",
        "    inputs = periodic_padding_flexible(inputs, axis=2,padding=padding_size)\n",
        "    inputs = periodic_padding_flexible(inputs, axis=3,padding=padding_size)\n",
        "  \n",
        "  conv = Conv3D(n_filters, filter_size, activation='relu', padding='valid', kernel_initializer=kernel_init, bias_initializer=bias_init)(inputs)\n",
        "  print(conv.shape)\n",
        "  conv = periodic_padding_flexible(conv, axis=(1,2,3),padding=(padding_size,padding_size,padding_size))\n",
        "  conv = Conv3D(n_filters, filter_size, activation='relu', padding='valid', kernel_initializer=kernel_init, bias_initializer=bias_init)(conv)\n",
        "  print(conv.shape)\n",
        "  conv = BatchNormalization()(conv, training=False)\n",
        "      \n",
        "  if max_pooling:\n",
        "    next_layer = tf.keras.layers.MaxPooling3D(pool_size = (2,2,2))(conv)\n",
        "  else:\n",
        "    next_layer = conv\n",
        "  \n",
        "  skip_connection = conv   \n",
        "\n",
        "  print(\"end_of_block\") \n",
        "  return next_layer, skip_connection"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This second function ```UpConvBlock``` executes the following operations sequentially:\n",
        "\n",
        "- Transpose 3D Convolutional Layer\n",
        "- Merge with <i>skip connection</i> from the corresponding ```DownConvBlock```\n",
        "- Periodic Padding\n",
        "- 3D Convolutional Layer\n",
        "- Periodic Padding\n",
        "- 3D Convolutional Layer"
      ],
      "metadata": {
        "id": "i8h2MOJHa_Aq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YvMNFlsHseOF"
      },
      "outputs": [],
      "source": [
        "def UpConvBlock(prev_layer_input, skip_layer_input, filter_size = 3, n_filters=32):\n",
        "\n",
        "    padding_size = int((filter_size-1)/2)\n",
        "    kernel_init = tf.keras.initializers.GlorotUniform(seed=0)\n",
        "    bias_init = tf.keras.initializers.Zeros()   \n",
        "\n",
        "    up = Conv3DTranspose(n_filters, (filter_size,filter_size,filter_size),\n",
        "                         strides=(filter_size-1,filter_size-1,filter_size-1),\n",
        "                         padding='same', kernel_initializer=kernel_init, bias_initializer=bias_init)(prev_layer_input)\n",
        "\n",
        "    merge = concatenate([up, skip_layer_input], axis=4)\n",
        "    merge = periodic_padding_flexible(merge, axis=(1,2,3),padding=(padding_size,padding_size,padding_size))\n",
        "    conv = Conv3D(n_filters, filter_size, activation='relu', padding='valid', kernel_initializer=kernel_init,  bias_initializer=bias_init)(merge)\n",
        "    print(conv.shape)\n",
        "    conv = periodic_padding_flexible(conv, axis=(1,2,3),padding=(padding_size,padding_size,padding_size))\n",
        "    conv = Conv3D(n_filters, filter_size, activation='relu',padding='valid', kernel_initializer=kernel_init,  bias_initializer=bias_init)(conv)\n",
        "    print(conv.shape)\n",
        "\n",
        "    print(\"end_of_block\")\n",
        "    return conv"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The architecture will be composed of several ```DownConvBlock``` blocks followed by the same number of ```UpConvBlock``` blocks. "
      ],
      "metadata": {
        "id": "evWdWAkXbzfq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fe-fUh99sgKg"
      },
      "outputs": [],
      "source": [
        "\n",
        "def UNet3DModel(input_size=(16, 34, 34, 3), n_filters=32, filter_size=3, n_classes=1):\n",
        "  kernel_init =  tf.keras.initializers.GlorotUniform(seed=0)\n",
        "  bias_init = tf.keras.initializers.Zeros()  \n",
        "  \n",
        "  inputs = Input(input_size)\n",
        "  print(\"Inputs\", inputs.shape)\n",
        "\n",
        "  cblock0 = DownConvBlock(inputs,     n_filters = n_filters    , filter_size = filter_size, max_pooling=False, special_padding=True)\n",
        "  print(\"CB0\", cblock0[0].shape)\n",
        "\n",
        "  cblock1 = DownConvBlock(cblock0[0],     n_filters = n_filters    , filter_size = filter_size, max_pooling=True, special_padding=False)\n",
        "  print(\"CB1\", cblock1[0].shape)\n",
        "\n",
        "  cblock2 = DownConvBlock(cblock1[0], n_filters = n_filters*2  , filter_size = filter_size, max_pooling=True, special_padding=False)\n",
        "  print(\"CB2\", cblock2[0].shape)\n",
        "    \n",
        "  cblock3 = DownConvBlock(cblock2[0], n_filters = n_filters*4  , filter_size = filter_size, max_pooling=True, special_padding=False)\n",
        "  print(\"CB3\", cblock3[0].shape)\n",
        "  \n",
        "  cblock4 = DownConvBlock(cblock3[0], n_filters = n_filters*8  , filter_size = filter_size, max_pooling=False, special_padding=False)\n",
        "  print(\"CB4\", cblock4[0].shape)\n",
        "\n",
        "\n",
        "  print(\"------------------\")\n",
        "\n",
        "  ublock7 = UpConvBlock(cblock4[0]   , cblock3[1],  n_filters = n_filters * 4, filter_size = filter_size)\n",
        "  print(\"UB7\", ublock7.shape)\n",
        "  \n",
        "  ublock8 = UpConvBlock(ublock7   , cblock2[1],  n_filters = n_filters * 2, filter_size = filter_size)\n",
        "  print(\"UB8\", ublock8.shape)\n",
        "  \n",
        "  ublock9 = UpConvBlock(ublock8   , cblock1[1],  n_filters = n_filters, filter_size = filter_size)\n",
        "  print(\"UB9\", ublock9.shape)\n",
        "\n",
        "  ublock9 = periodic_padding_flexible(ublock9, axis=(1,2,3),padding=(1,1,1))\n",
        "  \n",
        "  conv9 = Conv3D(n_filters, 3, activation='relu', padding='valid', kernel_initializer=kernel_init,  bias_initializer=bias_init)(ublock9)\n",
        "  print(\"C9\", conv9.shape)\n",
        "  \n",
        "  conv10 = Conv3D(n_classes, 1, padding='same', kernel_initializer=kernel_init,  bias_initializer=bias_init)(conv9)\n",
        "  print(\"C10\", conv10.shape)\n",
        "\n",
        "  model = tf.keras.Model(inputs=inputs, outputs=conv10)  \n",
        "\n",
        "  return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this application, we are concerned with the detection and accurate prediction of hotspots (areas with higher temperatures), which are significantly less common than the rest of the material at a lower temperature. To address this, we are using a custon loss function based on a weighted Mean Squared Error (MSE)."
      ],
      "metadata": {
        "id": "vUDnv4nLeTQH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2a5FVUYPHTB"
      },
      "outputs": [],
      "source": [
        "def custom_mse(y_true,y_pred):\n",
        "    w_hot = 5.0\n",
        "    w_cold = 1.0\n",
        "    cutoff = 1.8\n",
        "    weightmat = tf.cast(tf.where(tf.greater(y_true, cutoff), w_hot, w_cold),float)\n",
        "    loss = tf.cast(K.square(y_pred - y_true),float)\n",
        "    loss = loss*weightmat\n",
        "    loss = K.mean(loss)\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, this last cell calls the function to generate the model, pair it with an optimizer and compile the model object."
      ],
      "metadata": {
        "id": "-uvZjSSeekHu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R3SJtgKps6tc"
      },
      "outputs": [],
      "source": [
        "model = UNet3DModel(input_size=(16, 34, 34, 3), n_filters=32, filter_size = 3, n_classes=1)\n",
        "optimizer = tf.optimizers.Adam(learning_rate = 0.0005)\n",
        "model.compile(loss=custom_mse, optimizer=optimizer, metrics=['mse'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the model\n",
        "\n",
        "This cell implements two techniques to prevent overfitting. The first one is a learning rate scheduler that decreases the learning rate after a fixed number of epochs. The second one is an early stopping criteria that monitors the validation loss to ensure the model continues to learn."
      ],
      "metadata": {
        "id": "BLFb04QIevl2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "baIEkfpntuOW"
      },
      "outputs": [],
      "source": [
        "# Learning Rate Scheduler\n",
        "def scheduler(epoch, lr):\n",
        "  if epoch == 500:\n",
        "    return lr /5\n",
        "  else:\n",
        "    return lr\n",
        "\n",
        "scheduler_cb = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
        "\n",
        "# Early stopping criteria\n",
        "early = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\",min_delta=0.0005, patience=200, verbose=0, mode=\"auto\", baseline=None, restore_best_weights=True)\n",
        "\n",
        "# Training (Fit)\n",
        "history = model.fit(train_data, train_labels, epochs=1000, validation_data=(validation_data, validation_labels), callbacks=[early, scheduler_cb])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This next plot shows loss of the trianing and validation sets."
      ],
      "metadata": {
        "id": "xIGiYmh3fwP2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UgCp0oNUAfzt"
      },
      "outputs": [],
      "source": [
        "print(history.history.keys())\n",
        "# # summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'valid'], loc='upper right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plotting the results\n",
        "\n",
        "After our training, we need to plot our results to verify if we are making a good prediction since the value of the loss can be heavily influenced by outliers. The following cell includes nine plots we use to evaluate performance on each of the points."
      ],
      "metadata": {
        "id": "Hb4C5FSDf-2n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for the volume plot of hotspot\n",
        "\n",
        "def area_hotspot(datapoint):\n",
        "  tinc = 5\n",
        "  tnum = int(4500/tinc)\n",
        "\n",
        "  tinc = 5\n",
        "  tnum = int(4500/tinc)\n",
        "\n",
        "  bins=np.zeros((tnum,2))\n",
        "  bins[:,0] = [ tinc*(0.5 + x) for x in list(range(tnum))]\n",
        "\n",
        "  for bin_index, temp in np.ndenumerate(datapoint):\n",
        "    theta = temp * 1000\n",
        "    tind=math.floor(theta/tinc)\n",
        "    bins[:tind, 1] += 4 # ~ Roughly 2*2*1 nm^3 (volume value from Chunyu)\n",
        "  \n",
        "  return bins"
      ],
      "metadata": {
        "id": "cVbLrXkBNWDI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def datapoint_results_print(simulation_point, simulation_temperatures, prediction_tensor):\n",
        "\n",
        "  # Simulation point is the input train_data[<point order>,<dim 0>,<dim 1>,<dim 2>,<channel>]\n",
        "  # For example, simulation_point = train_data[i,:,:,:,:]\n",
        "\n",
        "\n",
        "\n",
        "  fig = make_subplots(rows=3, cols=3, specs=[[{\"type\": \"scatter3d\"},{\"type\": \"scatter3d\"},{\"type\": \"scatter3d\"}],\n",
        "                                             [{\"type\": \"scatter3d\"},{\"type\": \"scatter3d\"},{\"type\": \"scatter\"}],\n",
        "                                             [{\"type\": \"histogram\"},{\"type\": \"histogram\"},{\"type\": \"scatter\"}]],\n",
        "                      subplot_titles=[\"Input 1\",\"Input 2\",\"Input 3\",\n",
        "                                      \"Temp (Labels)\",\"Temp (Predictions)\",\"Parity Plot\",\n",
        "                                      \"Temp (Distributions)\", \"Residuals\", \"Hotspot volume\"], horizontal_spacing = 0.1, vertical_spacing = 0.1)\n",
        "  \n",
        "  fig.update_layout(autosize=False, width=800, height=800) \n",
        "\n",
        "  # FIRST PLOT --> INPUT 1\n",
        "\n",
        "  input_1 = simulation_point[:,:,:,0].squeeze()\n",
        "\n",
        "  X,Y,Z = np.mgrid[0:input_1.shape[0], 0:input_1.shape[1], 0:input_1.shape[2]]\n",
        "  #input_1_xz = np.swapaxes(input_1, 2, 0)\n",
        "\n",
        "  trace_1 = go.Scatter3d(x = X.flatten(), y = Y.flatten(), z=Z.flatten(),\n",
        "                          mode='markers', marker=dict(colorscale='Viridis', size=8, line=dict(width=0), symbol='square', color = input_1.flatten(), colorbar=dict(thickness=20, len=0.3, x=0.27, y=0.9)), showlegend=False)\n",
        "  fig.add_trace(trace_1, row=1, col=1)\n",
        "\n",
        "  # SECOND PLOT --> INPUT 2\n",
        "\n",
        "  input_2 = simulation_point[:,:,:,1].squeeze()\n",
        "\n",
        "  X,Y,Z = np.mgrid[0:input_2.shape[0], 0:input_2.shape[1], 0:input_2.shape[2]]\n",
        "  #input_2_xz = np.swapaxes(input_2, 2, 0)\n",
        "\n",
        "  trace_2 = go.Scatter3d(x = X.flatten(), y = Y.flatten(), z=Z.flatten(),\n",
        "                          mode='markers', marker=dict(colorscale='Viridis', size=8, line=dict(width=0), symbol='square', color = input_2.flatten(), colorbar=dict(thickness=20, len=0.3, x=0.62, y=0.9)), showlegend=False)\n",
        "  fig.add_trace(trace_2, row=1, col=2)\n",
        "\n",
        "\n",
        "  # THIRD PLOT --> INPUT 3\n",
        "\n",
        "  input_3 = simulation_point[:,:,:,2].squeeze()\n",
        "\n",
        "  X,Y,Z = np.mgrid[0:input_3.shape[0], 0:input_3.shape[1], 0:input_3.shape[2]]\n",
        "  #input_3_xz = np.swapaxes(input_3, 2, 0)\n",
        "\n",
        "  trace_3 = go.Scatter3d(x = X.flatten(), y = Y.flatten(), z=Z.flatten(),\n",
        "                          mode='markers', marker=dict(colorscale='Viridis', size=8, line=dict(width=0), symbol='square', color = input_3.flatten(), colorbar=dict(thickness=20, len=0.3, x=1, y=0.9)), showlegend=False)\n",
        "  fig.add_trace(trace_3, row=1, col=3)\n",
        "\n",
        "\n",
        "  # FOURTH PLOT --> TEMPS (LABELS)\n",
        "\n",
        "  maxval = max(np.max(prediction_tensor), np.max(simulation_temperatures))\n",
        "  \n",
        "\n",
        "  X,Y,Z = np.mgrid[0:simulation_temperatures.shape[0], 0:simulation_temperatures.shape[1], 0:simulation_temperatures.shape[2]]\n",
        "  #simulation_temperatures_xz = np.swapaxes(simulation_temperatures, 2, 0)\n",
        "\n",
        "  trace_4 = go.Scatter3d(x = X.flatten(), y = Y.flatten(), z=Z.flatten(),\n",
        "                          mode='markers', marker=dict(cmin=0, cmax=maxval, colorscale='Reds', size=8, line=dict(width=0), symbol='square', color = simulation_temperatures.flatten(), colorbar=dict(thickness=20,len=0.3, x=0.27, y=0.5)), showlegend=False)\n",
        "  fig.add_trace(trace_4, row=2, col=1)\n",
        "\n",
        "\n",
        "  # FIFTH PLOT --> TEMPS (PREDICTIONS)\n",
        "\n",
        "  X,Y,Z = np.mgrid[0:prediction_tensor.shape[0], 0:prediction_tensor.shape[1], 0:prediction_tensor.shape[2]]\n",
        "  #prediction_tensor_xz = np.swapaxes(prediction_tensor, 2, 0)\n",
        "\n",
        "  trace_5 = go.Scatter3d(x = X.flatten(), y = Y.flatten(), z=Z.flatten(),\n",
        "                          mode='markers', marker=dict(cmin=0, cmax=maxval, colorscale='Reds', size=8, line=dict(width=0), symbol='square', color = prediction_tensor.flatten(), colorbar=dict(thickness=20, len=0.3, x=0.62, y=0.5)), showlegend=False)\n",
        "  fig.add_trace(trace_5, row=2, col=2)\n",
        "\n",
        "  # SIXTH PLOT --> PARITY PLOT\n",
        "\n",
        "  trace_6 = go.Scatter(x=simulation_temperatures.flatten(), y = prediction_tensor.flatten(), mode='markers', showlegend=False)\n",
        "  fig.add_trace(trace_6, row=2, col=3)\n",
        "  fig.update_xaxes(title=\"Scaled Temperature (K) - Labels\", row=2, col=3)\n",
        "  fig.update_yaxes(title=\"Scaled Temperature (K) - Predictions\", row=2, col=3)\n",
        "\n",
        "\n",
        "  # SEVENTH PLOT --> TEMP (Distributions)\n",
        "\n",
        "  trace_7 = go.Histogram(x=prediction_tensor.flatten(), name='Predictions', marker=dict(color='red'), showlegend=True, legendgroup = '1')\n",
        "  trace_7b = go.Histogram(x=simulation_temperatures.flatten(), name='Truth', marker=dict(color='green'), showlegend=True, legendgroup = '1')\n",
        "  fig.update_xaxes(title=\"Temperature (K) - Labels\", row=3, col=1)\n",
        "  fig.add_trace(trace_7, row=3, col=1)\n",
        "  fig.add_trace(trace_7b, row=3, col=1) \n",
        "\n",
        "\n",
        "  # EIGHTH PLOT --> RESIDUALS\n",
        "\n",
        "  diff_xz = prediction_tensor - simulation_temperatures\n",
        "  flat_diff_xz = diff_xz.flatten()\n",
        "\n",
        "  trace_8 = go.Histogram(x=flat_diff_xz, showlegend=False)\n",
        "  trace_line = go.Scatter(x=[0,0], y = [0,700], mode='lines', showlegend=False)\n",
        "\n",
        "  fig.add_trace(trace_8, row=3, col=2)\n",
        "  fig.add_trace(trace_line, row=3, col=2)\n",
        "\n",
        "  # NINTH PLOT ---> VOLUME OF HOTSPOT\n",
        "  \n",
        "  bins_labels = area_hotspot(simulation_temperatures)\n",
        "  bins_predictions = area_hotspot(prediction_tensor)\n",
        "  trace_9 = go.Scatter(x=bins_predictions[:,1], y=bins_predictions[:,0], mode='lines',  name='Predictions', marker=dict(color='red'), showlegend=True, legendgroup = '2')\n",
        "  trace_9b = go.Scatter(x=bins_labels[:,1], y=bins_labels[:,0], mode='lines', name='Truth', marker=dict(color='green'), showlegend=True, legendgroup = '2')\n",
        "  \n",
        "  fig.add_trace(trace_9, row=3, col=3)\n",
        "  fig.add_trace(trace_9b, row=3, col=3) \n",
        "  fig.update_xaxes(type=\"log\", row=3, col=3)\n",
        "  fig.update_xaxes(title=\"Hotspot Volume (nm^3)\", row=3, col=3)\n",
        "  fig.update_yaxes(title=\"Temperature (K)\", row=3, col=3)  \n",
        "\n",
        "  #### \n",
        "\n",
        "  fig.update_layout(autosize=False, width=1400, height=1000, legend_tracegroupgap = 180, legend=dict(font=dict(size=16),orientation=\"h\"))   \n",
        "\n",
        "  return fig"
      ],
      "metadata": {
        "id": "p65_1oRxgTu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since rendering multiple plots for each system is not ideal, we will save a numpy array with the model predictions and an HTML file with the interactive plots for exploration."
      ],
      "metadata": {
        "id": "0Jx7VbNCihTX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p results/train\n",
        "!mkdir -p results/validation"
      ],
      "metadata": {
        "id": "zVpCWbXshU4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PREDICTIONS OVER TRAINING DATA\n",
        "\n",
        "for i, title in enumerate(paths):\n",
        "  print(i)\n",
        "  inppoint = train_data[i,:,:,:,:].squeeze()\n",
        "  inplabel = train_labels[i,:,:,:,:].squeeze()\n",
        "\n",
        "  prediction_point = model.predict(tf.expand_dims(inppoint, axis=0))\n",
        "  prediction_tensor = prediction_point.squeeze()\n",
        "  #print(prediction_tensor.shape)\n",
        "\n",
        "  inppoint = inppoint[:, 1:-1, 1:-1] # NON PERIODIC GETS PLOTTED\n",
        "\n",
        "  fig = datapoint_results_print(inppoint, inplabel, prediction_tensor)\n",
        "\n",
        "  os.mkdir('results/'+str(title))\n",
        "\n",
        "  ### SAVING NUMPY PREDICTIONS\n",
        "  np.save('results/'+str(title)+'/prediction.npy', prediction_tensor)\n",
        "  ### SAVING HTML IMAGES\n",
        "  fig.write_html('results/'+str(title)+'/visualization.html')"
      ],
      "metadata": {
        "id": "nTuoEr8Khjpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PREDICTIONS OVER VALIDATION DATA\n",
        "\n",
        "for i, title in enumerate(validation_paths):\n",
        "  print(i)\n",
        "  inppoint = validation_data[i,:,:,:,:].squeeze()\n",
        "  inplabel = validation_labels[i,:,:,:,:].squeeze()\n",
        "\n",
        "  prediction_point = model.predict(tf.expand_dims(inppoint, axis=0))\n",
        "  prediction_tensor = prediction_point.squeeze()\n",
        "\n",
        "  inppoint = inppoint[:, 1:-1, 1:-1] # NON PERIODIC GETS PLOTTED\n",
        "  fig = datapoint_results_print(inppoint, inplabel, prediction_tensor)\n",
        "  \n",
        "  os.mkdir('results/'+str(title))\n",
        "\n",
        "  ### SAVING NUMPY PREDICTIONS\n",
        "  np.save('results/'+str(title)+'/prediction.npy', prediction_tensor)\n",
        "  ### SAVING HTML IMAGES\n",
        "  fig.write_html('results/'+str(title)+'/visualization.html')"
      ],
      "metadata": {
        "id": "nYhrkn0_iK52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cluster Analysis \n",
        "In the following cells we perform cluster analysis to compare hotspots produced by the MD simulation and the predictions made by the CNN. "
      ],
      "metadata": {
        "id": "yxkN6Bos4wrU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_clusters(temp_grid):\n",
        "  x = []\n",
        "  y = []\n",
        "  z = []\n",
        "  temp = []\n",
        "  for i in range(16):\n",
        "    for j in range(32):\n",
        "      for k in range(32):\n",
        "        if temp_grid[i,j,k] >= 1.6: #temp cut-off\n",
        "          x.append(i+1)\n",
        "          y.append(j+1)\n",
        "          z.append(k+1)\n",
        "          temp.append(temp_grid[i,j,k])\n",
        "  \n",
        "  df = pd.DataFrame(temp,columns=['Temperature'])\n",
        "  df['X'] = x\n",
        "  df['Y'] = y\n",
        "  df['Z'] = z\n",
        "\n",
        "  pos = df[['X','Y','Z']].values\n",
        "  agglo = AgglomerativeClustering(n_clusters=None, distance_threshold=1.8, linkage='single').fit(pos)\n",
        "  df['Cluster ID'] = agglo.labels_\n",
        "\n",
        "  return df\n",
        "\n",
        "def overlap(df1,df2):\n",
        "  md = df1.copy()\n",
        "  pred = df2.copy()\n",
        "  overlap = []\n",
        "  for i in range(md['Cluster ID'].max()+1):\n",
        "    for j in range(pred['Cluster ID'].max()+1):\n",
        "      md_tmp = md[md['Cluster ID'] == i]\n",
        "      pred_tmp = pred[pred['Cluster ID'] == j]\n",
        "      counts = 0\n",
        "      for k in range(len(md_tmp)):\n",
        "        for l in range(len(pred_tmp)):\n",
        "          if (md_tmp[['X','Y','Z']].values[k] == pred_tmp[['X','Y','Z']].values[l]).all():\n",
        "            counts += 1\n",
        "      #if counts > 0:\n",
        "      overlap.append([i,j,counts/len(md_tmp)])\n",
        "  df_overlap = pd.DataFrame(overlap,columns=['MD id','Pred id','fraction'])\n",
        "  over_array = df_overlap[df_overlap['fraction']>0].drop_duplicates(subset=['Pred id'],keep=False).drop_duplicates(subset=['MD id'],keep=False).values\n",
        "  new_id = []\n",
        "  for i in range(len(pred)):\n",
        "    if not (pred['Cluster ID'][i] in over_array[:,1].tolist()):\n",
        "      if pred['Cluster ID'][i] in df_overlap[df_overlap['fraction']>0]['Pred id'].tolist():\n",
        "        new_id.append(100)\n",
        "      else:\n",
        "        new_id.append(None)\n",
        "    for j in range(len(over_array)):\n",
        "      if pred['Cluster ID'][i] == over_array[j,1]:\n",
        "        new_id.append(int(over_array[j,0]))\n",
        "  pred['New ID'] = new_id\n",
        "\n",
        "  new_id = []\n",
        "  for i in range(len(md)):\n",
        "    if not (md['Cluster ID'][i] in over_array[:,0].tolist()):\n",
        "      if md['Cluster ID'][i] in df_overlap[df_overlap['fraction']>0]['MD id'].tolist():\n",
        "          new_id.append(100)\n",
        "      else:\n",
        "          new_id.append(None)\n",
        "    else:\n",
        "      new_id.append(int(md['Cluster ID'][i]))\n",
        "  md['New ID'] = new_id\n",
        "  return md, pred, df_overlap\n",
        "\n",
        "def analyze_clusters(df):\n",
        "  dataframe = df.copy()\n",
        "  vols = []\n",
        "  mean_temps = []\n",
        "  std_temps = []\n",
        "  cms = []\n",
        "  mis = []\n",
        "  ids = []\n",
        "  filt = dataframe[dataframe['New ID']<100].dropna()\n",
        "  for i in range(int(filt['New ID'].max())+1):\n",
        "    if i in filt['New ID'].tolist():\n",
        "      tmp = filt[filt['New ID'] == i]\n",
        "      v = len(tmp)*4\n",
        "      mean = np.mean(tmp['Temperature']*1000)\n",
        "      std = np.std(tmp['Temperature']*1000)\n",
        "      m = len(tmp)\n",
        "      cm_x = tmp['X'].sum()/m\n",
        "      cm_y = tmp['Y'].sum()/m\n",
        "      cm_z = tmp['Z'].sum()/m\n",
        "      cm = np.array([cm_x,cm_y,cm_z])\n",
        "      r_i = tmp[['X','Y','Z']].values\n",
        "      mom_int = 0\n",
        "      for j in range(len(tmp)):\n",
        "        mom_int += np.linalg.norm(r_i[j]-cm)**2\n",
        "      vols.append(v)\n",
        "      mean_temps.append(mean)\n",
        "      std_temps.append(std)\n",
        "      cms.append(cm)\n",
        "      mis.append(mom_int)\n",
        "      ids.append(i)\n",
        "    else:\n",
        "      pass\n",
        "  \n",
        "  df_clust = pd.DataFrame(ids, columns=['cluster id'])\n",
        "  df_clust['volume'] = vols\n",
        "  df_clust['mean T (K)'] = mean_temps\n",
        "  df_clust['std T (K)'] = std_temps\n",
        "  df_clust['R center of mass'] = cms\n",
        "  df_clust['moment of inertia'] = mis\n",
        "  return df_clust"
      ],
      "metadata": {
        "id": "TfOztIGF8B3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_clusters(md,pred):\n",
        "  fig = make_subplots(rows=1, cols=2, specs=[[{\"type\": \"scatter3d\"},{\"type\": \"scatter3d\"}]],\n",
        "                    subplot_titles=[\"MD Clusters\",\"CNN Clusters\"], horizontal_spacing = 0.1, vertical_spacing = 0.1)\n",
        "  fig.update_layout(autosize=False, width=1200, height=800) \n",
        "  trace_1 = go.Scatter3d(x = md['X'], y = md['Y'], z=md['Z'], hovertemplate = 'Cluster ID: %{marker.color:.2f}<extra></extra>',\n",
        "                      mode='markers',  marker=dict(symbol='square', color = md['Cluster ID']), showlegend=False)\n",
        "  fig.add_trace(trace_1, row=1, col=1)\n",
        "  trace_2 = go.Scatter3d(x = pred['X'], y = pred['Y'], z=pred['Z'], hovertemplate = 'Cluster ID: %{marker.color:.2f}<extra></extra>',\n",
        "                      mode='markers',  marker=dict(symbol='square', color = pred['Cluster ID']), showlegend=False)\n",
        "  fig.add_trace(trace_2, row=1, col=2)\n",
        "\n",
        "  return fig\n",
        "\n"
      ],
      "metadata": {
        "id": "VaPQdkGzPaXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize(md_corr, pred_corr, md_clust, pred_clust):\n",
        "  fig = make_subplots(rows=2, cols=2, specs=[[{\"type\": \"scatter3d\"},{\"type\": \"scatter3d\"}],\n",
        "                                            [{\"type\": \"scatter\"},{\"type\": \"scatter\"}]],\n",
        "                      subplot_titles=[\"MD Clusters\",\"CNN Clusters\",\"Temp vs Volume\",\"MI Parity Plot\"], horizontal_spacing = 0.1, vertical_spacing = 0.1)\n",
        "  fig.update_layout(autosize=False, width=1200, height=800) \n",
        "\n",
        "\n",
        "  ## MD SIMULATION CLUSTERS ##\n",
        "  tmp = md_corr[md_corr['New ID']<99].dropna()\n",
        "  trace_1 = go.Scatter3d(x = tmp['X'], y = tmp['Y'], z = tmp['Z'],text=tmp['New ID'],\n",
        "                        mode='markers', marker=dict(size=8, line=dict(width=0), symbol='square', colorscale='rainbow', color = tmp['New ID']), showlegend=False, scene='scene1')\n",
        "  tmp = md_corr[md_corr['New ID']==100]\n",
        "  trace_2 = go.Scatter3d(x = tmp['X'], y = tmp['Y'], z = tmp['Z'],text=tmp['Cluster ID'],\n",
        "                        mode='markers', marker=dict(size=8, line=dict(width=0), symbol='square', color = 'grey'), showlegend=False, scene='scene1')\n",
        "  tmp = md_corr[md_corr['New ID'].isnull()]\n",
        "  trace_3 = go.Scatter3d(x = tmp['X'], y = tmp['Y'], z = tmp['Z'],text=tmp['Cluster ID'],\n",
        "                        mode='markers', marker=dict(size=8, line=dict(width=0), symbol='square', color = 'black'), showlegend=False, scene='scene1')    \n",
        "  fig.add_trace(trace_1, row=1, col=1)\n",
        "  fig.add_trace(trace_2, row=1, col=1)\n",
        "  fig.add_trace(trace_3, row=1, col=1)\n",
        "\n",
        "  ## CNN PREDICTED CLUSTERS ##\n",
        "  tmp2 = pred_corr[pred_corr['New ID']<99].dropna()\n",
        "  trace_4 = go.Scatter3d(x = tmp2['X'], y = tmp2['Y'], z = tmp2['Z'],text=tmp2['New ID'],\n",
        "                        mode='markers', marker=dict(size=8, line=dict(width=0), symbol='square', colorscale='rainbow', color = tmp2['New ID']), showlegend=False, scene='scene2')\n",
        "  tmp2 = pred_corr[pred_corr['New ID']==100]\n",
        "  trace_5 = go.Scatter3d(x = tmp2['X'], y = tmp2['Y'], z = tmp2['Z'],text=tmp2['Cluster ID'],\n",
        "                        mode='markers', marker=dict(size=8, line=dict(width=0), symbol='square', color = 'grey'), showlegend=False, scene='scene2')\n",
        "  tmp2 = pred_corr[pred_corr['New ID'].isnull()]\n",
        "  trace_6 = go.Scatter3d(x = tmp2['X'], y = tmp2['Y'], z = tmp2['Z'],text=tmp2['Cluster ID'],\n",
        "                        mode='markers', marker=dict(size=8, line=dict(width=0), symbol='square', color = 'black'), showlegend=False, scene='scene2')\n",
        "\n",
        "  fig.add_trace(trace_4, row=1, col=2)\n",
        "  fig.add_trace(trace_5, row=1, col=2)\n",
        "  fig.add_trace(trace_6, row=1, col=2)\n",
        "\n",
        "\n",
        "  ### Hotspot Temperature vs Volume ###\n",
        "  trace_7 = go.Scatter(x=md_clust['volume'],y=md_clust['mean T (K)'],error_y=dict(type='data', array=md_clust['std T (K)'],visible=True),text=md_clust['cluster id'],\n",
        "                      mode='markers',marker=dict(symbol='square',colorscale='rainbow', color = md_clust['cluster id'],size=18),name='MD')\n",
        "  trace_8 = go.Scatter(x=pred_clust['volume'],y=pred_clust['mean T (K)'],error_y=dict(type='data', array=pred_clust['std T (K)'],visible=True),text=md_clust['cluster id'],\n",
        "                        mode='markers',marker=dict(symbol='circle',colorscale='rainbow', color = md_clust['cluster id'],size=18),name='CNN')\n",
        "\n",
        "  fig.add_trace(trace_7, row=2, col=1)\n",
        "  fig.add_trace(trace_8, row=2, col=1)\n",
        "  fig.update_xaxes(title=\"Volume\")\n",
        "  fig.update_yaxes(title=\"Temperature (K)\")\n",
        "\n",
        "  ### Hotspot Moment inertia parity plot ###\n",
        "  trace_9 = go.Scatter(x=np.log10(md_clust['moment of inertia']),y=np.log10(pred_clust['moment of inertia']),\n",
        "                      mode='markers',marker=dict(symbol='square',color='green',size=14), showlegend=False)\n",
        "\n",
        "  fig.add_trace(trace_9, row=2, col=2)\n",
        "  fig.update_yaxes(title=\"Predicted log(moment of inertia)\")\n",
        "  fig.update_xaxes(title=\"MD Simulated log(moment of inertia))\")\n",
        "\n",
        "  #def cam_change(layout, camera):\n",
        "  #   fig.layout.scene2.camera = camera\n",
        "  #fig.layout.scene1.on_change(cam_change, 'camera')\n",
        "  return fig"
      ],
      "metadata": {
        "id": "UePKih3V3s9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CLUSTER ANALYSIS TRAINING DATA\n",
        "\n",
        "for i, title in enumerate(paths):\n",
        "  print(i)\n",
        "  md_temp = np.load(str(title)+'/output.npy')/1000\n",
        "  pred_temp = np.load('results/'+str(title)+'/prediction.npy')\n",
        "\n",
        "  md_1 = get_clusters(md_temp)\n",
        "  pred_1 = get_clusters(pred_temp)\n",
        "\n",
        "  md_corr, pred_corr, over = overlap(md_1,pred_1)\n",
        "  md_clust = analyze_clusters(md_corr)\n",
        "  pred_clust = analyze_clusters(pred_corr)\n",
        "\n",
        "  fig = visualize(md_corr,pred_corr,md_clust,pred_clust)\n",
        "\n",
        "  fig.write_html('results/'+str(title)+'/cluster_1.6.html')"
      ],
      "metadata": {
        "id": "RhYlpaScCVoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_0"
      ],
      "metadata": {
        "id": "kS-zq2YzD_Yx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mi = go.Scatter(x=md_mis, y=pred_mis,mode='markers',marker=dict(symbol='square',color='Blue',size=10))\n",
        "mi_fig = go.Figure(mi)\n",
        "mi_fig.show()"
      ],
      "metadata": {
        "id": "YYw6n0izpIAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploration GUI  (Under development)"
      ],
      "metadata": {
        "id": "8GgptMfOlBs2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# FUNCTIONS TO OBSERVE\n",
        "def populate_sliders(change):\n",
        "\n",
        "  with output_plot:\n",
        "    clear_output()\n",
        "  \n",
        "  example_label = np.load(main_dropdown.value + \"/output.npy\").squeeze() / 1000\n",
        "  example_pred = np.load('results/'+main_dropdown.value+\"/prediction.npy\").squeeze()\n",
        "\n",
        "  md_tmp_slider.min = np.min(example_label)\n",
        "  md_tmp_slider.max = np.max(example_label)\n",
        "  md_tmp_slider.value = [np.min(example_label), np.max(example_label)]\n",
        "\n",
        "  pred_tmp_slider.min = np.min(example_pred)\n",
        "  pred_tmp_slider.max = np.max(example_pred)\n",
        "  pred_tmp_slider.value = [np.min(example_pred), np.max(example_pred)]\n",
        "\n",
        "def display_fullplot(change):\n",
        "\n",
        "  with output_plot:\n",
        "    clear_output()\n",
        "\n",
        "  example_inp = np.load(main_dropdown.value + \"/input.npy\").squeeze()\n",
        "  example_inp = example_inp[:, 1:-1, 1:-1] # NON PERIODIC GETS PLOTTED\n",
        "  example_label = np.load(main_dropdown.value + \"/output.npy\").squeeze() / 1000\n",
        "  example_pred = np.load('results/'+main_dropdown.value+\"/prediction.npy\").squeeze()\n",
        "\n",
        "  example_fig = datapoint_results_print(example_inp, example_label, example_pred)\n",
        "  example_fig = go.FigureWidget(example_fig)\n",
        "\n",
        "  if toggle_button.value == \"MD\":\n",
        "\n",
        "    map = np.where(np.logical_and(example_label >=md_tmp_slider.value[0], example_label <=md_tmp_slider.value[1]), 8, 0)\n",
        "    with example_fig.batch_update():\n",
        "      example_fig.data[0].marker.size = map.flatten()\n",
        "      example_fig.data[1].marker.size = map.flatten()\n",
        "      example_fig.data[2].marker.size = map.flatten()\n",
        "      example_fig.data[3].marker.size = map.flatten()\n",
        "      example_fig.data[4].marker.size = map.flatten()\n",
        "\n",
        "  elif toggle_button.value == \"Pred\":\n",
        "\n",
        "    map = np.where(np.logical_and(example_pred>=pred_tmp_slider.value[0],example_pred<=pred_tmp_slider.value[1]), 8, 0)\n",
        "    with example_fig.batch_update():\n",
        "      example_fig.data[0].marker.size = map.flatten()\n",
        "      example_fig.data[1].marker.size = map.flatten()\n",
        "      example_fig.data[2].marker.size = map.flatten()\n",
        "      example_fig.data[3].marker.size = map.flatten()\n",
        "      example_fig.data[4].marker.size = map.flatten()\n",
        "\n",
        "  with output_plot:\n",
        "    example_fig.show()\n",
        "\n",
        "def slider_function(b):\n",
        "  md_tmp_slider.disabled = (toggle_button.value == 'Pred')\n",
        "  pred_tmp_slider.disabled = (toggle_button.value == 'MD')\n",
        "\n",
        "# GETTING LABELS FOR SYSTEMS\n",
        "systems = [x[0] for x in os.walk('results')]\n",
        "systems.remove('results')\n",
        "systems.remove('results/train')\n",
        "systems.remove('results/validation')\n",
        "systems = sorted(['/'.join(x.split('/')[1:]) for x in systems])\n",
        "\n",
        "# WIDGETS\n",
        "main_dropdown = widgets.Dropdown(options=systems, description = \"System: \", continuous_update=False)\n",
        "toggle_button = widgets.ToggleButtons(options=['MD', 'Pred'], description='Filter by:', button_style='')\n",
        "md_tmp_slider = widgets.FloatRangeSlider(value=[0, 1], min=0, max=1, description='MD Temp:', disabled=False, continuous_update=False)\n",
        "pred_tmp_slider = widgets.FloatRangeSlider(value=[0, 1], min=0, max=1, description='CNN Temp:', disabled=True, continuous_update=False)\n",
        "generate_button = widgets.Button(description='Generate', button_style='success', icon='fa-hand-pointer-o')\n",
        "\n",
        "output_plot = widgets.Output()\n",
        "\n",
        "# OBSERVERS\n",
        "main_dropdown.observe(populate_sliders, names='value')\n",
        "toggle_button.observe(slider_function)\n",
        "generate_button.on_click(display_fullplot)\n",
        "\n",
        "controls_box =widgets.VBox([main_dropdown, toggle_button, md_tmp_slider, pred_tmp_slider, generate_button], layout=widgets.Layout(width='100%', display='flex', flex_flow='column', align_items='flex-start'))\n",
        "frame_box = widgets.HBox([controls_box, output_plot])\n",
        "\n",
        "display(frame_box)"
      ],
      "metadata": {
        "id": "-sPms_uLlel7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ru-M1kbxIO5a"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "pbx-local",
      "language": "python",
      "name": "pbx-local"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15"
    },
    "vscode": {
      "interpreter": {
        "hash": "f806901b044f1f905f6f73d9d34ac86b2be2e087ac41c051b6f31285dd315984"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}