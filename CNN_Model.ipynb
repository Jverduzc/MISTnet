{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5Nd2jeT8r8uu"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "import math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "from plotly.io import write_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "V08TFba6G_nw"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import initializers\n",
        "from keras.layers import Input, Dropout, BatchNormalization, Conv3DTranspose, concatenate, Dense, Conv3D, Flatten, MaxPooling3D\n",
        "from keras.models import Sequential\n",
        "import keras.backend as K\n",
        "tf.keras.utils.set_random_seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaleido"
      ],
      "metadata": {
        "id": "M79B0dH7ftZW",
        "outputId": "590c1704-4c0f-4ff4-f716-22256776b9ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: kaleido in /usr/local/lib/python3.7/dist-packages (0.2.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "KnH0b5QqG_ny",
        "outputId": "f2997d9b-7132-4949-fd37-b060726b3fca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  1\n"
          ]
        }
      ],
      "source": [
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
        "os.chdir(\"CNN_PBX_Model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "KZxqA1mmG_nz",
        "outputId": "78ab191d-8a63-4c1a-fa3e-84ea90634def",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 16, 34, 34, 3)\n",
            "(64, 16, 32, 32, 1)\n"
          ]
        }
      ],
      "source": [
        "# TRAINING DATA\n",
        "\n",
        "paths = [x[0] for x in os.walk('train/')][1:]\n",
        "\n",
        "train_data = []\n",
        "train_labels = []\n",
        "\n",
        "for i in paths:\n",
        "  train_ex = np.load(i + \"/input.npy\")\n",
        "\n",
        "  if train_ex.shape != (16,34,34):\n",
        "\n",
        "    first_axis_pad = int(0.5 * (34 - train_ex.shape[1]))\n",
        "    second_axis_pad = int(0.5 * (34 - train_ex.shape[2]))\n",
        "\n",
        "    train_ex = np.pad(train_ex, ((0, 0), (first_axis_pad, first_axis_pad), (second_axis_pad, second_axis_pad), (0,0)), 'wrap')\n",
        "\n",
        "  train_lb = np.load(i + \"/output.npy\")\n",
        "  train_data.append(train_ex)\n",
        "  train_labels.append(train_lb)\n",
        "\n",
        "train_data = np.array(train_data)\n",
        "train_labels = np.array(train_labels) / 1000\n",
        "\n",
        "print(train_data.shape)\n",
        "print(train_labels.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "02ORxH_vZNOC",
        "outputId": "320a569d-c94a-4e88-c9c8-a3346db45a5b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(28, 16, 34, 34, 3)\n",
            "(28, 16, 32, 32, 1)\n"
          ]
        }
      ],
      "source": [
        "# VALIDATION DATA\n",
        "\n",
        "validation_paths = [x[0] for x in os.walk('validation/')][1:]\n",
        "\n",
        "validation_data = []\n",
        "validation_labels = []\n",
        "\n",
        "for i in validation_paths:\n",
        "  validation_ex= np.load(i + \"/input.npy\")\n",
        "\n",
        "  if validation_ex.shape != (16,34,34):\n",
        "\n",
        "    first_axis_pad = int(0.5 * (34 - validation_ex.shape[1]))\n",
        "    second_axis_pad = int(0.5 * (34 - validation_ex.shape[2]))\n",
        "\n",
        "    validation_ex = np.pad(validation_ex, ((0, 0), (first_axis_pad, first_axis_pad), (second_axis_pad, second_axis_pad), (0,0)), 'wrap')\n",
        "\n",
        "\n",
        "  validation_lb = np.load(i + \"/output.npy\")\n",
        "  validation_data.append(validation_ex)\n",
        "  validation_labels.append(validation_lb)\n",
        "\n",
        "validation_data = np.array(validation_data)\n",
        "validation_labels = np.array(validation_labels) / 1000\n",
        "\n",
        "print(validation_data.shape)\n",
        "print(validation_labels.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "fWvLsqQXsWGy"
      },
      "outputs": [],
      "source": [
        "def periodic_padding_flexible(tensor, axis, padding=1):\n",
        "\n",
        "    if isinstance(axis,int):\n",
        "        axis = (axis,)\n",
        "    if isinstance(padding,int):\n",
        "        padding = (padding,)\n",
        "\n",
        "    ndim = len(tensor.shape)\n",
        "\n",
        "    for ax,p in zip(axis,padding):\n",
        "        # create a slice object that selects everything from all axes,\n",
        "        # except only 0:p for the specified for right, and -p: for left\n",
        "\n",
        "        ind_right = [slice(-p,None) if i == ax else slice(None) for i in range(ndim)]\n",
        "        ind_left = [slice(0, p) if i == ax else slice(None) for i in range(ndim)]\n",
        "        right = tensor[ind_right]\n",
        "        left = tensor[ind_left]\n",
        "        middle = tensor\n",
        "        tensor = tf.concat([right,middle,left], axis=ax)\n",
        "\n",
        "    return tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "rLhZ3NIocWJ9"
      },
      "outputs": [],
      "source": [
        "def DownConvBlock(inputs, n_filters=32, filter_size = 3, max_pooling=True, special_padding=False):\n",
        "\n",
        "  padding_size = int((filter_size-1)/2)\n",
        "  kernel_init =   tf.keras.initializers.GlorotUniform(seed=0)\n",
        "  bias_init = tf.keras.initializers.Zeros()\n",
        "\n",
        "\n",
        "  # PERIODIC PADDING\n",
        "\n",
        "  inputs = periodic_padding_flexible(inputs, axis=1,padding=padding_size)\n",
        "  if special_padding == False:\n",
        "    inputs = periodic_padding_flexible(inputs, axis=2,padding=padding_size)\n",
        "    inputs = periodic_padding_flexible(inputs, axis=3,padding=padding_size)\n",
        "  \n",
        "  conv = Conv3D(n_filters, filter_size, activation='relu', padding='valid', kernel_initializer=kernel_init, bias_initializer=bias_init)(inputs)\n",
        "  print(conv.shape)\n",
        "  conv = periodic_padding_flexible(conv, axis=(1,2,3),padding=(padding_size,padding_size,padding_size))\n",
        "  conv = Conv3D(n_filters, filter_size, activation='relu', padding='valid', kernel_initializer=kernel_init, bias_initializer=bias_init)(conv)\n",
        "  print(conv.shape)\n",
        "  conv = BatchNormalization()(conv, training=False)\n",
        "      \n",
        "  if max_pooling:\n",
        "    next_layer = tf.keras.layers.MaxPooling3D(pool_size = (2,2,2))(conv)\n",
        "  else:\n",
        "    next_layer = conv\n",
        "  \n",
        "  skip_connection = conv   \n",
        "\n",
        "  print(\"end_of_block\") \n",
        "  return next_layer, skip_connection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "YvMNFlsHseOF"
      },
      "outputs": [],
      "source": [
        "def UpConvBlock(prev_layer_input, skip_layer_input, filter_size = 3, n_filters=32):\n",
        "\n",
        "    padding_size = int((filter_size-1)/2)\n",
        "    kernel_init = tf.keras.initializers.GlorotUniform(seed=0)\n",
        "    bias_init = tf.keras.initializers.Zeros()   \n",
        "\n",
        "    up = Conv3DTranspose(n_filters, (filter_size,filter_size,filter_size),\n",
        "                         strides=(filter_size-1,filter_size-1,filter_size-1),\n",
        "                         padding='same', kernel_initializer=kernel_init, bias_initializer=bias_init)(prev_layer_input)\n",
        "\n",
        "    merge = concatenate([up, skip_layer_input], axis=4)\n",
        "    merge = periodic_padding_flexible(merge, axis=(1,2,3),padding=(padding_size,padding_size,padding_size))\n",
        "    conv = Conv3D(n_filters, filter_size, activation='relu', padding='valid', kernel_initializer=kernel_init,  bias_initializer=bias_init)(merge)\n",
        "    print(conv.shape)\n",
        "    conv = periodic_padding_flexible(conv, axis=(1,2,3),padding=(padding_size,padding_size,padding_size))\n",
        "    conv = Conv3D(n_filters, filter_size, activation='relu',padding='valid', kernel_initializer=kernel_init,  bias_initializer=bias_init)(conv)\n",
        "    print(conv.shape)\n",
        "\n",
        "    print(\"end_of_block\")\n",
        "    return conv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "fe-fUh99sgKg"
      },
      "outputs": [],
      "source": [
        "\n",
        "def UNet3DModel(input_size=(16, 34, 34, 3), n_filters=32, filter_size=3, n_classes=1):\n",
        "  kernel_init =  tf.keras.initializers.GlorotUniform(seed=0)\n",
        "  bias_init = tf.keras.initializers.Zeros()  \n",
        "  \n",
        "  inputs = Input(input_size)\n",
        "  print(\"Inputs\", inputs.shape)\n",
        "\n",
        "  cblock0 = DownConvBlock(inputs,     n_filters = n_filters    , filter_size = filter_size, max_pooling=False, special_padding=True)\n",
        "  print(\"CB0\", cblock0[0].shape)\n",
        "\n",
        "  cblock1 = DownConvBlock(inputs,     n_filters = n_filters    , filter_size = filter_size, max_pooling=True, special_padding=True)\n",
        "  print(\"CB1\", cblock1[0].shape)\n",
        "\n",
        "  cblock2 = DownConvBlock(cblock1[0], n_filters = n_filters*2  , filter_size = filter_size, max_pooling=True, special_padding=False)\n",
        "  print(\"CB2\", cblock2[0].shape)\n",
        "    \n",
        "  cblock3 = DownConvBlock(cblock2[0], n_filters = n_filters*4  , filter_size = filter_size, max_pooling=True, special_padding=False)\n",
        "  print(\"CB3\", cblock3[0].shape)\n",
        "  \n",
        "  cblock4 = DownConvBlock(cblock3[0], n_filters = n_filters*8  , filter_size = filter_size, max_pooling=False, special_padding=False)\n",
        "  print(\"CB4\", cblock4[0].shape)\n",
        "\n",
        "\n",
        "  print(\"------------------\")\n",
        "\n",
        "  ublock7 = UpConvBlock(cblock4[0]   , cblock3[1],  n_filters = n_filters * 4, filter_size = filter_size)\n",
        "  print(\"UB7\", ublock7.shape)\n",
        "  \n",
        "  ublock8 = UpConvBlock(ublock7   , cblock2[1],  n_filters = n_filters * 2, filter_size = filter_size)\n",
        "  print(\"UB8\", ublock8.shape)\n",
        "  \n",
        "  ublock9 = UpConvBlock(ublock8   , cblock1[1],  n_filters = n_filters, filter_size = filter_size)\n",
        "  print(\"UB9\", ublock9.shape)\n",
        "\n",
        "  ublock9 = periodic_padding_flexible(ublock9, axis=(1,2,3),padding=(1,1,1))\n",
        "  \n",
        "  conv9 = Conv3D(n_filters, 3, activation='relu', padding='valid', kernel_initializer=kernel_init,  bias_initializer=bias_init)(ublock9)\n",
        "  print(\"C9\", conv9.shape)\n",
        "  \n",
        "  conv10 = Conv3D(n_classes, 1, padding='same', kernel_initializer=kernel_init,  bias_initializer=bias_init)(conv9)\n",
        "  print(\"C10\", conv10.shape)\n",
        "\n",
        "  model = tf.keras.Model(inputs=inputs, outputs=conv10)  \n",
        "\n",
        "  return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "d2a5FVUYPHTB"
      },
      "outputs": [],
      "source": [
        "\n",
        "def custom_mse(y_true,y_pred):\n",
        "    w_hot = 5.0\n",
        "    w_cold = 1.0\n",
        "    cutoff = 1.8\n",
        "    weightmat = tf.cast(tf.where(tf.greater(y_true, cutoff), w_hot, w_cold),float)\n",
        "    loss = tf.cast(K.square(y_pred - y_true),float)\n",
        "    loss = loss*weightmat\n",
        "    loss = K.mean(loss)\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "R3SJtgKps6tc",
        "outputId": "af612c59-1665-461b-b5bc-94a750044e89",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs (None, 16, 34, 34, 3)\n",
            "(None, 16, 32, 32, 32)\n",
            "(None, 16, 32, 32, 32)\n",
            "end_of_block\n",
            "CB0 (None, 16, 32, 32, 32)\n",
            "(None, 16, 32, 32, 32)\n",
            "(None, 16, 32, 32, 32)\n",
            "end_of_block\n",
            "CB1 (None, 8, 16, 16, 32)\n",
            "(None, 8, 16, 16, 64)\n",
            "(None, 8, 16, 16, 64)\n",
            "end_of_block\n",
            "CB2 (None, 4, 8, 8, 64)\n",
            "(None, 4, 8, 8, 128)\n",
            "(None, 4, 8, 8, 128)\n",
            "end_of_block\n",
            "CB3 (None, 2, 4, 4, 128)\n",
            "(None, 2, 4, 4, 256)\n",
            "(None, 2, 4, 4, 256)\n",
            "end_of_block\n",
            "CB4 (None, 2, 4, 4, 256)\n",
            "------------------\n",
            "(None, 4, 8, 8, 128)\n",
            "(None, 4, 8, 8, 128)\n",
            "end_of_block\n",
            "UB7 (None, 4, 8, 8, 128)\n",
            "(None, 8, 16, 16, 64)\n",
            "(None, 8, 16, 16, 64)\n",
            "end_of_block\n",
            "UB8 (None, 8, 16, 16, 64)\n",
            "(None, 16, 32, 32, 32)\n",
            "(None, 16, 32, 32, 32)\n",
            "end_of_block\n",
            "UB9 (None, 16, 32, 32, 32)\n",
            "C9 (None, 16, 32, 32, 32)\n",
            "C10 (None, 16, 32, 32, 1)\n"
          ]
        }
      ],
      "source": [
        "model = UNet3DModel(input_size=(16, 34, 34, 3), n_filters=32, filter_size = 3, n_classes=1)\n",
        "optimizer = tf.optimizers.Adam(learning_rate = 0.0005)\n",
        "model.compile(loss=custom_mse, optimizer=optimizer, metrics=['mse'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "baIEkfpntuOW",
        "outputId": "5b97c671-df83-48ca-d21a-4a2d57aa0e21",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "2/2 [==============================] - 13s 4s/step - loss: 1.7163 - mse: 1.2085 - val_loss: 0.9396 - val_mse: 0.6231 - lr: 5.0000e-04\n",
            "Epoch 2/1000\n",
            "2/2 [==============================] - 1s 736ms/step - loss: 0.7435 - mse: 0.4189 - val_loss: 18.3235 - val_mse: 17.6345 - lr: 5.0000e-04\n",
            "Epoch 3/1000\n",
            "2/2 [==============================] - 1s 852ms/step - loss: 9.4488 - mse: 8.9768 - val_loss: 0.6523 - val_mse: 0.3868 - lr: 5.0000e-04\n",
            "Epoch 4/1000\n",
            "2/2 [==============================] - 1s 760ms/step - loss: 0.8536 - mse: 0.4902 - val_loss: 0.9400 - val_mse: 0.6220 - lr: 5.0000e-04\n",
            "Epoch 5/1000\n",
            "2/2 [==============================] - 1s 783ms/step - loss: 1.0751 - mse: 0.6679 - val_loss: 1.0296 - val_mse: 0.6972 - lr: 5.0000e-04\n",
            "Epoch 6/1000\n",
            "2/2 [==============================] - 1s 745ms/step - loss: 1.1499 - mse: 0.7294 - val_loss: 1.0715 - val_mse: 0.7327 - lr: 5.0000e-04\n",
            "Epoch 7/1000\n",
            "2/2 [==============================] - 1s 739ms/step - loss: 1.1845 - mse: 0.7581 - val_loss: 1.0888 - val_mse: 0.7476 - lr: 5.0000e-04\n",
            "Epoch 8/1000\n",
            "2/2 [==============================] - 1s 761ms/step - loss: 1.1967 - mse: 0.7687 - val_loss: 1.0891 - val_mse: 0.7481 - lr: 5.0000e-04\n",
            "Epoch 9/1000\n",
            "2/2 [==============================] - 1s 731ms/step - loss: 1.1930 - mse: 0.7662 - val_loss: 1.0767 - val_mse: 0.7380 - lr: 5.0000e-04\n",
            "Epoch 10/1000\n",
            "2/2 [==============================] - 1s 738ms/step - loss: 1.1769 - mse: 0.7536 - val_loss: 1.0533 - val_mse: 0.7186 - lr: 5.0000e-04\n",
            "Epoch 11/1000\n",
            "2/2 [==============================] - 1s 724ms/step - loss: 1.1495 - mse: 0.7316 - val_loss: 1.0161 - val_mse: 0.6875 - lr: 5.0000e-04\n",
            "Epoch 12/1000\n",
            "2/2 [==============================] - 1s 724ms/step - loss: 1.1068 - mse: 0.6971 - val_loss: 0.9581 - val_mse: 0.6390 - lr: 5.0000e-04\n",
            "Epoch 13/1000\n",
            "2/2 [==============================] - 1s 715ms/step - loss: 1.0404 - mse: 0.6433 - val_loss: 0.8622 - val_mse: 0.5592 - lr: 5.0000e-04\n",
            "Epoch 14/1000\n",
            "2/2 [==============================] - 1s 759ms/step - loss: 0.9231 - mse: 0.5484 - val_loss: 0.6361 - val_mse: 0.3745 - lr: 5.0000e-04\n",
            "Epoch 15/1000\n",
            "2/2 [==============================] - 1s 730ms/step - loss: 0.6016 - mse: 0.3041 - val_loss: 2.5075 - val_mse: 2.4486 - lr: 5.0000e-04\n",
            "Epoch 16/1000\n",
            "2/2 [==============================] - 1s 761ms/step - loss: 1.5153 - mse: 1.3516 - val_loss: 0.5804 - val_mse: 0.3277 - lr: 5.0000e-04\n",
            "Epoch 17/1000\n",
            "2/2 [==============================] - 1s 731ms/step - loss: 0.6923 - mse: 0.3650 - val_loss: 0.6461 - val_mse: 0.3829 - lr: 5.0000e-04\n",
            "Epoch 18/1000\n",
            "2/2 [==============================] - 1s 720ms/step - loss: 0.7530 - mse: 0.4143 - val_loss: 0.6548 - val_mse: 0.3898 - lr: 5.0000e-04\n",
            "Epoch 19/1000\n",
            "2/2 [==============================] - 1s 724ms/step - loss: 0.7423 - mse: 0.4055 - val_loss: 0.6035 - val_mse: 0.3483 - lr: 5.0000e-04\n",
            "Epoch 20/1000\n",
            "2/2 [==============================] - 1s 757ms/step - loss: 0.6873 - mse: 0.3627 - val_loss: 0.5406 - val_mse: 0.2982 - lr: 5.0000e-04\n",
            "Epoch 21/1000\n",
            "2/2 [==============================] - 1s 763ms/step - loss: 0.6199 - mse: 0.3111 - val_loss: 0.4582 - val_mse: 0.2347 - lr: 5.0000e-04\n",
            "Epoch 22/1000\n",
            "2/2 [==============================] - 1s 752ms/step - loss: 0.5298 - mse: 0.2454 - val_loss: 0.3678 - val_mse: 0.1688 - lr: 5.0000e-04\n",
            "Epoch 23/1000\n",
            "2/2 [==============================] - 1s 758ms/step - loss: 0.4365 - mse: 0.1821 - val_loss: 0.2860 - val_mse: 0.1150 - lr: 5.0000e-04\n",
            "Epoch 24/1000\n",
            "2/2 [==============================] - 1s 756ms/step - loss: 0.3533 - mse: 0.1344 - val_loss: 0.2320 - val_mse: 0.0923 - lr: 5.0000e-04\n",
            "Epoch 25/1000\n",
            "2/2 [==============================] - 1s 720ms/step - loss: 0.3058 - mse: 0.1239 - val_loss: 0.2317 - val_mse: 0.1225 - lr: 5.0000e-04\n",
            "Epoch 26/1000\n",
            "2/2 [==============================] - 1s 719ms/step - loss: 0.3090 - mse: 0.1631 - val_loss: 0.2766 - val_mse: 0.1897 - lr: 5.0000e-04\n",
            "Epoch 27/1000\n",
            "2/2 [==============================] - 1s 723ms/step - loss: 0.3496 - mse: 0.2264 - val_loss: 0.3040 - val_mse: 0.2256 - lr: 5.0000e-04\n",
            "Epoch 28/1000\n",
            "2/2 [==============================] - 1s 718ms/step - loss: 0.3594 - mse: 0.2435 - val_loss: 0.2780 - val_mse: 0.1941 - lr: 5.0000e-04\n",
            "Epoch 29/1000\n",
            "2/2 [==============================] - 1s 718ms/step - loss: 0.3283 - mse: 0.2037 - val_loss: 0.2372 - val_mse: 0.1393 - lr: 5.0000e-04\n",
            "Epoch 30/1000\n",
            "2/2 [==============================] - 1s 734ms/step - loss: 0.2945 - mse: 0.1530 - val_loss: 0.2157 - val_mse: 0.1021 - lr: 5.0000e-04\n",
            "Epoch 31/1000\n",
            "2/2 [==============================] - 1s 735ms/step - loss: 0.2780 - mse: 0.1210 - val_loss: 0.2121 - val_mse: 0.0890 - lr: 5.0000e-04\n",
            "Epoch 32/1000\n",
            "2/2 [==============================] - 1s 746ms/step - loss: 0.2752 - mse: 0.1117 - val_loss: 0.2107 - val_mse: 0.0902 - lr: 5.0000e-04\n",
            "Epoch 33/1000\n",
            "2/2 [==============================] - 1s 763ms/step - loss: 0.2715 - mse: 0.1130 - val_loss: 0.2073 - val_mse: 0.0912 - lr: 5.0000e-04\n",
            "Epoch 34/1000\n",
            "2/2 [==============================] - 1s 741ms/step - loss: 0.2647 - mse: 0.1120 - val_loss: 0.2004 - val_mse: 0.0854 - lr: 5.0000e-04\n",
            "Epoch 35/1000\n",
            "2/2 [==============================] - 1s 739ms/step - loss: 0.2529 - mse: 0.1058 - val_loss: 0.1953 - val_mse: 0.0871 - lr: 5.0000e-04\n",
            "Epoch 36/1000\n",
            "2/2 [==============================] - 1s 735ms/step - loss: 0.2437 - mse: 0.1048 - val_loss: 0.1907 - val_mse: 0.0831 - lr: 5.0000e-04\n",
            "Epoch 37/1000\n",
            "2/2 [==============================] - 1s 732ms/step - loss: 0.2358 - mse: 0.0997 - val_loss: 0.1873 - val_mse: 0.0801 - lr: 5.0000e-04\n",
            "Epoch 38/1000\n",
            "2/2 [==============================] - 1s 736ms/step - loss: 0.2286 - mse: 0.0956 - val_loss: 0.1831 - val_mse: 0.0790 - lr: 5.0000e-04\n",
            "Epoch 39/1000\n",
            "2/2 [==============================] - 1s 733ms/step - loss: 0.2202 - mse: 0.0945 - val_loss: 0.1798 - val_mse: 0.0827 - lr: 5.0000e-04\n",
            "Epoch 40/1000\n",
            "2/2 [==============================] - 1s 733ms/step - loss: 0.2135 - mse: 0.0973 - val_loss: 0.1777 - val_mse: 0.0849 - lr: 5.0000e-04\n",
            "Epoch 41/1000\n",
            "2/2 [==============================] - 1s 728ms/step - loss: 0.2077 - mse: 0.0967 - val_loss: 0.1740 - val_mse: 0.0802 - lr: 5.0000e-04\n",
            "Epoch 42/1000\n",
            "2/2 [==============================] - 1s 735ms/step - loss: 0.2007 - mse: 0.0910 - val_loss: 0.1709 - val_mse: 0.0769 - lr: 5.0000e-04\n",
            "Epoch 43/1000\n",
            "2/2 [==============================] - 1s 733ms/step - loss: 0.1941 - mse: 0.0868 - val_loss: 0.1679 - val_mse: 0.0756 - lr: 5.0000e-04\n",
            "Epoch 44/1000\n",
            "2/2 [==============================] - 1s 744ms/step - loss: 0.1872 - mse: 0.0850 - val_loss: 0.1649 - val_mse: 0.0765 - lr: 5.0000e-04\n",
            "Epoch 45/1000\n",
            "2/2 [==============================] - 1s 730ms/step - loss: 0.1808 - mse: 0.0862 - val_loss: 0.1622 - val_mse: 0.0773 - lr: 5.0000e-04\n",
            "Epoch 46/1000\n",
            "2/2 [==============================] - 1s 738ms/step - loss: 0.1738 - mse: 0.0836 - val_loss: 0.1591 - val_mse: 0.0722 - lr: 5.0000e-04\n",
            "Epoch 47/1000\n",
            "2/2 [==============================] - 1s 735ms/step - loss: 0.1665 - mse: 0.0773 - val_loss: 0.1565 - val_mse: 0.0701 - lr: 5.0000e-04\n",
            "Epoch 48/1000\n",
            "2/2 [==============================] - 1s 743ms/step - loss: 0.1596 - mse: 0.0746 - val_loss: 0.1537 - val_mse: 0.0707 - lr: 5.0000e-04\n",
            "Epoch 49/1000\n",
            "2/2 [==============================] - 1s 730ms/step - loss: 0.1526 - mse: 0.0745 - val_loss: 0.1512 - val_mse: 0.0709 - lr: 5.0000e-04\n",
            "Epoch 50/1000\n",
            "2/2 [==============================] - 1s 734ms/step - loss: 0.1461 - mse: 0.0732 - val_loss: 0.1493 - val_mse: 0.0691 - lr: 5.0000e-04\n",
            "Epoch 51/1000\n",
            "2/2 [==============================] - 1s 728ms/step - loss: 0.1393 - mse: 0.0682 - val_loss: 0.1483 - val_mse: 0.0675 - lr: 5.0000e-04\n",
            "Epoch 52/1000\n",
            "2/2 [==============================] - 1s 732ms/step - loss: 0.1332 - mse: 0.0651 - val_loss: 0.1463 - val_mse: 0.0693 - lr: 5.0000e-04\n",
            "Epoch 53/1000\n",
            "2/2 [==============================] - 1s 742ms/step - loss: 0.1274 - mse: 0.0667 - val_loss: 0.1451 - val_mse: 0.0712 - lr: 5.0000e-04\n",
            "Epoch 54/1000\n",
            "2/2 [==============================] - 1s 854ms/step - loss: 0.1219 - mse: 0.0655 - val_loss: 0.1461 - val_mse: 0.0693 - lr: 5.0000e-04\n",
            "Epoch 55/1000\n",
            "2/2 [==============================] - 1s 703ms/step - loss: 0.1165 - mse: 0.0600 - val_loss: 0.1474 - val_mse: 0.0710 - lr: 5.0000e-04\n",
            "Epoch 56/1000\n",
            "2/2 [==============================] - 1s 861ms/step - loss: 0.1120 - mse: 0.0599 - val_loss: 0.1474 - val_mse: 0.0758 - lr: 5.0000e-04\n",
            "Epoch 57/1000\n",
            "2/2 [==============================] - 1s 701ms/step - loss: 0.1073 - mse: 0.0610 - val_loss: 0.1508 - val_mse: 0.0799 - lr: 5.0000e-04\n",
            "Epoch 58/1000\n",
            "2/2 [==============================] - 1s 707ms/step - loss: 0.1048 - mse: 0.0642 - val_loss: 0.1529 - val_mse: 0.0829 - lr: 5.0000e-04\n",
            "Epoch 59/1000\n",
            "2/2 [==============================] - 1s 861ms/step - loss: 0.1020 - mse: 0.0625 - val_loss: 0.1535 - val_mse: 0.0830 - lr: 5.0000e-04\n",
            "Epoch 60/1000\n",
            "2/2 [==============================] - 1s 698ms/step - loss: 0.0992 - mse: 0.0617 - val_loss: 0.1475 - val_mse: 0.0809 - lr: 5.0000e-04\n",
            "Epoch 61/1000\n",
            "2/2 [==============================] - 1s 704ms/step - loss: 0.0965 - mse: 0.0607 - val_loss: 0.1496 - val_mse: 0.0778 - lr: 5.0000e-04\n",
            "Epoch 62/1000\n",
            "2/2 [==============================] - 1s 708ms/step - loss: 0.0943 - mse: 0.0573 - val_loss: 0.1447 - val_mse: 0.0762 - lr: 5.0000e-04\n",
            "Epoch 63/1000\n",
            "2/2 [==============================] - 1s 699ms/step - loss: 0.0922 - mse: 0.0569 - val_loss: 0.1469 - val_mse: 0.0758 - lr: 5.0000e-04\n",
            "Epoch 64/1000\n",
            "2/2 [==============================] - 1s 738ms/step - loss: 0.0900 - mse: 0.0547 - val_loss: 0.1427 - val_mse: 0.0760 - lr: 5.0000e-04\n",
            "Epoch 65/1000\n",
            "2/2 [==============================] - 1s 709ms/step - loss: 0.0880 - mse: 0.0543 - val_loss: 0.1423 - val_mse: 0.0773 - lr: 5.0000e-04\n",
            "Epoch 66/1000\n",
            "2/2 [==============================] - 1s 708ms/step - loss: 0.0870 - mse: 0.0544 - val_loss: 0.1437 - val_mse: 0.0776 - lr: 5.0000e-04\n",
            "Epoch 67/1000\n",
            "2/2 [==============================] - 1s 714ms/step - loss: 0.0852 - mse: 0.0555 - val_loss: 0.1551 - val_mse: 0.0815 - lr: 5.0000e-04\n",
            "Epoch 68/1000\n",
            "2/2 [==============================] - 1s 740ms/step - loss: 0.0849 - mse: 0.0523 - val_loss: 0.1412 - val_mse: 0.0781 - lr: 5.0000e-04\n",
            "Epoch 69/1000\n",
            "2/2 [==============================] - 1s 712ms/step - loss: 0.0850 - mse: 0.0551 - val_loss: 0.1465 - val_mse: 0.0767 - lr: 5.0000e-04\n",
            "Epoch 70/1000\n",
            "2/2 [==============================] - 1s 733ms/step - loss: 0.0814 - mse: 0.0504 - val_loss: 0.1399 - val_mse: 0.0763 - lr: 5.0000e-04\n",
            "Epoch 71/1000\n",
            "2/2 [==============================] - 1s 711ms/step - loss: 0.0826 - mse: 0.0563 - val_loss: 0.1462 - val_mse: 0.0757 - lr: 5.0000e-04\n",
            "Epoch 72/1000\n",
            "2/2 [==============================] - 1s 704ms/step - loss: 0.0804 - mse: 0.0485 - val_loss: 0.1447 - val_mse: 0.0752 - lr: 5.0000e-04\n",
            "Epoch 73/1000\n",
            "2/2 [==============================] - 1s 731ms/step - loss: 0.0787 - mse: 0.0501 - val_loss: 0.1389 - val_mse: 0.0753 - lr: 5.0000e-04\n",
            "Epoch 74/1000\n",
            "2/2 [==============================] - 1s 712ms/step - loss: 0.0780 - mse: 0.0516 - val_loss: 0.1476 - val_mse: 0.0767 - lr: 5.0000e-04\n",
            "Epoch 75/1000\n",
            "2/2 [==============================] - 1s 734ms/step - loss: 0.0777 - mse: 0.0477 - val_loss: 0.1379 - val_mse: 0.0756 - lr: 5.0000e-04\n",
            "Epoch 76/1000\n",
            "2/2 [==============================] - 1s 716ms/step - loss: 0.0771 - mse: 0.0526 - val_loss: 0.1438 - val_mse: 0.0751 - lr: 5.0000e-04\n",
            "Epoch 77/1000\n",
            "2/2 [==============================] - 1s 720ms/step - loss: 0.0758 - mse: 0.0467 - val_loss: 0.1380 - val_mse: 0.0732 - lr: 5.0000e-04\n",
            "Epoch 78/1000\n",
            "2/2 [==============================] - 1s 722ms/step - loss: 0.0745 - mse: 0.0495 - val_loss: 0.1395 - val_mse: 0.0730 - lr: 5.0000e-04\n",
            "Epoch 79/1000\n",
            "2/2 [==============================] - 1s 712ms/step - loss: 0.0736 - mse: 0.0460 - val_loss: 0.1384 - val_mse: 0.0723 - lr: 5.0000e-04\n",
            "Epoch 80/1000\n",
            "2/2 [==============================] - 1s 705ms/step - loss: 0.0734 - mse: 0.0488 - val_loss: 0.1387 - val_mse: 0.0720 - lr: 5.0000e-04\n",
            "Epoch 81/1000\n",
            "2/2 [==============================] - 1s 716ms/step - loss: 0.0714 - mse: 0.0447 - val_loss: 0.1391 - val_mse: 0.0719 - lr: 5.0000e-04\n",
            "Epoch 82/1000\n",
            "2/2 [==============================] - 1s 738ms/step - loss: 0.0710 - mse: 0.0463 - val_loss: 0.1361 - val_mse: 0.0710 - lr: 5.0000e-04\n",
            "Epoch 83/1000\n",
            "2/2 [==============================] - 1s 710ms/step - loss: 0.0695 - mse: 0.0446 - val_loss: 0.1402 - val_mse: 0.0720 - lr: 5.0000e-04\n",
            "Epoch 84/1000\n",
            "2/2 [==============================] - 1s 737ms/step - loss: 0.0691 - mse: 0.0437 - val_loss: 0.1321 - val_mse: 0.0706 - lr: 5.0000e-04\n",
            "Epoch 85/1000\n",
            "2/2 [==============================] - 1s 707ms/step - loss: 0.0711 - mse: 0.0459 - val_loss: 0.1353 - val_mse: 0.0705 - lr: 5.0000e-04\n",
            "Epoch 86/1000\n",
            "2/2 [==============================] - 1s 707ms/step - loss: 0.0684 - mse: 0.0459 - val_loss: 0.1350 - val_mse: 0.0703 - lr: 5.0000e-04\n",
            "Epoch 87/1000\n",
            "2/2 [==============================] - 1s 709ms/step - loss: 0.0690 - mse: 0.0437 - val_loss: 0.1351 - val_mse: 0.0697 - lr: 5.0000e-04\n",
            "Epoch 88/1000\n",
            "2/2 [==============================] - 1s 711ms/step - loss: 0.0689 - mse: 0.0466 - val_loss: 0.1343 - val_mse: 0.0691 - lr: 5.0000e-04\n",
            "Epoch 89/1000\n",
            "2/2 [==============================] - 1s 704ms/step - loss: 0.0685 - mse: 0.0431 - val_loss: 0.1357 - val_mse: 0.0691 - lr: 5.0000e-04\n",
            "Epoch 90/1000\n",
            "2/2 [==============================] - 1s 867ms/step - loss: 0.0679 - mse: 0.0457 - val_loss: 0.1319 - val_mse: 0.0682 - lr: 5.0000e-04\n",
            "Epoch 91/1000\n",
            "2/2 [==============================] - 1s 719ms/step - loss: 0.0656 - mse: 0.0420 - val_loss: 0.1398 - val_mse: 0.0704 - lr: 5.0000e-04\n",
            "Epoch 92/1000\n",
            "2/2 [==============================] - 1s 750ms/step - loss: 0.0661 - mse: 0.0423 - val_loss: 0.1298 - val_mse: 0.0686 - lr: 5.0000e-04\n",
            "Epoch 93/1000\n",
            "2/2 [==============================] - 1s 713ms/step - loss: 0.0647 - mse: 0.0436 - val_loss: 0.1423 - val_mse: 0.0716 - lr: 5.0000e-04\n",
            "Epoch 94/1000\n",
            "2/2 [==============================] - 1s 741ms/step - loss: 0.0658 - mse: 0.0401 - val_loss: 0.1286 - val_mse: 0.0698 - lr: 5.0000e-04\n",
            "Epoch 95/1000\n",
            "2/2 [==============================] - 1s 710ms/step - loss: 0.0663 - mse: 0.0467 - val_loss: 0.1391 - val_mse: 0.0704 - lr: 5.0000e-04\n",
            "Epoch 96/1000\n",
            "2/2 [==============================] - 1s 712ms/step - loss: 0.0650 - mse: 0.0404 - val_loss: 0.1333 - val_mse: 0.0685 - lr: 5.0000e-04\n",
            "Epoch 97/1000\n",
            "2/2 [==============================] - 1s 713ms/step - loss: 0.0636 - mse: 0.0429 - val_loss: 0.1305 - val_mse: 0.0677 - lr: 5.0000e-04\n",
            "Epoch 98/1000\n",
            "2/2 [==============================] - 1s 716ms/step - loss: 0.0623 - mse: 0.0407 - val_loss: 0.1376 - val_mse: 0.0688 - lr: 5.0000e-04\n",
            "Epoch 99/1000\n",
            "2/2 [==============================] - 1s 751ms/step - loss: 0.0625 - mse: 0.0388 - val_loss: 0.1273 - val_mse: 0.0677 - lr: 5.0000e-04\n",
            "Epoch 100/1000\n",
            "2/2 [==============================] - 1s 715ms/step - loss: 0.0651 - mse: 0.0435 - val_loss: 0.1357 - val_mse: 0.0691 - lr: 5.0000e-04\n",
            "Epoch 101/1000\n",
            "2/2 [==============================] - 1s 727ms/step - loss: 0.0619 - mse: 0.0411 - val_loss: 0.1307 - val_mse: 0.0685 - lr: 5.0000e-04\n",
            "Epoch 102/1000\n",
            "2/2 [==============================] - 1s 721ms/step - loss: 0.0613 - mse: 0.0408 - val_loss: 0.1377 - val_mse: 0.0697 - lr: 5.0000e-04\n",
            "Epoch 103/1000\n",
            "2/2 [==============================] - 1s 734ms/step - loss: 0.0618 - mse: 0.0403 - val_loss: 0.1301 - val_mse: 0.0668 - lr: 5.0000e-04\n",
            "Epoch 104/1000\n",
            "2/2 [==============================] - 1s 711ms/step - loss: 0.0598 - mse: 0.0383 - val_loss: 0.1334 - val_mse: 0.0672 - lr: 5.0000e-04\n",
            "Epoch 105/1000\n",
            "2/2 [==============================] - 1s 717ms/step - loss: 0.0600 - mse: 0.0390 - val_loss: 0.1315 - val_mse: 0.0668 - lr: 5.0000e-04\n",
            "Epoch 106/1000\n",
            "2/2 [==============================] - 1s 719ms/step - loss: 0.0594 - mse: 0.0373 - val_loss: 0.1287 - val_mse: 0.0665 - lr: 5.0000e-04\n",
            "Epoch 107/1000\n",
            "2/2 [==============================] - 1s 713ms/step - loss: 0.0586 - mse: 0.0391 - val_loss: 0.1346 - val_mse: 0.0679 - lr: 5.0000e-04\n",
            "Epoch 108/1000\n",
            "2/2 [==============================] - 1s 739ms/step - loss: 0.0585 - mse: 0.0364 - val_loss: 0.1263 - val_mse: 0.0675 - lr: 5.0000e-04\n",
            "Epoch 109/1000\n",
            "2/2 [==============================] - 1s 718ms/step - loss: 0.0590 - mse: 0.0399 - val_loss: 0.1390 - val_mse: 0.0700 - lr: 5.0000e-04\n",
            "Epoch 110/1000\n",
            "2/2 [==============================] - 1s 712ms/step - loss: 0.0579 - mse: 0.0369 - val_loss: 0.1265 - val_mse: 0.0665 - lr: 5.0000e-04\n",
            "Epoch 111/1000\n",
            "2/2 [==============================] - 1s 708ms/step - loss: 0.0582 - mse: 0.0382 - val_loss: 0.1330 - val_mse: 0.0667 - lr: 5.0000e-04\n",
            "Epoch 112/1000\n",
            "2/2 [==============================] - 1s 718ms/step - loss: 0.0582 - mse: 0.0381 - val_loss: 0.1316 - val_mse: 0.0664 - lr: 5.0000e-04\n",
            "Epoch 113/1000\n",
            "2/2 [==============================] - 1s 716ms/step - loss: 0.0570 - mse: 0.0352 - val_loss: 0.1280 - val_mse: 0.0657 - lr: 5.0000e-04\n",
            "Epoch 114/1000\n",
            "2/2 [==============================] - 1s 706ms/step - loss: 0.0560 - mse: 0.0374 - val_loss: 0.1322 - val_mse: 0.0664 - lr: 5.0000e-04\n",
            "Epoch 115/1000\n",
            "2/2 [==============================] - 1s 708ms/step - loss: 0.0551 - mse: 0.0344 - val_loss: 0.1266 - val_mse: 0.0651 - lr: 5.0000e-04\n",
            "Epoch 116/1000\n",
            "2/2 [==============================] - 1s 867ms/step - loss: 0.0547 - mse: 0.0352 - val_loss: 0.1303 - val_mse: 0.0657 - lr: 5.0000e-04\n",
            "Epoch 117/1000\n",
            "2/2 [==============================] - 1s 712ms/step - loss: 0.0541 - mse: 0.0347 - val_loss: 0.1302 - val_mse: 0.0658 - lr: 5.0000e-04\n",
            "Epoch 118/1000\n",
            "2/2 [==============================] - 1s 703ms/step - loss: 0.0536 - mse: 0.0340 - val_loss: 0.1271 - val_mse: 0.0653 - lr: 5.0000e-04\n",
            "Epoch 119/1000\n",
            "2/2 [==============================] - 1s 729ms/step - loss: 0.0541 - mse: 0.0345 - val_loss: 0.1244 - val_mse: 0.0652 - lr: 5.0000e-04\n",
            "Epoch 120/1000\n",
            "2/2 [==============================] - 1s 704ms/step - loss: 0.0560 - mse: 0.0362 - val_loss: 0.1264 - val_mse: 0.0651 - lr: 5.0000e-04\n",
            "Epoch 121/1000\n",
            "2/2 [==============================] - 1s 707ms/step - loss: 0.0535 - mse: 0.0359 - val_loss: 0.1352 - val_mse: 0.0674 - lr: 5.0000e-04\n",
            "Epoch 122/1000\n",
            "2/2 [==============================] - 1s 711ms/step - loss: 0.0535 - mse: 0.0331 - val_loss: 0.1246 - val_mse: 0.0647 - lr: 5.0000e-04\n",
            "Epoch 123/1000\n",
            "2/2 [==============================] - 1s 722ms/step - loss: 0.0534 - mse: 0.0343 - val_loss: 0.1315 - val_mse: 0.0662 - lr: 5.0000e-04\n",
            "Epoch 124/1000\n",
            "2/2 [==============================] - 1s 712ms/step - loss: 0.0528 - mse: 0.0347 - val_loss: 0.1303 - val_mse: 0.0657 - lr: 5.0000e-04\n",
            "Epoch 125/1000\n",
            "2/2 [==============================] - 1s 711ms/step - loss: 0.0519 - mse: 0.0324 - val_loss: 0.1245 - val_mse: 0.0642 - lr: 5.0000e-04\n",
            "Epoch 126/1000\n",
            "2/2 [==============================] - 1s 706ms/step - loss: 0.0519 - mse: 0.0333 - val_loss: 0.1276 - val_mse: 0.0645 - lr: 5.0000e-04\n",
            "Epoch 127/1000\n",
            "2/2 [==============================] - 1s 705ms/step - loss: 0.0509 - mse: 0.0329 - val_loss: 0.1289 - val_mse: 0.0648 - lr: 5.0000e-04\n",
            "Epoch 128/1000\n",
            "2/2 [==============================] - 1s 857ms/step - loss: 0.0505 - mse: 0.0322 - val_loss: 0.1254 - val_mse: 0.0635 - lr: 5.0000e-04\n",
            "Epoch 129/1000\n",
            "2/2 [==============================] - 1s 735ms/step - loss: 0.0506 - mse: 0.0319 - val_loss: 0.1226 - val_mse: 0.0637 - lr: 5.0000e-04\n",
            "Epoch 130/1000\n",
            "2/2 [==============================] - 1s 711ms/step - loss: 0.0541 - mse: 0.0346 - val_loss: 0.1233 - val_mse: 0.0656 - lr: 5.0000e-04\n",
            "Epoch 131/1000\n",
            "2/2 [==============================] - 1s 708ms/step - loss: 0.0536 - mse: 0.0367 - val_loss: 0.1374 - val_mse: 0.0682 - lr: 5.0000e-04\n",
            "Epoch 132/1000\n",
            "2/2 [==============================] - 1s 701ms/step - loss: 0.0541 - mse: 0.0352 - val_loss: 0.1279 - val_mse: 0.0636 - lr: 5.0000e-04\n",
            "Epoch 133/1000\n",
            "2/2 [==============================] - 1s 717ms/step - loss: 0.0519 - mse: 0.0317 - val_loss: 0.1232 - val_mse: 0.0638 - lr: 5.0000e-04\n",
            "Epoch 134/1000\n",
            "2/2 [==============================] - 1s 714ms/step - loss: 0.0512 - mse: 0.0349 - val_loss: 0.1342 - val_mse: 0.0673 - lr: 5.0000e-04\n",
            "Epoch 135/1000\n",
            "2/2 [==============================] - 1s 715ms/step - loss: 0.0507 - mse: 0.0321 - val_loss: 0.1241 - val_mse: 0.0645 - lr: 5.0000e-04\n",
            "Epoch 136/1000\n",
            "2/2 [==============================] - 1s 706ms/step - loss: 0.0499 - mse: 0.0325 - val_loss: 0.1318 - val_mse: 0.0661 - lr: 5.0000e-04\n",
            "Epoch 137/1000\n",
            "2/2 [==============================] - 1s 706ms/step - loss: 0.0496 - mse: 0.0317 - val_loss: 0.1257 - val_mse: 0.0640 - lr: 5.0000e-04\n",
            "Epoch 138/1000\n",
            "2/2 [==============================] - 1s 720ms/step - loss: 0.0490 - mse: 0.0312 - val_loss: 0.1254 - val_mse: 0.0642 - lr: 5.0000e-04\n",
            "Epoch 139/1000\n",
            "2/2 [==============================] - 1s 713ms/step - loss: 0.0484 - mse: 0.0318 - val_loss: 0.1320 - val_mse: 0.0658 - lr: 5.0000e-04\n",
            "Epoch 140/1000\n",
            "2/2 [==============================] - 1s 710ms/step - loss: 0.0495 - mse: 0.0324 - val_loss: 0.1357 - val_mse: 0.0676 - lr: 5.0000e-04\n",
            "Epoch 141/1000\n",
            "2/2 [==============================] - 1s 709ms/step - loss: 0.0491 - mse: 0.0310 - val_loss: 0.1244 - val_mse: 0.0633 - lr: 5.0000e-04\n",
            "Epoch 142/1000\n",
            "2/2 [==============================] - 1s 717ms/step - loss: 0.0488 - mse: 0.0311 - val_loss: 0.1234 - val_mse: 0.0624 - lr: 5.0000e-04\n",
            "Epoch 143/1000\n",
            "2/2 [==============================] - 1s 718ms/step - loss: 0.0475 - mse: 0.0314 - val_loss: 0.1347 - val_mse: 0.0665 - lr: 5.0000e-04\n",
            "Epoch 144/1000\n",
            "2/2 [==============================] - 1s 710ms/step - loss: 0.0486 - mse: 0.0310 - val_loss: 0.1267 - val_mse: 0.0639 - lr: 5.0000e-04\n",
            "Epoch 145/1000\n",
            "2/2 [==============================] - 1s 712ms/step - loss: 0.0469 - mse: 0.0299 - val_loss: 0.1256 - val_mse: 0.0637 - lr: 5.0000e-04\n",
            "Epoch 146/1000\n",
            "2/2 [==============================] - 1s 711ms/step - loss: 0.0465 - mse: 0.0307 - val_loss: 0.1329 - val_mse: 0.0663 - lr: 5.0000e-04\n",
            "Epoch 147/1000\n",
            "2/2 [==============================] - 1s 711ms/step - loss: 0.0471 - mse: 0.0302 - val_loss: 0.1285 - val_mse: 0.0647 - lr: 5.0000e-04\n",
            "Epoch 148/1000\n",
            "2/2 [==============================] - 1s 751ms/step - loss: 0.0463 - mse: 0.0291 - val_loss: 0.1207 - val_mse: 0.0632 - lr: 5.0000e-04\n",
            "Epoch 149/1000\n",
            "2/2 [==============================] - 1s 706ms/step - loss: 0.0514 - mse: 0.0343 - val_loss: 0.1251 - val_mse: 0.0642 - lr: 5.0000e-04\n",
            "Epoch 150/1000\n",
            "2/2 [==============================] - 1s 719ms/step - loss: 0.0473 - mse: 0.0327 - val_loss: 0.1370 - val_mse: 0.0679 - lr: 5.0000e-04\n",
            "Epoch 151/1000\n",
            "2/2 [==============================] - 1s 708ms/step - loss: 0.0471 - mse: 0.0292 - val_loss: 0.1210 - val_mse: 0.0624 - lr: 5.0000e-04\n",
            "Epoch 152/1000\n",
            "2/2 [==============================] - 1s 715ms/step - loss: 0.0466 - mse: 0.0304 - val_loss: 0.1294 - val_mse: 0.0646 - lr: 5.0000e-04\n",
            "Epoch 153/1000\n",
            "2/2 [==============================] - 1s 719ms/step - loss: 0.0462 - mse: 0.0306 - val_loss: 0.1299 - val_mse: 0.0651 - lr: 5.0000e-04\n",
            "Epoch 154/1000\n",
            "2/2 [==============================] - 1s 709ms/step - loss: 0.0453 - mse: 0.0284 - val_loss: 0.1230 - val_mse: 0.0630 - lr: 5.0000e-04\n",
            "Epoch 155/1000\n",
            "2/2 [==============================] - 1s 861ms/step - loss: 0.0451 - mse: 0.0296 - val_loss: 0.1309 - val_mse: 0.0651 - lr: 5.0000e-04\n",
            "Epoch 156/1000\n",
            "2/2 [==============================] - 1s 708ms/step - loss: 0.0447 - mse: 0.0289 - val_loss: 0.1278 - val_mse: 0.0641 - lr: 5.0000e-04\n",
            "Epoch 157/1000\n",
            "2/2 [==============================] - 1s 715ms/step - loss: 0.0447 - mse: 0.0282 - val_loss: 0.1205 - val_mse: 0.0626 - lr: 5.0000e-04\n",
            "Epoch 158/1000\n",
            "2/2 [==============================] - 1s 858ms/step - loss: 0.0471 - mse: 0.0314 - val_loss: 0.1257 - val_mse: 0.0633 - lr: 5.0000e-04\n",
            "Epoch 159/1000\n",
            "2/2 [==============================] - 1s 705ms/step - loss: 0.0443 - mse: 0.0301 - val_loss: 0.1356 - val_mse: 0.0671 - lr: 5.0000e-04\n",
            "Epoch 160/1000\n",
            "2/2 [==============================] - 1s 710ms/step - loss: 0.0445 - mse: 0.0282 - val_loss: 0.1222 - val_mse: 0.0622 - lr: 5.0000e-04\n",
            "Epoch 161/1000\n",
            "2/2 [==============================] - 1s 709ms/step - loss: 0.0446 - mse: 0.0289 - val_loss: 0.1262 - val_mse: 0.0639 - lr: 5.0000e-04\n",
            "Epoch 162/1000\n",
            "2/2 [==============================] - 1s 708ms/step - loss: 0.0436 - mse: 0.0295 - val_loss: 0.1351 - val_mse: 0.0673 - lr: 5.0000e-04\n",
            "Epoch 163/1000\n",
            "2/2 [==============================] - 1s 720ms/step - loss: 0.0436 - mse: 0.0277 - val_loss: 0.1231 - val_mse: 0.0625 - lr: 5.0000e-04\n",
            "Epoch 164/1000\n",
            "2/2 [==============================] - 1s 716ms/step - loss: 0.0434 - mse: 0.0281 - val_loss: 0.1259 - val_mse: 0.0633 - lr: 5.0000e-04\n",
            "Epoch 165/1000\n",
            "2/2 [==============================] - 1s 719ms/step - loss: 0.0425 - mse: 0.0285 - val_loss: 0.1328 - val_mse: 0.0658 - lr: 5.0000e-04\n",
            "Epoch 166/1000\n",
            "2/2 [==============================] - 1s 712ms/step - loss: 0.0427 - mse: 0.0273 - val_loss: 0.1239 - val_mse: 0.0625 - lr: 5.0000e-04\n",
            "Epoch 167/1000\n",
            "2/2 [==============================] - 1s 721ms/step - loss: 0.0428 - mse: 0.0276 - val_loss: 0.1228 - val_mse: 0.0630 - lr: 5.0000e-04\n",
            "Epoch 168/1000\n",
            "2/2 [==============================] - 1s 721ms/step - loss: 0.0425 - mse: 0.0284 - val_loss: 0.1261 - val_mse: 0.0628 - lr: 5.0000e-04\n",
            "Epoch 169/1000\n",
            "2/2 [==============================] - 1s 740ms/step - loss: 0.0421 - mse: 0.0277 - val_loss: 0.1370 - val_mse: 0.0685 - lr: 5.0000e-04\n",
            "Epoch 170/1000\n",
            "2/2 [==============================] - 1s 711ms/step - loss: 0.0436 - mse: 0.0290 - val_loss: 0.1260 - val_mse: 0.0633 - lr: 5.0000e-04\n",
            "Epoch 171/1000\n",
            "2/2 [==============================] - 1s 744ms/step - loss: 0.0421 - mse: 0.0264 - val_loss: 0.1193 - val_mse: 0.0626 - lr: 5.0000e-04\n",
            "Epoch 172/1000\n",
            "2/2 [==============================] - 1s 723ms/step - loss: 0.0446 - mse: 0.0301 - val_loss: 0.1295 - val_mse: 0.0650 - lr: 5.0000e-04\n",
            "Epoch 173/1000\n",
            "2/2 [==============================] - 1s 711ms/step - loss: 0.0419 - mse: 0.0285 - val_loss: 0.1302 - val_mse: 0.0641 - lr: 5.0000e-04\n",
            "Epoch 174/1000\n",
            "2/2 [==============================] - 1s 710ms/step - loss: 0.0410 - mse: 0.0256 - val_loss: 0.1215 - val_mse: 0.0618 - lr: 5.0000e-04\n",
            "Epoch 175/1000\n",
            "2/2 [==============================] - 1s 726ms/step - loss: 0.0414 - mse: 0.0273 - val_loss: 0.1295 - val_mse: 0.0649 - lr: 5.0000e-04\n",
            "Epoch 176/1000\n",
            "2/2 [==============================] - 1s 715ms/step - loss: 0.0409 - mse: 0.0274 - val_loss: 0.1293 - val_mse: 0.0642 - lr: 5.0000e-04\n",
            "Epoch 177/1000\n",
            "2/2 [==============================] - 1s 720ms/step - loss: 0.0404 - mse: 0.0255 - val_loss: 0.1200 - val_mse: 0.0617 - lr: 5.0000e-04\n",
            "Epoch 178/1000\n",
            "2/2 [==============================] - 1s 721ms/step - loss: 0.0427 - mse: 0.0287 - val_loss: 0.1260 - val_mse: 0.0636 - lr: 5.0000e-04\n",
            "Epoch 179/1000\n",
            "2/2 [==============================] - 1s 711ms/step - loss: 0.0404 - mse: 0.0277 - val_loss: 0.1316 - val_mse: 0.0644 - lr: 5.0000e-04\n",
            "Epoch 180/1000\n",
            "2/2 [==============================] - 1s 714ms/step - loss: 0.0406 - mse: 0.0256 - val_loss: 0.1236 - val_mse: 0.0619 - lr: 5.0000e-04\n",
            "Epoch 181/1000\n",
            "2/2 [==============================] - 1s 717ms/step - loss: 0.0396 - mse: 0.0259 - val_loss: 0.1248 - val_mse: 0.0632 - lr: 5.0000e-04\n",
            "Epoch 182/1000\n",
            "2/2 [==============================] - 1s 723ms/step - loss: 0.0393 - mse: 0.0265 - val_loss: 0.1276 - val_mse: 0.0631 - lr: 5.0000e-04\n",
            "Epoch 183/1000\n",
            "2/2 [==============================] - 1s 718ms/step - loss: 0.0395 - mse: 0.0259 - val_loss: 0.1323 - val_mse: 0.0660 - lr: 5.0000e-04\n",
            "Epoch 184/1000\n",
            "2/2 [==============================] - 1s 718ms/step - loss: 0.0393 - mse: 0.0259 - val_loss: 0.1248 - val_mse: 0.0628 - lr: 5.0000e-04\n",
            "Epoch 185/1000\n",
            "2/2 [==============================] - 1s 711ms/step - loss: 0.0394 - mse: 0.0254 - val_loss: 0.1189 - val_mse: 0.0614 - lr: 5.0000e-04\n",
            "Epoch 186/1000\n",
            "2/2 [==============================] - 1s 706ms/step - loss: 0.0419 - mse: 0.0278 - val_loss: 0.1248 - val_mse: 0.0632 - lr: 5.0000e-04\n",
            "Epoch 187/1000\n",
            "2/2 [==============================] - 1s 714ms/step - loss: 0.0395 - mse: 0.0274 - val_loss: 0.1340 - val_mse: 0.0659 - lr: 5.0000e-04\n",
            "Epoch 188/1000\n",
            "2/2 [==============================] - 1s 719ms/step - loss: 0.0393 - mse: 0.0250 - val_loss: 0.1205 - val_mse: 0.0608 - lr: 5.0000e-04\n",
            "Epoch 189/1000\n",
            "2/2 [==============================] - 1s 719ms/step - loss: 0.0400 - mse: 0.0260 - val_loss: 0.1245 - val_mse: 0.0630 - lr: 5.0000e-04\n",
            "Epoch 190/1000\n",
            "2/2 [==============================] - 1s 713ms/step - loss: 0.0388 - mse: 0.0268 - val_loss: 0.1324 - val_mse: 0.0651 - lr: 5.0000e-04\n",
            "Epoch 191/1000\n",
            "2/2 [==============================] - 1s 727ms/step - loss: 0.0391 - mse: 0.0251 - val_loss: 0.1226 - val_mse: 0.0609 - lr: 5.0000e-04\n",
            "Epoch 192/1000\n",
            "2/2 [==============================] - 1s 715ms/step - loss: 0.0382 - mse: 0.0248 - val_loss: 0.1234 - val_mse: 0.0626 - lr: 5.0000e-04\n",
            "Epoch 193/1000\n",
            "2/2 [==============================] - 1s 720ms/step - loss: 0.0379 - mse: 0.0258 - val_loss: 0.1266 - val_mse: 0.0624 - lr: 5.0000e-04\n",
            "Epoch 194/1000\n",
            "2/2 [==============================] - 1s 720ms/step - loss: 0.0375 - mse: 0.0245 - val_loss: 0.1274 - val_mse: 0.0633 - lr: 5.0000e-04\n",
            "Epoch 195/1000\n",
            "2/2 [==============================] - 1s 873ms/step - loss: 0.0369 - mse: 0.0243 - val_loss: 0.1250 - val_mse: 0.0631 - lr: 5.0000e-04\n",
            "Epoch 196/1000\n",
            "2/2 [==============================] - 1s 710ms/step - loss: 0.0370 - mse: 0.0247 - val_loss: 0.1211 - val_mse: 0.0611 - lr: 5.0000e-04\n",
            "Epoch 197/1000\n",
            "2/2 [==============================] - 1s 710ms/step - loss: 0.0382 - mse: 0.0249 - val_loss: 0.1204 - val_mse: 0.0617 - lr: 5.0000e-04\n",
            "Epoch 198/1000\n",
            "2/2 [==============================] - 1s 717ms/step - loss: 0.0384 - mse: 0.0263 - val_loss: 0.1271 - val_mse: 0.0631 - lr: 5.0000e-04\n",
            "Epoch 199/1000\n",
            "2/2 [==============================] - 1s 717ms/step - loss: 0.0371 - mse: 0.0255 - val_loss: 0.1323 - val_mse: 0.0645 - lr: 5.0000e-04\n",
            "Epoch 200/1000\n",
            "2/2 [==============================] - 1s 713ms/step - loss: 0.0378 - mse: 0.0245 - val_loss: 0.1259 - val_mse: 0.0631 - lr: 5.0000e-04\n",
            "Epoch 201/1000\n",
            "2/2 [==============================] - 1s 710ms/step - loss: 0.0370 - mse: 0.0244 - val_loss: 0.1205 - val_mse: 0.0609 - lr: 5.0000e-04\n",
            "Epoch 202/1000\n",
            "2/2 [==============================] - 1s 727ms/step - loss: 0.0366 - mse: 0.0249 - val_loss: 0.1295 - val_mse: 0.0637 - lr: 5.0000e-04\n",
            "Epoch 203/1000\n",
            "2/2 [==============================] - 1s 756ms/step - loss: 0.0372 - mse: 0.0250 - val_loss: 0.1320 - val_mse: 0.0653 - lr: 5.0000e-04\n",
            "Epoch 204/1000\n",
            "2/2 [==============================] - 1s 711ms/step - loss: 0.0362 - mse: 0.0236 - val_loss: 0.1207 - val_mse: 0.0605 - lr: 5.0000e-04\n",
            "Epoch 205/1000\n",
            "2/2 [==============================] - 1s 709ms/step - loss: 0.0371 - mse: 0.0242 - val_loss: 0.1210 - val_mse: 0.0612 - lr: 5.0000e-04\n",
            "Epoch 206/1000\n",
            "2/2 [==============================] - 1s 720ms/step - loss: 0.0358 - mse: 0.0247 - val_loss: 0.1291 - val_mse: 0.0638 - lr: 5.0000e-04\n",
            "Epoch 207/1000\n",
            "2/2 [==============================] - 1s 721ms/step - loss: 0.0364 - mse: 0.0246 - val_loss: 0.1330 - val_mse: 0.0657 - lr: 5.0000e-04\n",
            "Epoch 208/1000\n",
            "2/2 [==============================] - 1s 721ms/step - loss: 0.0359 - mse: 0.0235 - val_loss: 0.1226 - val_mse: 0.0616 - lr: 5.0000e-04\n",
            "Epoch 209/1000\n",
            "2/2 [==============================] - 1s 715ms/step - loss: 0.0358 - mse: 0.0237 - val_loss: 0.1201 - val_mse: 0.0606 - lr: 5.0000e-04\n",
            "Epoch 210/1000\n",
            "2/2 [==============================] - 1s 713ms/step - loss: 0.0353 - mse: 0.0242 - val_loss: 0.1283 - val_mse: 0.0634 - lr: 5.0000e-04\n",
            "Epoch 211/1000\n",
            "2/2 [==============================] - 1s 718ms/step - loss: 0.0352 - mse: 0.0241 - val_loss: 0.1319 - val_mse: 0.0649 - lr: 5.0000e-04\n",
            "Epoch 212/1000\n",
            "2/2 [==============================] - 1s 714ms/step - loss: 0.0350 - mse: 0.0231 - val_loss: 0.1238 - val_mse: 0.0618 - lr: 5.0000e-04\n",
            "Epoch 213/1000\n",
            "2/2 [==============================] - 1s 723ms/step - loss: 0.0344 - mse: 0.0229 - val_loss: 0.1212 - val_mse: 0.0610 - lr: 5.0000e-04\n",
            "Epoch 214/1000\n",
            "2/2 [==============================] - 1s 714ms/step - loss: 0.0348 - mse: 0.0233 - val_loss: 0.1230 - val_mse: 0.0615 - lr: 5.0000e-04\n",
            "Epoch 215/1000\n",
            "2/2 [==============================] - 1s 720ms/step - loss: 0.0338 - mse: 0.0230 - val_loss: 0.1259 - val_mse: 0.0623 - lr: 5.0000e-04\n",
            "Epoch 216/1000\n",
            "2/2 [==============================] - 1s 720ms/step - loss: 0.0335 - mse: 0.0226 - val_loss: 0.1263 - val_mse: 0.0623 - lr: 5.0000e-04\n",
            "Epoch 217/1000\n",
            "2/2 [==============================] - 1s 720ms/step - loss: 0.0334 - mse: 0.0223 - val_loss: 0.1254 - val_mse: 0.0625 - lr: 5.0000e-04\n",
            "Epoch 218/1000\n",
            "2/2 [==============================] - 1s 718ms/step - loss: 0.0333 - mse: 0.0223 - val_loss: 0.1213 - val_mse: 0.0611 - lr: 5.0000e-04\n",
            "Epoch 219/1000\n",
            "2/2 [==============================] - 1s 899ms/step - loss: 0.0356 - mse: 0.0239 - val_loss: 0.1188 - val_mse: 0.0620 - lr: 5.0000e-04\n",
            "Epoch 220/1000\n",
            "2/2 [==============================] - 1s 709ms/step - loss: 0.0378 - mse: 0.0264 - val_loss: 0.1215 - val_mse: 0.0606 - lr: 5.0000e-04\n",
            "Epoch 221/1000\n",
            "2/2 [==============================] - 1s 710ms/step - loss: 0.0335 - mse: 0.0232 - val_loss: 0.1352 - val_mse: 0.0666 - lr: 5.0000e-04\n",
            "Epoch 222/1000\n",
            "2/2 [==============================] - 1s 707ms/step - loss: 0.0352 - mse: 0.0235 - val_loss: 0.1256 - val_mse: 0.0620 - lr: 5.0000e-04\n",
            "Epoch 223/1000\n",
            "2/2 [==============================] - 1s 715ms/step - loss: 0.0332 - mse: 0.0215 - val_loss: 0.1201 - val_mse: 0.0605 - lr: 5.0000e-04\n",
            "Epoch 224/1000\n",
            "2/2 [==============================] - 1s 714ms/step - loss: 0.0336 - mse: 0.0230 - val_loss: 0.1278 - val_mse: 0.0631 - lr: 5.0000e-04\n",
            "Epoch 225/1000\n",
            "2/2 [==============================] - 1s 707ms/step - loss: 0.0331 - mse: 0.0227 - val_loss: 0.1301 - val_mse: 0.0636 - lr: 5.0000e-04\n",
            "Epoch 226/1000\n",
            "2/2 [==============================] - 1s 715ms/step - loss: 0.0329 - mse: 0.0219 - val_loss: 0.1245 - val_mse: 0.0618 - lr: 5.0000e-04\n",
            "Epoch 227/1000\n",
            "2/2 [==============================] - 1s 722ms/step - loss: 0.0325 - mse: 0.0216 - val_loss: 0.1209 - val_mse: 0.0604 - lr: 5.0000e-04\n",
            "Epoch 228/1000\n",
            "2/2 [==============================] - 1s 721ms/step - loss: 0.0329 - mse: 0.0226 - val_loss: 0.1235 - val_mse: 0.0619 - lr: 5.0000e-04\n",
            "Epoch 229/1000\n",
            "2/2 [==============================] - 1s 709ms/step - loss: 0.0319 - mse: 0.0220 - val_loss: 0.1271 - val_mse: 0.0623 - lr: 5.0000e-04\n",
            "Epoch 230/1000\n",
            "2/2 [==============================] - 1s 721ms/step - loss: 0.0324 - mse: 0.0222 - val_loss: 0.1330 - val_mse: 0.0658 - lr: 5.0000e-04\n",
            "Epoch 231/1000\n",
            "2/2 [==============================] - 1s 713ms/step - loss: 0.0327 - mse: 0.0223 - val_loss: 0.1281 - val_mse: 0.0628 - lr: 5.0000e-04\n",
            "Epoch 232/1000\n",
            "2/2 [==============================] - 1s 714ms/step - loss: 0.0314 - mse: 0.0212 - val_loss: 0.1241 - val_mse: 0.0615 - lr: 5.0000e-04\n",
            "Epoch 233/1000\n",
            "2/2 [==============================] - 1s 720ms/step - loss: 0.0312 - mse: 0.0213 - val_loss: 0.1240 - val_mse: 0.0613 - lr: 5.0000e-04\n",
            "Epoch 234/1000\n",
            "2/2 [==============================] - 1s 710ms/step - loss: 0.0310 - mse: 0.0212 - val_loss: 0.1250 - val_mse: 0.0619 - lr: 5.0000e-04\n",
            "Epoch 235/1000\n",
            "2/2 [==============================] - 1s 712ms/step - loss: 0.0309 - mse: 0.0210 - val_loss: 0.1238 - val_mse: 0.0611 - lr: 5.0000e-04\n",
            "Epoch 236/1000\n",
            "2/2 [==============================] - 1s 734ms/step - loss: 0.0307 - mse: 0.0210 - val_loss: 0.1244 - val_mse: 0.0615 - lr: 5.0000e-04\n",
            "Epoch 237/1000\n",
            "2/2 [==============================] - 1s 725ms/step - loss: 0.0305 - mse: 0.0208 - val_loss: 0.1238 - val_mse: 0.0615 - lr: 5.0000e-04\n",
            "Epoch 238/1000\n",
            "2/2 [==============================] - 1s 724ms/step - loss: 0.0306 - mse: 0.0210 - val_loss: 0.1218 - val_mse: 0.0607 - lr: 5.0000e-04\n",
            "Epoch 239/1000\n",
            "2/2 [==============================] - 1s 732ms/step - loss: 0.0314 - mse: 0.0215 - val_loss: 0.1198 - val_mse: 0.0611 - lr: 5.0000e-04\n",
            "Epoch 240/1000\n",
            "2/2 [==============================] - 1s 710ms/step - loss: 0.0361 - mse: 0.0250 - val_loss: 0.1185 - val_mse: 0.0626 - lr: 5.0000e-04\n",
            "Epoch 241/1000\n",
            "2/2 [==============================] - 1s 709ms/step - loss: 0.0394 - mse: 0.0276 - val_loss: 0.1212 - val_mse: 0.0593 - lr: 5.0000e-04\n",
            "Epoch 242/1000\n",
            "2/2 [==============================] - 1s 710ms/step - loss: 0.0323 - mse: 0.0230 - val_loss: 0.1374 - val_mse: 0.0682 - lr: 5.0000e-04\n",
            "Epoch 243/1000\n",
            "2/2 [==============================] - 1s 712ms/step - loss: 0.0329 - mse: 0.0217 - val_loss: 0.1189 - val_mse: 0.0594 - lr: 5.0000e-04\n",
            "Epoch 244/1000\n",
            "2/2 [==============================] - 1s 727ms/step - loss: 0.0333 - mse: 0.0225 - val_loss: 0.1286 - val_mse: 0.0640 - lr: 5.0000e-04\n",
            "Epoch 245/1000\n",
            "2/2 [==============================] - 1s 714ms/step - loss: 0.0324 - mse: 0.0231 - val_loss: 0.1305 - val_mse: 0.0621 - lr: 5.0000e-04\n",
            "Epoch 246/1000\n",
            "2/2 [==============================] - 1s 716ms/step - loss: 0.0322 - mse: 0.0201 - val_loss: 0.1202 - val_mse: 0.0611 - lr: 5.0000e-04\n",
            "Epoch 247/1000\n",
            "2/2 [==============================] - 1s 716ms/step - loss: 0.0333 - mse: 0.0235 - val_loss: 0.1311 - val_mse: 0.0644 - lr: 5.0000e-04\n",
            "Epoch 248/1000\n",
            "2/2 [==============================] - 1s 707ms/step - loss: 0.0327 - mse: 0.0231 - val_loss: 0.1286 - val_mse: 0.0625 - lr: 5.0000e-04\n",
            "Epoch 249/1000\n",
            "2/2 [==============================] - 1s 719ms/step - loss: 0.0306 - mse: 0.0201 - val_loss: 0.1222 - val_mse: 0.0623 - lr: 5.0000e-04\n",
            "Epoch 250/1000\n",
            "2/2 [==============================] - 1s 716ms/step - loss: 0.0311 - mse: 0.0220 - val_loss: 0.1321 - val_mse: 0.0638 - lr: 5.0000e-04\n",
            "Epoch 251/1000\n",
            "2/2 [==============================] - 1s 715ms/step - loss: 0.0313 - mse: 0.0212 - val_loss: 0.1248 - val_mse: 0.0620 - lr: 5.0000e-04\n",
            "Epoch 252/1000\n",
            "2/2 [==============================] - 1s 713ms/step - loss: 0.0307 - mse: 0.0209 - val_loss: 0.1206 - val_mse: 0.0597 - lr: 5.0000e-04\n",
            "Epoch 253/1000\n",
            "2/2 [==============================] - 1s 714ms/step - loss: 0.0297 - mse: 0.0211 - val_loss: 0.1332 - val_mse: 0.0653 - lr: 5.0000e-04\n",
            "Epoch 254/1000\n",
            "2/2 [==============================] - 1s 709ms/step - loss: 0.0308 - mse: 0.0213 - val_loss: 0.1252 - val_mse: 0.0620 - lr: 5.0000e-04\n",
            "Epoch 255/1000\n",
            "2/2 [==============================] - 1s 723ms/step - loss: 0.0294 - mse: 0.0197 - val_loss: 0.1220 - val_mse: 0.0605 - lr: 5.0000e-04\n",
            "Epoch 256/1000\n",
            "2/2 [==============================] - 1s 875ms/step - loss: 0.0290 - mse: 0.0203 - val_loss: 0.1287 - val_mse: 0.0637 - lr: 5.0000e-04\n",
            "Epoch 257/1000\n",
            "2/2 [==============================] - 1s 726ms/step - loss: 0.0290 - mse: 0.0202 - val_loss: 0.1253 - val_mse: 0.0611 - lr: 5.0000e-04\n",
            "Epoch 258/1000\n",
            "2/2 [==============================] - 1s 716ms/step - loss: 0.0286 - mse: 0.0194 - val_loss: 0.1254 - val_mse: 0.0625 - lr: 5.0000e-04\n",
            "Epoch 259/1000\n",
            "2/2 [==============================] - 1s 709ms/step - loss: 0.0284 - mse: 0.0198 - val_loss: 0.1235 - val_mse: 0.0608 - lr: 5.0000e-04\n",
            "Epoch 260/1000\n",
            "2/2 [==============================] - 1s 710ms/step - loss: 0.0282 - mse: 0.0195 - val_loss: 0.1271 - val_mse: 0.0628 - lr: 5.0000e-04\n",
            "Epoch 261/1000\n",
            "2/2 [==============================] - 1s 715ms/step - loss: 0.0281 - mse: 0.0196 - val_loss: 0.1256 - val_mse: 0.0616 - lr: 5.0000e-04\n",
            "Epoch 262/1000\n",
            "2/2 [==============================] - 1s 714ms/step - loss: 0.0278 - mse: 0.0192 - val_loss: 0.1266 - val_mse: 0.0627 - lr: 5.0000e-04\n",
            "Epoch 263/1000\n",
            "2/2 [==============================] - 1s 715ms/step - loss: 0.0277 - mse: 0.0194 - val_loss: 0.1257 - val_mse: 0.0619 - lr: 5.0000e-04\n",
            "Epoch 264/1000\n",
            "2/2 [==============================] - 1s 714ms/step - loss: 0.0276 - mse: 0.0190 - val_loss: 0.1243 - val_mse: 0.0618 - lr: 5.0000e-04\n",
            "Epoch 265/1000\n",
            "2/2 [==============================] - 1s 717ms/step - loss: 0.0279 - mse: 0.0194 - val_loss: 0.1228 - val_mse: 0.0609 - lr: 5.0000e-04\n",
            "Epoch 266/1000\n",
            "2/2 [==============================] - 1s 723ms/step - loss: 0.0277 - mse: 0.0194 - val_loss: 0.1257 - val_mse: 0.0621 - lr: 5.0000e-04\n",
            "Epoch 267/1000\n",
            "2/2 [==============================] - 1s 722ms/step - loss: 0.0272 - mse: 0.0192 - val_loss: 0.1283 - val_mse: 0.0628 - lr: 5.0000e-04\n",
            "Epoch 268/1000\n",
            "2/2 [==============================] - 1s 713ms/step - loss: 0.0273 - mse: 0.0191 - val_loss: 0.1278 - val_mse: 0.0629 - lr: 5.0000e-04\n",
            "Epoch 269/1000\n",
            "2/2 [==============================] - 1s 717ms/step - loss: 0.0270 - mse: 0.0189 - val_loss: 0.1252 - val_mse: 0.0614 - lr: 5.0000e-04\n",
            "Epoch 270/1000\n",
            "2/2 [==============================] - 1s 743ms/step - loss: 0.0268 - mse: 0.0187 - val_loss: 0.1252 - val_mse: 0.0621 - lr: 5.0000e-04\n",
            "Epoch 271/1000\n",
            "2/2 [==============================] - 1s 728ms/step - loss: 0.0268 - mse: 0.0187 - val_loss: 0.1236 - val_mse: 0.0610 - lr: 5.0000e-04\n",
            "Epoch 272/1000\n",
            "2/2 [==============================] - 1s 719ms/step - loss: 0.0268 - mse: 0.0187 - val_loss: 0.1224 - val_mse: 0.0609 - lr: 5.0000e-04\n",
            "Epoch 273/1000\n",
            "2/2 [==============================] - 1s 720ms/step - loss: 0.0270 - mse: 0.0189 - val_loss: 0.1226 - val_mse: 0.0610 - lr: 5.0000e-04\n",
            "Epoch 274/1000\n",
            "2/2 [==============================] - 1s 716ms/step - loss: 0.0269 - mse: 0.0189 - val_loss: 0.1237 - val_mse: 0.0611 - lr: 5.0000e-04\n",
            "Epoch 275/1000\n",
            "2/2 [==============================] - 1s 873ms/step - loss: 0.0266 - mse: 0.0185 - val_loss: 0.1236 - val_mse: 0.0616 - lr: 5.0000e-04\n",
            "Epoch 276/1000\n",
            "2/2 [==============================] - 1s 712ms/step - loss: 0.0274 - mse: 0.0189 - val_loss: 0.1198 - val_mse: 0.0607 - lr: 5.0000e-04\n",
            "Epoch 277/1000\n",
            "2/2 [==============================] - 1s 714ms/step - loss: 0.0296 - mse: 0.0211 - val_loss: 0.1207 - val_mse: 0.0607 - lr: 5.0000e-04\n",
            "Epoch 278/1000\n",
            "2/2 [==============================] - 1s 710ms/step - loss: 0.0276 - mse: 0.0196 - val_loss: 0.1264 - val_mse: 0.0619 - lr: 5.0000e-04\n",
            "Epoch 279/1000\n",
            "2/2 [==============================] - 1s 720ms/step - loss: 0.0265 - mse: 0.0190 - val_loss: 0.1348 - val_mse: 0.0655 - lr: 5.0000e-04\n",
            "Epoch 280/1000\n",
            "2/2 [==============================] - 1s 720ms/step - loss: 0.0283 - mse: 0.0196 - val_loss: 0.1313 - val_mse: 0.0647 - lr: 5.0000e-04\n",
            "Epoch 281/1000\n",
            "2/2 [==============================] - 1s 727ms/step - loss: 0.0263 - mse: 0.0183 - val_loss: 0.1209 - val_mse: 0.0595 - lr: 5.0000e-04\n",
            "Epoch 282/1000\n",
            "2/2 [==============================] - 1s 721ms/step - loss: 0.0271 - mse: 0.0189 - val_loss: 0.1237 - val_mse: 0.0620 - lr: 5.0000e-04\n",
            "Epoch 283/1000\n",
            "2/2 [==============================] - 1s 714ms/step - loss: 0.0262 - mse: 0.0189 - val_loss: 0.1273 - val_mse: 0.0614 - lr: 5.0000e-04\n",
            "Epoch 284/1000\n",
            "2/2 [==============================] - 1s 711ms/step - loss: 0.0269 - mse: 0.0189 - val_loss: 0.1368 - val_mse: 0.0675 - lr: 5.0000e-04\n",
            "Epoch 285/1000\n",
            "2/2 [==============================] - 1s 723ms/step - loss: 0.0274 - mse: 0.0195 - val_loss: 0.1263 - val_mse: 0.0616 - lr: 5.0000e-04\n",
            "Epoch 286/1000\n",
            "2/2 [==============================] - 1s 720ms/step - loss: 0.0255 - mse: 0.0176 - val_loss: 0.1218 - val_mse: 0.0607 - lr: 5.0000e-04\n",
            "Epoch 287/1000\n",
            "2/2 [==============================] - 1s 729ms/step - loss: 0.0260 - mse: 0.0182 - val_loss: 0.1241 - val_mse: 0.0613 - lr: 5.0000e-04\n",
            "Epoch 288/1000\n",
            "2/2 [==============================] - 1s 713ms/step - loss: 0.0252 - mse: 0.0181 - val_loss: 0.1278 - val_mse: 0.0623 - lr: 5.0000e-04\n",
            "Epoch 289/1000\n",
            "2/2 [==============================] - 1s 713ms/step - loss: 0.0254 - mse: 0.0179 - val_loss: 0.1318 - val_mse: 0.0653 - lr: 5.0000e-04\n",
            "Epoch 290/1000\n",
            "2/2 [==============================] - 1s 711ms/step - loss: 0.0254 - mse: 0.0181 - val_loss: 0.1258 - val_mse: 0.0608 - lr: 5.0000e-04\n",
            "Epoch 291/1000\n",
            "2/2 [==============================] - 1s 715ms/step - loss: 0.0249 - mse: 0.0173 - val_loss: 0.1271 - val_mse: 0.0635 - lr: 5.0000e-04\n",
            "Epoch 292/1000\n",
            "2/2 [==============================] - 1s 722ms/step - loss: 0.0251 - mse: 0.0178 - val_loss: 0.1207 - val_mse: 0.0596 - lr: 5.0000e-04\n",
            "Epoch 293/1000\n",
            "2/2 [==============================] - 1s 719ms/step - loss: 0.0255 - mse: 0.0184 - val_loss: 0.1265 - val_mse: 0.0633 - lr: 5.0000e-04\n",
            "Epoch 294/1000\n",
            "2/2 [==============================] - 1s 720ms/step - loss: 0.0247 - mse: 0.0178 - val_loss: 0.1301 - val_mse: 0.0629 - lr: 5.0000e-04\n",
            "Epoch 295/1000\n",
            "2/2 [==============================] - 1s 717ms/step - loss: 0.0252 - mse: 0.0179 - val_loss: 0.1324 - val_mse: 0.0654 - lr: 5.0000e-04\n",
            "Epoch 296/1000\n",
            "2/2 [==============================] - 1s 720ms/step - loss: 0.0245 - mse: 0.0173 - val_loss: 0.1222 - val_mse: 0.0601 - lr: 5.0000e-04\n",
            "Epoch 297/1000\n",
            "2/2 [==============================] - 1s 719ms/step - loss: 0.0250 - mse: 0.0176 - val_loss: 0.1220 - val_mse: 0.0614 - lr: 5.0000e-04\n",
            "Epoch 298/1000\n",
            "2/2 [==============================] - 1s 709ms/step - loss: 0.0260 - mse: 0.0185 - val_loss: 0.1222 - val_mse: 0.0601 - lr: 5.0000e-04\n",
            "Epoch 299/1000\n",
            "2/2 [==============================] - 1s 710ms/step - loss: 0.0243 - mse: 0.0175 - val_loss: 0.1274 - val_mse: 0.0630 - lr: 5.0000e-04\n",
            "Epoch 300/1000\n",
            "2/2 [==============================] - 1s 722ms/step - loss: 0.0239 - mse: 0.0171 - val_loss: 0.1271 - val_mse: 0.0620 - lr: 5.0000e-04\n",
            "Epoch 301/1000\n",
            "2/2 [==============================] - 1s 715ms/step - loss: 0.0238 - mse: 0.0169 - val_loss: 0.1285 - val_mse: 0.0634 - lr: 5.0000e-04\n",
            "Epoch 302/1000\n",
            "2/2 [==============================] - 1s 718ms/step - loss: 0.0237 - mse: 0.0168 - val_loss: 0.1244 - val_mse: 0.0612 - lr: 5.0000e-04\n",
            "Epoch 303/1000\n",
            "2/2 [==============================] - 1s 896ms/step - loss: 0.0235 - mse: 0.0170 - val_loss: 0.1258 - val_mse: 0.0620 - lr: 5.0000e-04\n",
            "Epoch 304/1000\n",
            "2/2 [==============================] - 1s 714ms/step - loss: 0.0233 - mse: 0.0168 - val_loss: 0.1298 - val_mse: 0.0638 - lr: 5.0000e-04\n",
            "Epoch 305/1000\n",
            "2/2 [==============================] - 1s 723ms/step - loss: 0.0237 - mse: 0.0173 - val_loss: 0.1335 - val_mse: 0.0649 - lr: 5.0000e-04\n",
            "Epoch 306/1000\n",
            "2/2 [==============================] - 1s 714ms/step - loss: 0.0255 - mse: 0.0185 - val_loss: 0.1433 - val_mse: 0.0713 - lr: 5.0000e-04\n",
            "Epoch 307/1000\n",
            "2/2 [==============================] - 1s 710ms/step - loss: 0.0305 - mse: 0.0221 - val_loss: 0.1398 - val_mse: 0.0679 - lr: 5.0000e-04\n",
            "Epoch 308/1000\n",
            "2/2 [==============================] - 1s 725ms/step - loss: 0.0261 - mse: 0.0175 - val_loss: 0.1209 - val_mse: 0.0607 - lr: 5.0000e-04\n",
            "Epoch 309/1000\n",
            "2/2 [==============================] - 1s 710ms/step - loss: 0.0266 - mse: 0.0191 - val_loss: 0.1219 - val_mse: 0.0597 - lr: 5.0000e-04\n",
            "Epoch 310/1000\n",
            "2/2 [==============================] - 1s 720ms/step - loss: 0.0243 - mse: 0.0179 - val_loss: 0.1382 - val_mse: 0.0678 - lr: 5.0000e-04\n",
            "Epoch 311/1000\n",
            "2/2 [==============================] - 1s 726ms/step - loss: 0.0256 - mse: 0.0179 - val_loss: 0.1218 - val_mse: 0.0599 - lr: 5.0000e-04\n",
            "Epoch 312/1000\n",
            "2/2 [==============================] - 1s 726ms/step - loss: 0.0254 - mse: 0.0172 - val_loss: 0.1215 - val_mse: 0.0604 - lr: 5.0000e-04\n",
            "Epoch 313/1000\n",
            "2/2 [==============================] - 1s 719ms/step - loss: 0.0240 - mse: 0.0178 - val_loss: 0.1316 - val_mse: 0.0646 - lr: 5.0000e-04\n",
            "Epoch 314/1000\n",
            "2/2 [==============================] - 1s 717ms/step - loss: 0.0240 - mse: 0.0171 - val_loss: 0.1269 - val_mse: 0.0621 - lr: 5.0000e-04\n",
            "Epoch 315/1000\n",
            "2/2 [==============================] - 1s 717ms/step - loss: 0.0230 - mse: 0.0163 - val_loss: 0.1224 - val_mse: 0.0606 - lr: 5.0000e-04\n",
            "Epoch 316/1000\n",
            "2/2 [==============================] - 1s 719ms/step - loss: 0.0231 - mse: 0.0167 - val_loss: 0.1273 - val_mse: 0.0625 - lr: 5.0000e-04\n",
            "Epoch 317/1000\n",
            "2/2 [==============================] - 1s 719ms/step - loss: 0.0226 - mse: 0.0165 - val_loss: 0.1303 - val_mse: 0.0638 - lr: 5.0000e-04\n",
            "Epoch 318/1000\n",
            "2/2 [==============================] - 1s 710ms/step - loss: 0.0225 - mse: 0.0159 - val_loss: 0.1239 - val_mse: 0.0614 - lr: 5.0000e-04\n",
            "Epoch 319/1000\n",
            "2/2 [==============================] - 1s 715ms/step - loss: 0.0226 - mse: 0.0163 - val_loss: 0.1251 - val_mse: 0.0617 - lr: 5.0000e-04\n",
            "Epoch 320/1000\n",
            "2/2 [==============================] - 1s 719ms/step - loss: 0.0222 - mse: 0.0163 - val_loss: 0.1304 - val_mse: 0.0640 - lr: 5.0000e-04\n",
            "Epoch 321/1000\n",
            "2/2 [==============================] - 1s 712ms/step - loss: 0.0223 - mse: 0.0161 - val_loss: 0.1263 - val_mse: 0.0620 - lr: 5.0000e-04\n",
            "Epoch 322/1000\n",
            "2/2 [==============================] - 1s 721ms/step - loss: 0.0219 - mse: 0.0156 - val_loss: 0.1246 - val_mse: 0.0617 - lr: 5.0000e-04\n",
            "Epoch 323/1000\n",
            "2/2 [==============================] - 1s 874ms/step - loss: 0.0219 - mse: 0.0161 - val_loss: 0.1266 - val_mse: 0.0623 - lr: 5.0000e-04\n",
            "Epoch 324/1000\n",
            "2/2 [==============================] - 1s 707ms/step - loss: 0.0216 - mse: 0.0158 - val_loss: 0.1303 - val_mse: 0.0642 - lr: 5.0000e-04\n",
            "Epoch 325/1000\n",
            "2/2 [==============================] - 1s 709ms/step - loss: 0.0216 - mse: 0.0157 - val_loss: 0.1265 - val_mse: 0.0621 - lr: 5.0000e-04\n",
            "Epoch 326/1000\n",
            "2/2 [==============================] - 1s 716ms/step - loss: 0.0213 - mse: 0.0154 - val_loss: 0.1259 - val_mse: 0.0622 - lr: 5.0000e-04\n",
            "Epoch 327/1000\n",
            "2/2 [==============================] - 1s 711ms/step - loss: 0.0213 - mse: 0.0155 - val_loss: 0.1257 - val_mse: 0.0618 - lr: 5.0000e-04\n",
            "Epoch 328/1000\n",
            "2/2 [==============================] - 1s 715ms/step - loss: 0.0211 - mse: 0.0153 - val_loss: 0.1265 - val_mse: 0.0627 - lr: 5.0000e-04\n",
            "Epoch 329/1000\n",
            "2/2 [==============================] - 1s 722ms/step - loss: 0.0211 - mse: 0.0153 - val_loss: 0.1256 - val_mse: 0.0618 - lr: 5.0000e-04\n",
            "Epoch 330/1000\n",
            "2/2 [==============================] - 1s 718ms/step - loss: 0.0209 - mse: 0.0153 - val_loss: 0.1256 - val_mse: 0.0621 - lr: 5.0000e-04\n",
            "Epoch 331/1000\n",
            "2/2 [==============================] - 1s 726ms/step - loss: 0.0208 - mse: 0.0152 - val_loss: 0.1256 - val_mse: 0.0622 - lr: 5.0000e-04\n",
            "Epoch 332/1000\n",
            "2/2 [==============================] - 1s 713ms/step - loss: 0.0208 - mse: 0.0153 - val_loss: 0.1246 - val_mse: 0.0616 - lr: 5.0000e-04\n",
            "Epoch 333/1000\n",
            "2/2 [==============================] - 1s 709ms/step - loss: 0.0208 - mse: 0.0152 - val_loss: 0.1244 - val_mse: 0.0616 - lr: 5.0000e-04\n",
            "Epoch 334/1000\n",
            "2/2 [==============================] - 1s 719ms/step - loss: 0.0213 - mse: 0.0154 - val_loss: 0.1220 - val_mse: 0.0612 - lr: 5.0000e-04\n",
            "Epoch 335/1000\n",
            "2/2 [==============================] - 1s 714ms/step - loss: 0.0230 - mse: 0.0170 - val_loss: 0.1201 - val_mse: 0.0606 - lr: 5.0000e-04\n",
            "Epoch 336/1000\n",
            "2/2 [==============================] - 1s 714ms/step - loss: 0.0243 - mse: 0.0176 - val_loss: 0.1221 - val_mse: 0.0614 - lr: 5.0000e-04\n",
            "Epoch 337/1000\n",
            "2/2 [==============================] - 1s 740ms/step - loss: 0.0217 - mse: 0.0161 - val_loss: 0.1308 - val_mse: 0.0634 - lr: 5.0000e-04\n",
            "Epoch 338/1000\n",
            "2/2 [==============================] - 1s 718ms/step - loss: 0.0221 - mse: 0.0163 - val_loss: 0.1373 - val_mse: 0.0677 - lr: 5.0000e-04\n",
            "Epoch 339/1000\n",
            "2/2 [==============================] - 1s 714ms/step - loss: 0.0218 - mse: 0.0157 - val_loss: 0.1219 - val_mse: 0.0598 - lr: 5.0000e-04\n",
            "Epoch 340/1000\n",
            "2/2 [==============================] - 1s 717ms/step - loss: 0.0222 - mse: 0.0158 - val_loss: 0.1230 - val_mse: 0.0618 - lr: 5.0000e-04\n",
            "Epoch 341/1000\n",
            "2/2 [==============================] - 1s 715ms/step - loss: 0.0220 - mse: 0.0160 - val_loss: 0.1288 - val_mse: 0.0620 - lr: 5.0000e-04\n",
            "Epoch 342/1000\n",
            "2/2 [==============================] - 1s 714ms/step - loss: 0.0210 - mse: 0.0153 - val_loss: 0.1330 - val_mse: 0.0660 - lr: 5.0000e-04\n",
            "Epoch 343/1000\n",
            "2/2 [==============================] - 1s 711ms/step - loss: 0.0205 - mse: 0.0148 - val_loss: 0.1233 - val_mse: 0.0602 - lr: 5.0000e-04\n",
            "Epoch 344/1000\n",
            "2/2 [==============================] - 1s 713ms/step - loss: 0.0207 - mse: 0.0149 - val_loss: 0.1253 - val_mse: 0.0626 - lr: 5.0000e-04\n",
            "Epoch 345/1000\n",
            "2/2 [==============================] - 1s 713ms/step - loss: 0.0205 - mse: 0.0151 - val_loss: 0.1273 - val_mse: 0.0618 - lr: 5.0000e-04\n",
            "Epoch 346/1000\n",
            "2/2 [==============================] - 1s 712ms/step - loss: 0.0199 - mse: 0.0146 - val_loss: 0.1298 - val_mse: 0.0645 - lr: 5.0000e-04\n",
            "Epoch 347/1000\n",
            "2/2 [==============================] - 1s 710ms/step - loss: 0.0198 - mse: 0.0145 - val_loss: 0.1256 - val_mse: 0.0615 - lr: 5.0000e-04\n",
            "Epoch 348/1000\n",
            "2/2 [==============================] - 1s 859ms/step - loss: 0.0197 - mse: 0.0145 - val_loss: 0.1277 - val_mse: 0.0636 - lr: 5.0000e-04\n",
            "Epoch 349/1000\n",
            "2/2 [==============================] - 1s 724ms/step - loss: 0.0196 - mse: 0.0145 - val_loss: 0.1275 - val_mse: 0.0621 - lr: 5.0000e-04\n",
            "Epoch 350/1000\n",
            "2/2 [==============================] - 1s 708ms/step - loss: 0.0194 - mse: 0.0143 - val_loss: 0.1301 - val_mse: 0.0641 - lr: 5.0000e-04\n",
            "Epoch 351/1000\n",
            "2/2 [==============================] - 1s 724ms/step - loss: 0.0193 - mse: 0.0143 - val_loss: 0.1286 - val_mse: 0.0631 - lr: 5.0000e-04\n",
            "Epoch 352/1000\n",
            "2/2 [==============================] - 1s 862ms/step - loss: 0.0191 - mse: 0.0141 - val_loss: 0.1273 - val_mse: 0.0627 - lr: 5.0000e-04\n",
            "Epoch 353/1000\n",
            "2/2 [==============================] - 1s 721ms/step - loss: 0.0191 - mse: 0.0139 - val_loss: 0.1253 - val_mse: 0.0622 - lr: 5.0000e-04\n",
            "Epoch 354/1000\n",
            "2/2 [==============================] - 1s 720ms/step - loss: 0.0194 - mse: 0.0145 - val_loss: 0.1247 - val_mse: 0.0616 - lr: 5.0000e-04\n",
            "Epoch 355/1000\n",
            "2/2 [==============================] - 1s 723ms/step - loss: 0.0192 - mse: 0.0142 - val_loss: 0.1266 - val_mse: 0.0626 - lr: 5.0000e-04\n",
            "Epoch 356/1000\n",
            "2/2 [==============================] - 1s 871ms/step - loss: 0.0188 - mse: 0.0140 - val_loss: 0.1279 - val_mse: 0.0625 - lr: 5.0000e-04\n",
            "Epoch 357/1000\n",
            "2/2 [==============================] - 1s 724ms/step - loss: 0.0187 - mse: 0.0139 - val_loss: 0.1314 - val_mse: 0.0649 - lr: 5.0000e-04\n",
            "Epoch 358/1000\n",
            "2/2 [==============================] - 1s 717ms/step - loss: 0.0187 - mse: 0.0139 - val_loss: 0.1285 - val_mse: 0.0628 - lr: 5.0000e-04\n",
            "Epoch 359/1000\n",
            "2/2 [==============================] - 1s 711ms/step - loss: 0.0184 - mse: 0.0136 - val_loss: 0.1283 - val_mse: 0.0633 - lr: 5.0000e-04\n",
            "Epoch 360/1000\n",
            "2/2 [==============================] - 1s 721ms/step - loss: 0.0183 - mse: 0.0136 - val_loss: 0.1268 - val_mse: 0.0622 - lr: 5.0000e-04\n",
            "Epoch 361/1000\n",
            "2/2 [==============================] - 1s 717ms/step - loss: 0.0183 - mse: 0.0136 - val_loss: 0.1261 - val_mse: 0.0623 - lr: 5.0000e-04\n",
            "Epoch 362/1000\n",
            "2/2 [==============================] - 1s 717ms/step - loss: 0.0185 - mse: 0.0136 - val_loss: 0.1238 - val_mse: 0.0618 - lr: 5.0000e-04\n",
            "Epoch 363/1000\n",
            "2/2 [==============================] - 1s 724ms/step - loss: 0.0205 - mse: 0.0151 - val_loss: 0.1191 - val_mse: 0.0607 - lr: 5.0000e-04\n",
            "Epoch 364/1000\n",
            "2/2 [==============================] - 1s 709ms/step - loss: 0.0258 - mse: 0.0196 - val_loss: 0.1220 - val_mse: 0.0628 - lr: 5.0000e-04\n",
            "Epoch 365/1000\n",
            "2/2 [==============================] - 1s 708ms/step - loss: 0.0236 - mse: 0.0167 - val_loss: 0.1286 - val_mse: 0.0619 - lr: 5.0000e-04\n",
            "Epoch 366/1000\n",
            "2/2 [==============================] - 1s 710ms/step - loss: 0.0217 - mse: 0.0161 - val_loss: 0.1392 - val_mse: 0.0684 - lr: 5.0000e-04\n",
            "Epoch 367/1000\n",
            "2/2 [==============================] - 1s 706ms/step - loss: 0.0210 - mse: 0.0147 - val_loss: 0.1194 - val_mse: 0.0592 - lr: 5.0000e-04\n",
            "Epoch 368/1000\n",
            "2/2 [==============================] - 1s 720ms/step - loss: 0.0217 - mse: 0.0162 - val_loss: 0.1273 - val_mse: 0.0628 - lr: 5.0000e-04\n",
            "Epoch 369/1000\n",
            "2/2 [==============================] - 1s 717ms/step - loss: 0.0196 - mse: 0.0149 - val_loss: 0.1347 - val_mse: 0.0654 - lr: 5.0000e-04\n",
            "Epoch 370/1000\n",
            "2/2 [==============================] - 1s 724ms/step - loss: 0.0194 - mse: 0.0137 - val_loss: 0.1224 - val_mse: 0.0612 - lr: 5.0000e-04\n",
            "Epoch 371/1000\n",
            "2/2 [==============================] - 1s 739ms/step - loss: 0.0202 - mse: 0.0146 - val_loss: 0.1274 - val_mse: 0.0624 - lr: 5.0000e-04\n",
            "Epoch 372/1000\n",
            "2/2 [==============================] - 1s 723ms/step - loss: 0.0189 - mse: 0.0143 - val_loss: 0.1373 - val_mse: 0.0675 - lr: 5.0000e-04\n",
            "Epoch 373/1000\n",
            "2/2 [==============================] - 1s 722ms/step - loss: 0.0190 - mse: 0.0137 - val_loss: 0.1232 - val_mse: 0.0609 - lr: 5.0000e-04\n",
            "Epoch 374/1000\n",
            "2/2 [==============================] - 1s 735ms/step - loss: 0.0189 - mse: 0.0141 - val_loss: 0.1277 - val_mse: 0.0631 - lr: 5.0000e-04\n",
            "Epoch 375/1000\n",
            "2/2 [==============================] - 1s 721ms/step - loss: 0.0181 - mse: 0.0138 - val_loss: 0.1323 - val_mse: 0.0647 - lr: 5.0000e-04\n",
            "Epoch 376/1000\n",
            "2/2 [==============================] - 1s 727ms/step - loss: 0.0179 - mse: 0.0132 - val_loss: 0.1246 - val_mse: 0.0616 - lr: 5.0000e-04\n",
            "Epoch 377/1000\n",
            "2/2 [==============================] - 1s 725ms/step - loss: 0.0182 - mse: 0.0134 - val_loss: 0.1255 - val_mse: 0.0618 - lr: 5.0000e-04\n",
            "Epoch 378/1000\n",
            "2/2 [==============================] - 1s 715ms/step - loss: 0.0177 - mse: 0.0135 - val_loss: 0.1307 - val_mse: 0.0641 - lr: 5.0000e-04\n",
            "Epoch 379/1000\n",
            "2/2 [==============================] - 1s 719ms/step - loss: 0.0177 - mse: 0.0132 - val_loss: 0.1294 - val_mse: 0.0633 - lr: 5.0000e-04\n",
            "Epoch 380/1000\n",
            "2/2 [==============================] - 1s 730ms/step - loss: 0.0173 - mse: 0.0129 - val_loss: 0.1265 - val_mse: 0.0625 - lr: 5.0000e-04\n",
            "Epoch 381/1000\n",
            "2/2 [==============================] - 1s 713ms/step - loss: 0.0174 - mse: 0.0131 - val_loss: 0.1263 - val_mse: 0.0620 - lr: 5.0000e-04\n",
            "Epoch 382/1000\n",
            "2/2 [==============================] - 1s 721ms/step - loss: 0.0172 - mse: 0.0130 - val_loss: 0.1295 - val_mse: 0.0635 - lr: 5.0000e-04\n",
            "Epoch 383/1000\n",
            "2/2 [==============================] - 1s 723ms/step - loss: 0.0171 - mse: 0.0129 - val_loss: 0.1309 - val_mse: 0.0645 - lr: 5.0000e-04\n",
            "Epoch 384/1000\n",
            "2/2 [==============================] - 1s 721ms/step - loss: 0.0170 - mse: 0.0127 - val_loss: 0.1264 - val_mse: 0.0621 - lr: 5.0000e-04\n",
            "Epoch 385/1000\n",
            "2/2 [==============================] - 1s 712ms/step - loss: 0.0170 - mse: 0.0127 - val_loss: 0.1265 - val_mse: 0.0623 - lr: 5.0000e-04\n",
            "Epoch 386/1000\n",
            "2/2 [==============================] - 1s 723ms/step - loss: 0.0167 - mse: 0.0127 - val_loss: 0.1303 - val_mse: 0.0639 - lr: 5.0000e-04\n",
            "Epoch 387/1000\n",
            "2/2 [==============================] - 1s 727ms/step - loss: 0.0167 - mse: 0.0127 - val_loss: 0.1284 - val_mse: 0.0630 - lr: 5.0000e-04\n",
            "Epoch 388/1000\n",
            "2/2 [==============================] - 1s 709ms/step - loss: 0.0166 - mse: 0.0125 - val_loss: 0.1266 - val_mse: 0.0622 - lr: 5.0000e-04\n",
            "Epoch 389/1000\n",
            "2/2 [==============================] - 1s 714ms/step - loss: 0.0166 - mse: 0.0125 - val_loss: 0.1276 - val_mse: 0.0630 - lr: 5.0000e-04\n",
            "Epoch 390/1000\n",
            "2/2 [==============================] - 1s 724ms/step - loss: 0.0164 - mse: 0.0125 - val_loss: 0.1286 - val_mse: 0.0631 - lr: 5.0000e-04\n",
            "Epoch 391/1000\n",
            "2/2 [==============================] - 1s 720ms/step - loss: 0.0163 - mse: 0.0124 - val_loss: 0.1291 - val_mse: 0.0634 - lr: 5.0000e-04\n",
            "Epoch 392/1000\n",
            "2/2 [==============================] - 1s 720ms/step - loss: 0.0163 - mse: 0.0124 - val_loss: 0.1301 - val_mse: 0.0639 - lr: 5.0000e-04\n",
            "Epoch 393/1000\n",
            "2/2 [==============================] - 1s 711ms/step - loss: 0.0162 - mse: 0.0123 - val_loss: 0.1283 - val_mse: 0.0630 - lr: 5.0000e-04\n",
            "Epoch 394/1000\n",
            "2/2 [==============================] - 1s 715ms/step - loss: 0.0161 - mse: 0.0122 - val_loss: 0.1267 - val_mse: 0.0626 - lr: 5.0000e-04\n",
            "Epoch 395/1000\n",
            "2/2 [==============================] - 1s 717ms/step - loss: 0.0162 - mse: 0.0123 - val_loss: 0.1257 - val_mse: 0.0621 - lr: 5.0000e-04\n",
            "Epoch 396/1000\n",
            "2/2 [==============================] - 1s 709ms/step - loss: 0.0162 - mse: 0.0123 - val_loss: 0.1276 - val_mse: 0.0629 - lr: 5.0000e-04\n",
            "Epoch 397/1000\n",
            "2/2 [==============================] - 1s 715ms/step - loss: 0.0159 - mse: 0.0122 - val_loss: 0.1282 - val_mse: 0.0630 - lr: 5.0000e-04\n",
            "Epoch 398/1000\n",
            "2/2 [==============================] - 1s 729ms/step - loss: 0.0158 - mse: 0.0121 - val_loss: 0.1291 - val_mse: 0.0636 - lr: 5.0000e-04\n",
            "Epoch 399/1000\n",
            "2/2 [==============================] - 1s 716ms/step - loss: 0.0158 - mse: 0.0120 - val_loss: 0.1304 - val_mse: 0.0640 - lr: 5.0000e-04\n",
            "Epoch 400/1000\n",
            "2/2 [==============================] - 1s 729ms/step - loss: 0.0159 - mse: 0.0121 - val_loss: 0.1319 - val_mse: 0.0650 - lr: 5.0000e-04\n",
            "Epoch 401/1000\n",
            "2/2 [==============================] - 1s 725ms/step - loss: 0.0159 - mse: 0.0122 - val_loss: 0.1306 - val_mse: 0.0642 - lr: 5.0000e-04\n",
            "Epoch 402/1000\n",
            "2/2 [==============================] - 1s 868ms/step - loss: 0.0157 - mse: 0.0119 - val_loss: 0.1294 - val_mse: 0.0637 - lr: 5.0000e-04\n",
            "Epoch 403/1000\n",
            "2/2 [==============================] - 1s 715ms/step - loss: 0.0155 - mse: 0.0118 - val_loss: 0.1274 - val_mse: 0.0628 - lr: 5.0000e-04\n",
            "Epoch 404/1000\n",
            "2/2 [==============================] - 1s 718ms/step - loss: 0.0156 - mse: 0.0119 - val_loss: 0.1258 - val_mse: 0.0620 - lr: 5.0000e-04\n",
            "Epoch 405/1000\n",
            "2/2 [==============================] - 1s 732ms/step - loss: 0.0159 - mse: 0.0120 - val_loss: 0.1247 - val_mse: 0.0622 - lr: 5.0000e-04\n",
            "Epoch 406/1000\n",
            "2/2 [==============================] - 1s 722ms/step - loss: 0.0172 - mse: 0.0130 - val_loss: 0.1209 - val_mse: 0.0608 - lr: 5.0000e-04\n",
            "Epoch 407/1000\n",
            "2/2 [==============================] - 1s 721ms/step - loss: 0.0210 - mse: 0.0159 - val_loss: 0.1217 - val_mse: 0.0615 - lr: 5.0000e-04\n",
            "Epoch 408/1000\n",
            "2/2 [==============================] - 1s 724ms/step - loss: 0.0188 - mse: 0.0140 - val_loss: 0.1299 - val_mse: 0.0633 - lr: 5.0000e-04\n",
            "Epoch 409/1000\n",
            "2/2 [==============================] - 1s 722ms/step - loss: 0.0170 - mse: 0.0129 - val_loss: 0.1342 - val_mse: 0.0648 - lr: 5.0000e-04\n",
            "Epoch 410/1000\n",
            "2/2 [==============================] - 1s 715ms/step - loss: 0.0173 - mse: 0.0125 - val_loss: 0.1277 - val_mse: 0.0632 - lr: 5.0000e-04\n",
            "Epoch 411/1000\n",
            "2/2 [==============================] - 1s 710ms/step - loss: 0.0164 - mse: 0.0123 - val_loss: 0.1211 - val_mse: 0.0595 - lr: 5.0000e-04\n",
            "Epoch 412/1000\n",
            "2/2 [==============================] - 1s 728ms/step - loss: 0.0170 - mse: 0.0132 - val_loss: 0.1289 - val_mse: 0.0640 - lr: 5.0000e-04\n",
            "Epoch 413/1000\n",
            "2/2 [==============================] - 1s 718ms/step - loss: 0.0160 - mse: 0.0123 - val_loss: 0.1318 - val_mse: 0.0639 - lr: 5.0000e-04\n",
            "Epoch 414/1000\n",
            "2/2 [==============================] - 1s 721ms/step - loss: 0.0159 - mse: 0.0119 - val_loss: 0.1267 - val_mse: 0.0627 - lr: 5.0000e-04\n",
            "Epoch 415/1000\n",
            "2/2 [==============================] - 1s 710ms/step - loss: 0.0158 - mse: 0.0117 - val_loss: 0.1254 - val_mse: 0.0622 - lr: 5.0000e-04\n",
            "Epoch 416/1000\n",
            "2/2 [==============================] - 1s 718ms/step - loss: 0.0157 - mse: 0.0122 - val_loss: 0.1284 - val_mse: 0.0628 - lr: 5.0000e-04\n",
            "Epoch 417/1000\n",
            "2/2 [==============================] - 1s 726ms/step - loss: 0.0155 - mse: 0.0120 - val_loss: 0.1374 - val_mse: 0.0684 - lr: 5.0000e-04\n",
            "Epoch 418/1000\n",
            "2/2 [==============================] - 1s 715ms/step - loss: 0.0163 - mse: 0.0126 - val_loss: 0.1281 - val_mse: 0.0621 - lr: 5.0000e-04\n",
            "Epoch 419/1000\n",
            "2/2 [==============================] - 1s 738ms/step - loss: 0.0154 - mse: 0.0114 - val_loss: 0.1248 - val_mse: 0.0622 - lr: 5.0000e-04\n"
          ]
        }
      ],
      "source": [
        "#early = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\",min_delta=0.001, patience=200, verbose=0, mode=\"auto\", baseline=None, restore_best_weights=True)\n",
        "\n",
        "early = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\",min_delta=0.0005, patience=200, verbose=0, mode=\"auto\", baseline=None, restore_best_weights=True)\n",
        "\n",
        "\n",
        "def scheduler(epoch, lr):\n",
        "  if epoch == 500:\n",
        "    return lr /5\n",
        "  else:\n",
        "    return lr\n",
        "\n",
        "\n",
        "scheduler_cb = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
        "\n",
        "\n",
        "history = model.fit(train_data, train_labels, epochs=1000, validation_data=(validation_data, validation_labels), callbacks=[early, scheduler_cb])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "UgCp0oNUAfzt",
        "outputId": "a811df6a-ca46-4fec-face-9c76604718f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['loss', 'mse', 'val_loss', 'val_mse', 'lr'])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "print(history.history.keys())\n",
        "# # summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "#plt.ylim([0,0.2])\n",
        "plt.legend(['train', 'valid'], loc='upper right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def area_hotspot(datapoint):\n",
        "  tinc = 5\n",
        "  tnum = int(4500/tinc)\n",
        "\n",
        "  tinc = 5\n",
        "  tnum = int(4500/tinc)\n",
        "\n",
        "  bins=np.zeros((tnum,2))\n",
        "  bins[:,0] = [ tinc*(0.5 + x) for x in list(range(tnum))]\n",
        "\n",
        "  for bin_index, temp in np.ndenumerate(datapoint):\n",
        "    theta = temp * 1000\n",
        "    tind=math.floor(theta/tinc)\n",
        "    bins[:tind, 1] += 4 # ~ Roughly 2*2*1 nm^3 (volume value from Chunyu)\n",
        "  \n",
        "  return bins\n",
        "\n",
        "def datapoint_results_print(simulation_point, simulation_temperatures):\n",
        "\n",
        "  # Simulation point is the input train_data[<point order>,<dim 0>,<dim 1>,<dim 2>,<channel>]\n",
        "  # For example, simulation_point = train_data[i,:,:,:,:]\n",
        "\n",
        "\n",
        "\n",
        "  fig = make_subplots(rows=3, cols=3, specs=[[{\"type\": \"scatter3d\"},{\"type\": \"scatter3d\"},{\"type\": \"scatter3d\"}],\n",
        "                                             [{\"type\": \"scatter3d\"},{\"type\": \"scatter3d\"},{\"type\": \"scatter\"}],\n",
        "                                             [{\"type\": \"histogram\"},{\"type\": \"histogram\"},{\"type\": \"scatter\"}]],\n",
        "                      subplot_titles=[\"Input 1\",\"Input 2\",\"Input 3\",\n",
        "                                      \"Temp (Labels)\",\"Temp (Predictions)\",\"Parity Plot\",\n",
        "                                      \"Temp (Distributions)\", \"Residuals\", \"Hotspot volume\"], horizontal_spacing = 0.1, vertical_spacing = 0.1)\n",
        "  \n",
        "  fig.update_layout(autosize=False, width=1000, height=1000) \n",
        "\n",
        "  # FIRST PLOT --> INPUT 1\n",
        "\n",
        "  input_1 = simulation_point[:,:,:,0].squeeze()\n",
        "\n",
        "  X,Y,Z = np.mgrid[0:input_1.shape[2], 0:input_1.shape[1], 0:input_1.shape[0]]\n",
        "  input_1_xz = np.swapaxes(input_1, 2, 0)\n",
        "\n",
        "  trace_1 = go.Scatter3d(x = X.flatten(), y = Y.flatten(), z=Z.flatten(),\n",
        "                          mode='markers', marker=dict(colorscale='Viridis', symbol='square', color = input_1_xz.flatten(), colorbar=dict(thickness=20, len=0.3, x=0.27, y=0.9)), showlegend=False)\n",
        "  fig.add_trace(trace_1, row=1, col=1)\n",
        "\n",
        "  # SECOND PLOT --> INPUT 2\n",
        "\n",
        "  input_2 = simulation_point[:,:,:,1].squeeze()\n",
        "\n",
        "  X,Y,Z = np.mgrid[0:input_2.shape[2], 0:input_2.shape[1], 0:input_2.shape[0]]\n",
        "  input_2_xz = np.swapaxes(input_2, 2, 0)\n",
        "\n",
        "  trace_2 = go.Scatter3d(x = X.flatten(), y = Y.flatten(), z=Z.flatten(),\n",
        "                          mode='markers', marker=dict(colorscale='Viridis', symbol='square', color = input_2_xz.flatten(), colorbar=dict(thickness=20, len=0.3, x=0.62, y=0.9)), showlegend=False)\n",
        "  fig.add_trace(trace_2, row=1, col=2)\n",
        "\n",
        "\n",
        "  # THIRD PLOT --> INPUT 3\n",
        "\n",
        "  input_3 = simulation_point[:,:,:,2].squeeze()\n",
        "\n",
        "  X,Y,Z = np.mgrid[0:input_3.shape[2], 0:input_3.shape[1], 0:input_3.shape[0]]\n",
        "  input_3_xz = np.swapaxes(input_3, 2, 0)\n",
        "\n",
        "  trace_3 = go.Scatter3d(x = X.flatten(), y = Y.flatten(), z=Z.flatten(),\n",
        "                          mode='markers', marker=dict(colorscale='Viridis', symbol='square', color = input_3_xz.flatten(), colorbar=dict(thickness=20, len=0.3, x=1, y=0.9)), showlegend=False)\n",
        "  fig.add_trace(trace_3, row=1, col=3)\n",
        "\n",
        "\n",
        "  # FOURTH PLOT --> TEMPS (LABELS)\n",
        "  \n",
        "  pred = tf.expand_dims(simulation_point, axis=0)\n",
        "  prediction_point = model.predict(pred)\n",
        "  prediction_tensor = prediction_point.squeeze()\n",
        "\n",
        "  maxval = max(np.max(prediction_tensor), np.max(simulation_temperatures))\n",
        "  \n",
        "\n",
        "  X,Y,Z = np.mgrid[0:simulation_temperatures.shape[2], 0:simulation_temperatures.shape[1], 0:simulation_temperatures.shape[0]]\n",
        "  simulation_temperatures_xz = np.swapaxes(simulation_temperatures, 2, 0)\n",
        "\n",
        "  trace_4 = go.Scatter3d(x = X.flatten(), y = Y.flatten(), z=Z.flatten(),\n",
        "                          mode='markers', marker=dict(colorscale='Viridis', symbol='square', color = simulation_temperatures_xz.flatten(), colorbar=dict(thickness=20,len=0.3, x=0.27, y=0.5)), showlegend=False)\n",
        "  fig.add_trace(trace_4, row=2, col=1)\n",
        "\n",
        "\n",
        "  # FIFTH PLOT --> TEMPS (PREDICTIONS)\n",
        "\n",
        "  X,Y,Z = np.mgrid[0:prediction_tensor.shape[2], 0:prediction_tensor.shape[1], 0:prediction_tensor.shape[0]]\n",
        "  prediction_tensor_xz = np.swapaxes(prediction_tensor, 2, 0)\n",
        "\n",
        "  trace_5 = go.Scatter3d(x = X.flatten(), y = Y.flatten(), z=Z.flatten(),\n",
        "                          mode='markers', marker=dict(colorscale='Viridis', symbol='square', color = prediction_tensor_xz.flatten(), colorbar=dict(thickness=20, len=0.3, x=0.62, y=0.5)), showlegend=False)\n",
        "  fig.add_trace(trace_5, row=2, col=2)\n",
        "\n",
        "  # SIXTH PLOT --> PARITY PLOT\n",
        "\n",
        "  trace_6 = go.Scatter(x=simulation_temperatures.flatten(), y = prediction_tensor.flatten(), mode='markers', showlegend=False)\n",
        "  fig.add_trace(trace_6, row=2, col=3)\n",
        "  fig.update_xaxes(title=\"Scaled Temperature (K) - Labels\", row=2, col=3)\n",
        "  fig.update_yaxes(title=\"Scaled Temperature (K) - Predictions\", row=2, col=3)\n",
        "\n",
        "\n",
        "  # SEVENTH PLOT --> TEMP (Distributions)\n",
        "\n",
        "  trace_7 = go.Histogram(x=prediction_tensor.flatten(), name='Predictions', marker=dict(color='red'), showlegend=True, legendgroup = '1')\n",
        "  trace_7b = go.Histogram(x=simulation_temperatures.flatten(), name='Truth', marker=dict(color='green'), showlegend=True, legendgroup = '1')\n",
        "  fig.update_xaxes(title=\"Temperature (K) - Labels\", row=3, col=1)\n",
        "  fig.add_trace(trace_7, row=3, col=1)\n",
        "  fig.add_trace(trace_7b, row=3, col=1) \n",
        "\n",
        "\n",
        "  # EIGHTH PLOT --> RESIDUALS\n",
        "\n",
        "  diff_xz = prediction_tensor - simulation_temperatures\n",
        "  flat_diff_xz = diff_xz.flatten()\n",
        "\n",
        "  trace_8 = go.Histogram(x=flat_diff_xz, showlegend=False)\n",
        "  trace_line = go.Scatter(x=[0,0], y = [0,700], mode='lines', showlegend=False)\n",
        "\n",
        "  fig.add_trace(trace_8, row=3, col=2)\n",
        "  fig.add_trace(trace_line, row=3, col=2)\n",
        "\n",
        "  # NINTH PLOT ---> VOLUME OF HOTSPOT\n",
        "  \n",
        "  bins_labels = area_hotspot(simulation_temperatures)\n",
        "  bins_predictions = area_hotspot(prediction_tensor)\n",
        "  trace_9 = go.Scatter(x=bins_predictions[:,1], y=bins_predictions[:,0], mode='lines',  name='Predictions', marker=dict(color='red'), showlegend=True, legendgroup = '2')\n",
        "  trace_9b = go.Scatter(x=bins_labels[:,1], y=bins_labels[:,0], mode='lines', name='Truth', marker=dict(color='green'), showlegend=True, legendgroup = '2')\n",
        "  \n",
        "  fig.add_trace(trace_9, row=3, col=3)\n",
        "  fig.add_trace(trace_9b, row=3, col=3) \n",
        "  fig.update_xaxes(type=\"log\", row=3, col=3)\n",
        "  fig.update_xaxes(title=\"Hotspot Volume (nm^3)\", row=3, col=3)\n",
        "  fig.update_yaxes(title=\"Temperature (K)\", row=3, col=3)  \n",
        "\n",
        "  #### \n",
        "\n",
        "  fig.update_layout(autosize=False, width=1400, height=1000, legend_tracegroupgap = 180, legend=dict(font=dict(size=16),orientation=\"h\"))   \n",
        "\n",
        "  return fig, prediction_tensor\n",
        "\n"
      ],
      "metadata": {
        "id": "cVbLrXkBNWDI"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i,title in enumerate(paths):\n",
        "  print(i)\n",
        "  inppoint = train_data[i,:,:,:,:].squeeze()\n",
        "  inplabel = train_labels[i,:,:,:,:].squeeze()\n",
        "  fig, prediction = datapoint_results_print(inppoint, inplabel)\n",
        "\n",
        "  ### SAVING NUMPY PREDICTIONS\n",
        "  np.save('results/'+str(title)+'.npy', prediction)\n",
        "  #fig.write_image('results/'+str(title)+'.pdf')\n",
        "  fig.write_html('results/'+str(title)+'.html')"
      ],
      "metadata": {
        "id": "-PCvavvNP6kj",
        "outputId": "2b08a616-146a-4a33-e58c-c4d04402b4eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1/1 [==============================] - 1s 924ms/step\n",
            "1\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "2\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "3\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "4\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "5\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "6\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "7\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "8\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "9\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "10\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "11\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "12\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "13\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "14\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "15\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "16\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "17\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "18\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "19\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "20\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "21\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "22\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "23\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "24\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "25\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "26\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "27\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "28\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "29\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "30\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "31\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "32\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "33\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "34\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "35\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "36\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "37\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "38\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "39\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "40\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "41\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "42\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "43\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "44\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "45\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "46\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "47\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "48\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "49\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "50\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "51\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "52\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "53\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "54\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "55\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "56\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "57\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "58\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "59\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "60\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "61\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "62\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "63\n",
            "1/1 [==============================] - 0s 20ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, title in enumerate(validation_paths):\n",
        "  print(i)\n",
        "  inppoint = validation_data[i,:,:,:,:].squeeze()\n",
        "  inplabel = validation_labels[i,:,:,:,:].squeeze()\n",
        "  fig, prediction =  datapoint_results_print(inppoint, inplabel)\n",
        "  ### SAVING NUMPY PREDICTIONS\n",
        "  np.save('results/'+str(title)+'.npy', prediction)\n",
        "  #fig.write_image('results/'+str(title)+'.pdf')\n",
        "  fig.write_html('results/'+str(title)+'.html')"
      ],
      "metadata": {
        "id": "ClEFHwOOjvd5",
        "outputId": "990f8c7a-cdcb-4767-fe07-e7af60ffaa8a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "2\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "3\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "4\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "5\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "6\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "7\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "8\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "9\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "10\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "11\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "12\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "13\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "14\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "15\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "16\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "17\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "18\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "19\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "20\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "21\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "22\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "23\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "24\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "25\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "26\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "27\n",
            "1/1 [==============================] - 0s 19ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cDNLoWw4mN_b"
      },
      "execution_count": 17,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "pbx-local",
      "language": "python",
      "name": "pbx-local"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15"
    },
    "vscode": {
      "interpreter": {
        "hash": "f806901b044f1f905f6f73d9d34ac86b2be2e087ac41c051b6f31285dd315984"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}