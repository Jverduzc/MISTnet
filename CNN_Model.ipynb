{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jverduzc/CNN_PBX_Model/blob/master/CNN_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convolutional Neural Networks (CNN) for prediction of hotspots on PBX\n",
        "\n",
        "In this notebook we create a CNN model with a U-Net architecture for the prediction of temperature for a plastically bonded explosive molecular dynamics simulation."
      ],
      "metadata": {
        "id": "4QfRMy9VFrcW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Libraries\n",
        "\n",
        "First, we need to set up libraries that are not part of the default environment in Google Colab. In our case, this is only the rendering library ```kaleido```.\n",
        "\n",
        "You will need to restart the runtime to update the kernel.\n",
        "- Menu Runtime -> Restart Runtime"
      ],
      "metadata": {
        "id": "C2UgThuCGG3F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaleido"
      ],
      "metadata": {
        "id": "M79B0dH7ftZW",
        "outputId": "0440ba7d-1ce4-48e4-82d3-e7031cb8b159",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: kaleido in /usr/local/lib/python3.7/dist-packages (0.2.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will then import the required libraries for our notebook to run. The following cells import:\n",
        "- Standard python libraries\n",
        "- Plotting libraries\n",
        "- Machine learning libraries (tensorflow / keras)"
      ],
      "metadata": {
        "id": "2ECMQghLGzUA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5Nd2jeT8r8uu"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import copy\n",
        "import math\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import clear_output\n",
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()"
      ],
      "metadata": {
        "id": "qrgVqntVG6YP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "V08TFba6G_nw"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import initializers\n",
        "from keras.layers import Input, Dropout, BatchNormalization, Conv3DTranspose, concatenate, Dense, Conv3D, Flatten, MaxPooling3D\n",
        "from keras.models import Sequential\n",
        "import keras.backend as K\n",
        "tf.keras.utils.set_random_seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to verify that we are running an enviroment with a GPU in Colab. The next cell shows if you have a GPU front-end execution host allocated to the notebook. \n",
        "\n",
        "<font color='red'><b>Warning:</b></font> If you don't have one, you'll need to re-run the previous cells after going to:\n",
        "- Menu Runtime -> Change runtime type -> Hardware accelerator"
      ],
      "metadata": {
        "id": "Wh5PkIn3H9dD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "KnH0b5QqG_ny",
        "outputId": "f5c89886-be34-4fbd-c9e3-7e649652449b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  1\n"
          ]
        }
      ],
      "source": [
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data\n",
        "\n",
        "After setting up the environment, we need the repository files to create the model and access the training/validation data. \n",
        "\n",
        "<font color='orange'><b>Attention:</b></font> This is not trivial in Colab, but to access a private repository on Github like this, you need to provide Colab with your Github Key. You can get that key here: https://github.com/settings/tokens\n",
        "\n",
        "After that, you need to execute a command with this syntax:\n",
        "```\n",
        "!git clone https://username:github_key@github.com/Jverduzc/CNN_PBX_Model.git\n",
        "```\n",
        "You can fill up the blanks in the following cell. We will also change the current working directory. You should see the a new directory in your directory tree (on your left) with the files on the repository.\n"
      ],
      "metadata": {
        "id": "hYseV3r9LkME"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone XXXXXXX\n",
        "os.chdir(\"CNN_PBX_Model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cr1dF6TJHU2J",
        "outputId": "819583a6-5803-42d4-d0df-c7f62466fa22"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CNN_PBX_Model'...\n",
            "remote: Enumerating objects: 514, done.\u001b[K\n",
            "remote: Counting objects: 100% (514/514), done.\u001b[K\n",
            "remote: Compressing objects: 100% (328/328), done.\u001b[K\n",
            "remote: Total 514 (delta 230), reused 443 (delta 185), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (514/514), 23.85 MiB | 9.69 MiB/s, done.\n",
            "Resolving deltas: 100% (230/230), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training and validation data\n",
        "\n",
        "This notebook was designed to read everything in the ```/train/``` folder as training data and everything in the ```/validation/``` folder as validation data. These directories contain individual labeled subdirectories that represent each of our simulation systems (data points).\n",
        "\n",
        "In each of the simulation systems subdirectories there are two files as numpy arrays: ```input.npy``` and ```output.npy```.\n",
        "\n",
        "\n",
        "```input.npy``` contains a (16 x 34 x 34 x 3) array with the input mappings from our simulations. The first three numbers represent the dimensions (in bins) of our system. The last number represents the number of mappings for our inputs. For each of our 3D structures, we generate the following:\n",
        "- Total density\n",
        "- HE density\n",
        "- GB interface parameter\n",
        "\n",
        "```output.npy``` contains a (16 x 32 x 32 x 1) array with the output mapping (temperature) from our simulations. The first three numbers represent the dimensions (in bins) of our system, but note that they are different from the inputs due to periodic boundary conditions. The last number represents the temperature mapping. \n",
        "- Temperature\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "B2wbmJTjOpoe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please verify that after running the next cells you get the following results:\n",
        "\n",
        "<b>Training data:</b><br>\n",
        "\n",
        "(64, 16, 34, 34, 3) <br>\n",
        "(64, 16, 32, 32, 1) <br>\n",
        "\n",
        "<b>Validation data:</b><br>\n",
        "\n",
        "(28, 16, 34, 34, 3) <br>\n",
        "(28, 16, 32, 32, 1) <br>\n",
        "\n",
        "You can see that there is a new number in these arrays. It indicates the number of datapoints in each set. You can read this as having 64 points with (16 x 34 x 34 x 3) shape."
      ],
      "metadata": {
        "id": "ogxA0zxHRUIl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "KZxqA1mmG_nz",
        "outputId": "3aa99001-e56f-4446-fe99-db610bcb72f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 16, 34, 34, 3)\n",
            "(64, 16, 32, 32, 1)\n"
          ]
        }
      ],
      "source": [
        "# TRAINING DATA\n",
        "\n",
        "paths = [x[0] for x in os.walk('train/')][1:]\n",
        "\n",
        "train_data = []\n",
        "train_labels = []\n",
        "\n",
        "for i in paths:\n",
        "  train_ex = np.load(i + \"/input.npy\")\n",
        "\n",
        "  if train_ex.shape != (16,34,34,3):\n",
        "\n",
        "    first_axis_pad = int(0.5 * (34 - train_ex.shape[1]))\n",
        "    second_axis_pad = int(0.5 * (34 - train_ex.shape[2]))\n",
        "\n",
        "    train_ex = np.pad(train_ex, ((0, 0), (first_axis_pad, first_axis_pad), (second_axis_pad, second_axis_pad), (0,0)), 'wrap')\n",
        "\n",
        "  train_lb = np.load(i + \"/output.npy\")\n",
        "  train_data.append(train_ex)\n",
        "  train_labels.append(train_lb)\n",
        "\n",
        "train_data = np.array(train_data)\n",
        "train_labels = np.array(train_labels) / 1000\n",
        "\n",
        "print(train_data.shape)\n",
        "print(train_labels.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "02ORxH_vZNOC",
        "outputId": "43f3416d-88ff-443f-c017-099aaaa46635",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(28, 16, 34, 34, 3)\n",
            "(28, 16, 32, 32, 1)\n"
          ]
        }
      ],
      "source": [
        "# VALIDATION DATA\n",
        "\n",
        "validation_paths = [x[0] for x in os.walk('validation/')][1:]\n",
        "\n",
        "validation_data = []\n",
        "validation_labels = []\n",
        "\n",
        "for i in validation_paths:\n",
        "  validation_ex= np.load(i + \"/input.npy\")\n",
        "\n",
        "  if validation_ex.shape != (16,34,34,1):\n",
        "\n",
        "    first_axis_pad = int(0.5 * (34 - validation_ex.shape[1]))\n",
        "    second_axis_pad = int(0.5 * (34 - validation_ex.shape[2]))\n",
        "\n",
        "    validation_ex = np.pad(validation_ex, ((0, 0), (first_axis_pad, first_axis_pad), (second_axis_pad, second_axis_pad), (0,0)), 'wrap')\n",
        "\n",
        "\n",
        "  validation_lb = np.load(i + \"/output.npy\")\n",
        "  validation_data.append(validation_ex)\n",
        "  validation_labels.append(validation_lb)\n",
        "\n",
        "validation_data = np.array(validation_data)\n",
        "validation_labels = np.array(validation_labels) / 1000\n",
        "\n",
        "print(validation_data.shape)\n",
        "print(validation_labels.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model\n",
        "\n",
        "This notebook creates an architecture based on U-Net, a CNN algorithm for image segmentation. We will go a bit into the design of the architecture in the following cells.\n",
        "\n",
        "This first function addresses periodic padding for tensors in the model."
      ],
      "metadata": {
        "id": "xQrUeLx5R1mp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "fWvLsqQXsWGy"
      },
      "outputs": [],
      "source": [
        "# Code taken from: https://stackoverflow.com/questions/39088489/tensorflow-periodic-padding\n",
        "\n",
        "def periodic_padding_flexible(tensor, axis, padding=1):\n",
        "\n",
        "    if isinstance(axis,int):\n",
        "        axis = (axis,)\n",
        "    if isinstance(padding,int):\n",
        "        padding = (padding,)\n",
        "\n",
        "    ndim = len(tensor.shape)\n",
        "\n",
        "    for ax,p in zip(axis,padding):\n",
        "        # create a slice object that selects everything from all axes,\n",
        "        # except only 0:p for the specified for right, and -p: for left\n",
        "\n",
        "        ind_right = [slice(-p,None) if i == ax else slice(None) for i in range(ndim)]\n",
        "        ind_left = [slice(0, p) if i == ax else slice(None) for i in range(ndim)]\n",
        "        right = tensor[ind_right]\n",
        "        left = tensor[ind_left]\n",
        "        middle = tensor\n",
        "        tensor = tf.concat([right,middle,left], axis=ax)\n",
        "\n",
        "    return tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since this architecture is going to be complicated, we will start by creating mini-blocks. This first function ```DownConvBlock``` executes the following operations sequentially:\n",
        "\n",
        "- (Optional) Periodic Padding\n",
        "- 3D Convolutional Layer\n",
        "- Periodic Padding\n",
        "- 3D Convolutional Layer\n",
        "- Batch normalization\n",
        "- (Optional) MaxPooling3D Layer"
      ],
      "metadata": {
        "id": "i2sWbazsagHg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "rLhZ3NIocWJ9"
      },
      "outputs": [],
      "source": [
        "def DownConvBlock(inputs, n_filters=32, filter_size = 3, max_pooling=True, special_padding=False):\n",
        "\n",
        "  padding_size = int((filter_size-1)/2)\n",
        "  kernel_init =   tf.keras.initializers.GlorotUniform(seed=0)\n",
        "  bias_init = tf.keras.initializers.Zeros()\n",
        "\n",
        "\n",
        "  # PERIODIC PADDING\n",
        "\n",
        "  inputs = periodic_padding_flexible(inputs, axis=1,padding=padding_size)\n",
        "  if special_padding == False:\n",
        "    inputs = periodic_padding_flexible(inputs, axis=2,padding=padding_size)\n",
        "    inputs = periodic_padding_flexible(inputs, axis=3,padding=padding_size)\n",
        "  \n",
        "  conv = Conv3D(n_filters, filter_size, activation='relu', padding='valid', kernel_initializer=kernel_init, bias_initializer=bias_init)(inputs)\n",
        "  print(conv.shape)\n",
        "  conv = periodic_padding_flexible(conv, axis=(1,2,3),padding=(padding_size,padding_size,padding_size))\n",
        "  conv = Conv3D(n_filters, filter_size, activation='relu', padding='valid', kernel_initializer=kernel_init, bias_initializer=bias_init)(conv)\n",
        "  print(conv.shape)\n",
        "  conv = BatchNormalization()(conv, training=False)\n",
        "      \n",
        "  if max_pooling:\n",
        "    next_layer = tf.keras.layers.MaxPooling3D(pool_size = (2,2,2))(conv)\n",
        "  else:\n",
        "    next_layer = conv\n",
        "  \n",
        "  skip_connection = conv   \n",
        "\n",
        "  print(\"end_of_block\") \n",
        "  return next_layer, skip_connection"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This second function ```UpConvBlock``` executes the following operations sequentially:\n",
        "\n",
        "- Transpose 3D Convolutional Layer\n",
        "- Merge with <i>skip connection</i> from the corresponding ```DownConvBlock```\n",
        "- Periodic Padding\n",
        "- 3D Convolutional Layer\n",
        "- Periodic Padding\n",
        "- 3D Convolutional Layer"
      ],
      "metadata": {
        "id": "i8h2MOJHa_Aq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "YvMNFlsHseOF"
      },
      "outputs": [],
      "source": [
        "def UpConvBlock(prev_layer_input, skip_layer_input, filter_size = 3, n_filters=32):\n",
        "\n",
        "    padding_size = int((filter_size-1)/2)\n",
        "    kernel_init = tf.keras.initializers.GlorotUniform(seed=0)\n",
        "    bias_init = tf.keras.initializers.Zeros()   \n",
        "\n",
        "    up = Conv3DTranspose(n_filters, (filter_size,filter_size,filter_size),\n",
        "                         strides=(filter_size-1,filter_size-1,filter_size-1),\n",
        "                         padding='same', kernel_initializer=kernel_init, bias_initializer=bias_init)(prev_layer_input)\n",
        "\n",
        "    merge = concatenate([up, skip_layer_input], axis=4)\n",
        "    merge = periodic_padding_flexible(merge, axis=(1,2,3),padding=(padding_size,padding_size,padding_size))\n",
        "    conv = Conv3D(n_filters, filter_size, activation='relu', padding='valid', kernel_initializer=kernel_init,  bias_initializer=bias_init)(merge)\n",
        "    print(conv.shape)\n",
        "    conv = periodic_padding_flexible(conv, axis=(1,2,3),padding=(padding_size,padding_size,padding_size))\n",
        "    conv = Conv3D(n_filters, filter_size, activation='relu',padding='valid', kernel_initializer=kernel_init,  bias_initializer=bias_init)(conv)\n",
        "    print(conv.shape)\n",
        "\n",
        "    print(\"end_of_block\")\n",
        "    return conv"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The architecture will be composed of several ```DownConvBlock``` blocks followed by the same number of ```UpConvBlock``` blocks. "
      ],
      "metadata": {
        "id": "evWdWAkXbzfq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "fe-fUh99sgKg"
      },
      "outputs": [],
      "source": [
        "\n",
        "def UNet3DModel(input_size=(16, 34, 34, 3), n_filters=32, filter_size=3, n_classes=1):\n",
        "  kernel_init =  tf.keras.initializers.GlorotUniform(seed=0)\n",
        "  bias_init = tf.keras.initializers.Zeros()  \n",
        "  \n",
        "  inputs = Input(input_size)\n",
        "  print(\"Inputs\", inputs.shape)\n",
        "\n",
        "  cblock0 = DownConvBlock(inputs,     n_filters = n_filters    , filter_size = filter_size, max_pooling=False, special_padding=True)\n",
        "  print(\"CB0\", cblock0[0].shape)\n",
        "\n",
        "  cblock1 = DownConvBlock(cblock0[0],     n_filters = n_filters    , filter_size = filter_size, max_pooling=True, special_padding=False)\n",
        "  print(\"CB1\", cblock1[0].shape)\n",
        "\n",
        "  cblock2 = DownConvBlock(cblock1[0], n_filters = n_filters*2  , filter_size = filter_size, max_pooling=True, special_padding=False)\n",
        "  print(\"CB2\", cblock2[0].shape)\n",
        "    \n",
        "  cblock3 = DownConvBlock(cblock2[0], n_filters = n_filters*4  , filter_size = filter_size, max_pooling=True, special_padding=False)\n",
        "  print(\"CB3\", cblock3[0].shape)\n",
        "  \n",
        "  cblock4 = DownConvBlock(cblock3[0], n_filters = n_filters*8  , filter_size = filter_size, max_pooling=False, special_padding=False)\n",
        "  print(\"CB4\", cblock4[0].shape)\n",
        "\n",
        "\n",
        "  print(\"------------------\")\n",
        "\n",
        "  ublock7 = UpConvBlock(cblock4[0]   , cblock3[1],  n_filters = n_filters * 4, filter_size = filter_size)\n",
        "  print(\"UB7\", ublock7.shape)\n",
        "  \n",
        "  ublock8 = UpConvBlock(ublock7   , cblock2[1],  n_filters = n_filters * 2, filter_size = filter_size)\n",
        "  print(\"UB8\", ublock8.shape)\n",
        "  \n",
        "  ublock9 = UpConvBlock(ublock8   , cblock1[1],  n_filters = n_filters, filter_size = filter_size)\n",
        "  print(\"UB9\", ublock9.shape)\n",
        "\n",
        "  ublock9 = periodic_padding_flexible(ublock9, axis=(1,2,3),padding=(1,1,1))\n",
        "  \n",
        "  conv9 = Conv3D(n_filters, 3, activation='relu', padding='valid', kernel_initializer=kernel_init,  bias_initializer=bias_init)(ublock9)\n",
        "  print(\"C9\", conv9.shape)\n",
        "  \n",
        "  conv10 = Conv3D(n_classes, 1, padding='same', kernel_initializer=kernel_init,  bias_initializer=bias_init)(conv9)\n",
        "  print(\"C10\", conv10.shape)\n",
        "\n",
        "  model = tf.keras.Model(inputs=inputs, outputs=conv10)  \n",
        "\n",
        "  return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this application, we are concerned with the detection and accurate prediction of hotspots (areas with higher temperatures), which are significantly less common than the rest of the material at a lower temperature. To address this, we are using a custon loss function based on a weighted Mean Squared Error (MSE)."
      ],
      "metadata": {
        "id": "vUDnv4nLeTQH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "d2a5FVUYPHTB"
      },
      "outputs": [],
      "source": [
        "def custom_mse(y_true,y_pred):\n",
        "    w_hot = 5.0\n",
        "    w_cold = 1.0\n",
        "    cutoff = 1.8\n",
        "    weightmat = tf.cast(tf.where(tf.greater(y_true, cutoff), w_hot, w_cold),float)\n",
        "    loss = tf.cast(K.square(y_pred - y_true),float)\n",
        "    loss = loss*weightmat\n",
        "    loss = K.mean(loss)\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, this last cell calls the function to generate the model, pair it with an optimizer and compile the model object."
      ],
      "metadata": {
        "id": "-uvZjSSeekHu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "R3SJtgKps6tc",
        "outputId": "afa0d7ef-d1dc-405f-dd61-b44c0f55eec1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs (None, 16, 34, 34, 3)\n",
            "(None, 16, 32, 32, 32)\n",
            "(None, 16, 32, 32, 32)\n",
            "end_of_block\n",
            "CB0 (None, 16, 32, 32, 32)\n",
            "(None, 16, 32, 32, 32)\n",
            "(None, 16, 32, 32, 32)\n",
            "end_of_block\n",
            "CB1 (None, 8, 16, 16, 32)\n",
            "(None, 8, 16, 16, 64)\n",
            "(None, 8, 16, 16, 64)\n",
            "end_of_block\n",
            "CB2 (None, 4, 8, 8, 64)\n",
            "(None, 4, 8, 8, 128)\n",
            "(None, 4, 8, 8, 128)\n",
            "end_of_block\n",
            "CB3 (None, 2, 4, 4, 128)\n",
            "(None, 2, 4, 4, 256)\n",
            "(None, 2, 4, 4, 256)\n",
            "end_of_block\n",
            "CB4 (None, 2, 4, 4, 256)\n",
            "------------------\n",
            "(None, 4, 8, 8, 128)\n",
            "(None, 4, 8, 8, 128)\n",
            "end_of_block\n",
            "UB7 (None, 4, 8, 8, 128)\n",
            "(None, 8, 16, 16, 64)\n",
            "(None, 8, 16, 16, 64)\n",
            "end_of_block\n",
            "UB8 (None, 8, 16, 16, 64)\n",
            "(None, 16, 32, 32, 32)\n",
            "(None, 16, 32, 32, 32)\n",
            "end_of_block\n",
            "UB9 (None, 16, 32, 32, 32)\n",
            "C9 (None, 16, 32, 32, 32)\n",
            "C10 (None, 16, 32, 32, 1)\n"
          ]
        }
      ],
      "source": [
        "model = UNet3DModel(input_size=(16, 34, 34, 3), n_filters=32, filter_size = 3, n_classes=1)\n",
        "optimizer = tf.optimizers.Adam(learning_rate = 0.0005)\n",
        "model.compile(loss=custom_mse, optimizer=optimizer, metrics=['mse'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the model\n",
        "\n",
        "This cell implements two techniques to prevent overfitting. The first one is a learning rate scheduler that decreases the learning rate after a fixed number of epochs. The second one is an early stopping criteria that monitors the validation loss to ensure the model continues to learn."
      ],
      "metadata": {
        "id": "BLFb04QIevl2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "baIEkfpntuOW",
        "outputId": "e7367d37-c327-4e15-a1a7-05c4d3a1c44e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "2/2 [==============================] - 22s 3s/step - loss: 1.4019 - mse: 0.9410 - val_loss: 1.0538 - val_mse: 0.7189 - lr: 5.0000e-04\n",
            "Epoch 2/1000\n",
            "2/2 [==============================] - 1s 850ms/step - loss: 0.9618 - mse: 0.5797 - val_loss: 0.5595 - val_mse: 0.5150 - lr: 5.0000e-04\n",
            "Epoch 3/1000\n",
            "2/2 [==============================] - 1s 845ms/step - loss: 0.5105 - mse: 0.3542 - val_loss: 0.4022 - val_mse: 0.1932 - lr: 5.0000e-04\n",
            "Epoch 4/1000\n",
            "2/2 [==============================] - 1s 850ms/step - loss: 0.4792 - mse: 0.2089 - val_loss: 0.2798 - val_mse: 0.1112 - lr: 5.0000e-04\n",
            "Epoch 5/1000\n",
            "2/2 [==============================] - 1s 812ms/step - loss: 0.3512 - mse: 0.1451 - val_loss: 0.3203 - val_mse: 0.2373 - lr: 5.0000e-04\n",
            "Epoch 6/1000\n",
            "2/2 [==============================] - 2s 987ms/step - loss: 0.3886 - mse: 0.2644 - val_loss: 0.2428 - val_mse: 0.1393 - lr: 5.0000e-04\n",
            "Epoch 7/1000\n",
            "2/2 [==============================] - 1s 854ms/step - loss: 0.3063 - mse: 0.1424 - val_loss: 0.2386 - val_mse: 0.0920 - lr: 5.0000e-04\n",
            "Epoch 8/1000\n",
            "2/2 [==============================] - 1s 853ms/step - loss: 0.2958 - mse: 0.1106 - val_loss: 0.1900 - val_mse: 0.0839 - lr: 5.0000e-04\n",
            "Epoch 9/1000\n",
            "2/2 [==============================] - 1s 864ms/step - loss: 0.2654 - mse: 0.1613 - val_loss: 0.1785 - val_mse: 0.0849 - lr: 5.0000e-04\n",
            "Epoch 10/1000\n",
            "2/2 [==============================] - 1s 836ms/step - loss: 0.2149 - mse: 0.0921 - val_loss: 0.2100 - val_mse: 0.0819 - lr: 5.0000e-04\n",
            "Epoch 11/1000\n",
            "2/2 [==============================] - 1s 851ms/step - loss: 0.2007 - mse: 0.0776 - val_loss: 0.1565 - val_mse: 0.0900 - lr: 5.0000e-04\n",
            "Epoch 12/1000\n",
            "2/2 [==============================] - 2s 995ms/step - loss: 0.1685 - mse: 0.1235 - val_loss: 0.1448 - val_mse: 0.0657 - lr: 5.0000e-04\n",
            "Epoch 13/1000\n",
            "2/2 [==============================] - 1s 833ms/step - loss: 0.1377 - mse: 0.0666 - val_loss: 0.1797 - val_mse: 0.0802 - lr: 5.0000e-04\n",
            "Epoch 14/1000\n",
            "2/2 [==============================] - 1s 822ms/step - loss: 0.1211 - mse: 0.0641 - val_loss: 0.1573 - val_mse: 0.1009 - lr: 5.0000e-04\n",
            "Epoch 15/1000\n",
            "2/2 [==============================] - 1s 832ms/step - loss: 0.1245 - mse: 0.0981 - val_loss: 0.1789 - val_mse: 0.0886 - lr: 5.0000e-04\n",
            "Epoch 16/1000\n",
            "2/2 [==============================] - 2s 963ms/step - loss: 0.1227 - mse: 0.0697 - val_loss: 0.1718 - val_mse: 0.0861 - lr: 5.0000e-04\n",
            "Epoch 17/1000\n",
            "2/2 [==============================] - 2s 973ms/step - loss: 0.1033 - mse: 0.0684 - val_loss: 0.1517 - val_mse: 0.0910 - lr: 5.0000e-04\n",
            "Epoch 18/1000\n",
            "2/2 [==============================] - 1s 836ms/step - loss: 0.1084 - mse: 0.0814 - val_loss: 0.1603 - val_mse: 0.0778 - lr: 5.0000e-04\n",
            "Epoch 19/1000\n",
            "2/2 [==============================] - 1s 834ms/step - loss: 0.1041 - mse: 0.0586 - val_loss: 0.1501 - val_mse: 0.0711 - lr: 5.0000e-04\n",
            "Epoch 20/1000\n",
            "2/2 [==============================] - 2s 866ms/step - loss: 0.0942 - mse: 0.0599 - val_loss: 0.1377 - val_mse: 0.0796 - lr: 5.0000e-04\n",
            "Epoch 21/1000\n",
            "2/2 [==============================] - 1s 839ms/step - loss: 0.0960 - mse: 0.0708 - val_loss: 0.1397 - val_mse: 0.0662 - lr: 5.0000e-04\n",
            "Epoch 22/1000\n",
            "2/2 [==============================] - 1s 841ms/step - loss: 0.0934 - mse: 0.0520 - val_loss: 0.1453 - val_mse: 0.0678 - lr: 5.0000e-04\n",
            "Epoch 23/1000\n",
            "2/2 [==============================] - 1s 860ms/step - loss: 0.0881 - mse: 0.0523 - val_loss: 0.1356 - val_mse: 0.0772 - lr: 5.0000e-04\n",
            "Epoch 24/1000\n",
            "2/2 [==============================] - 1s 842ms/step - loss: 0.0878 - mse: 0.0644 - val_loss: 0.1385 - val_mse: 0.0699 - lr: 5.0000e-04\n",
            "Epoch 25/1000\n",
            "2/2 [==============================] - 1s 839ms/step - loss: 0.0823 - mse: 0.0501 - val_loss: 0.1445 - val_mse: 0.0724 - lr: 5.0000e-04\n",
            "Epoch 26/1000\n",
            "2/2 [==============================] - 1s 833ms/step - loss: 0.0799 - mse: 0.0510 - val_loss: 0.1388 - val_mse: 0.0784 - lr: 5.0000e-04\n",
            "Epoch 27/1000\n",
            "2/2 [==============================] - 1s 840ms/step - loss: 0.0806 - mse: 0.0590 - val_loss: 0.1416 - val_mse: 0.0736 - lr: 5.0000e-04\n",
            "Epoch 28/1000\n",
            "2/2 [==============================] - 1s 843ms/step - loss: 0.0772 - mse: 0.0490 - val_loss: 0.1410 - val_mse: 0.0721 - lr: 5.0000e-04\n",
            "Epoch 29/1000\n",
            "2/2 [==============================] - 2s 865ms/step - loss: 0.0751 - mse: 0.0485 - val_loss: 0.1340 - val_mse: 0.0740 - lr: 5.0000e-04\n",
            "Epoch 30/1000\n",
            "2/2 [==============================] - 2s 981ms/step - loss: 0.0749 - mse: 0.0526 - val_loss: 0.1339 - val_mse: 0.0683 - lr: 5.0000e-04\n",
            "Epoch 31/1000\n",
            "2/2 [==============================] - 2s 875ms/step - loss: 0.0739 - mse: 0.0454 - val_loss: 0.1325 - val_mse: 0.0671 - lr: 5.0000e-04\n",
            "Epoch 32/1000\n",
            "2/2 [==============================] - 2s 885ms/step - loss: 0.0714 - mse: 0.0468 - val_loss: 0.1299 - val_mse: 0.0705 - lr: 5.0000e-04\n",
            "Epoch 33/1000\n",
            "2/2 [==============================] - 2s 848ms/step - loss: 0.0721 - mse: 0.0484 - val_loss: 0.1333 - val_mse: 0.0664 - lr: 5.0000e-04\n",
            "Epoch 34/1000\n",
            "2/2 [==============================] - 1s 838ms/step - loss: 0.0697 - mse: 0.0443 - val_loss: 0.1298 - val_mse: 0.0687 - lr: 5.0000e-04\n",
            "Epoch 35/1000\n",
            "2/2 [==============================] - 1s 843ms/step - loss: 0.0687 - mse: 0.0466 - val_loss: 0.1312 - val_mse: 0.0662 - lr: 5.0000e-04\n",
            "Epoch 36/1000\n",
            "2/2 [==============================] - 2s 872ms/step - loss: 0.0680 - mse: 0.0427 - val_loss: 0.1289 - val_mse: 0.0665 - lr: 5.0000e-04\n",
            "Epoch 37/1000\n",
            "2/2 [==============================] - 2s 887ms/step - loss: 0.0664 - mse: 0.0442 - val_loss: 0.1278 - val_mse: 0.0672 - lr: 5.0000e-04\n",
            "Epoch 38/1000\n",
            "2/2 [==============================] - 2s 847ms/step - loss: 0.0662 - mse: 0.0437 - val_loss: 0.1291 - val_mse: 0.0653 - lr: 5.0000e-04\n",
            "Epoch 39/1000\n",
            "2/2 [==============================] - 2s 851ms/step - loss: 0.0658 - mse: 0.0437 - val_loss: 0.1283 - val_mse: 0.0659 - lr: 5.0000e-04\n",
            "Epoch 40/1000\n",
            "2/2 [==============================] - 2s 850ms/step - loss: 0.0650 - mse: 0.0418 - val_loss: 0.1282 - val_mse: 0.0659 - lr: 5.0000e-04\n",
            "Epoch 41/1000\n",
            "2/2 [==============================] - 2s 938ms/step - loss: 0.0645 - mse: 0.0441 - val_loss: 0.1288 - val_mse: 0.0647 - lr: 5.0000e-04\n",
            "Epoch 42/1000\n",
            "2/2 [==============================] - 2s 895ms/step - loss: 0.0635 - mse: 0.0400 - val_loss: 0.1265 - val_mse: 0.0651 - lr: 5.0000e-04\n",
            "Epoch 43/1000\n",
            "2/2 [==============================] - 2s 1s/step - loss: 0.0621 - mse: 0.0417 - val_loss: 0.1254 - val_mse: 0.0651 - lr: 5.0000e-04\n",
            "Epoch 44/1000\n",
            "2/2 [==============================] - 2s 864ms/step - loss: 0.0613 - mse: 0.0408 - val_loss: 0.1275 - val_mse: 0.0632 - lr: 5.0000e-04\n",
            "Epoch 45/1000\n",
            "2/2 [==============================] - 2s 886ms/step - loss: 0.0611 - mse: 0.0394 - val_loss: 0.1246 - val_mse: 0.0646 - lr: 5.0000e-04\n",
            "Epoch 46/1000\n",
            "2/2 [==============================] - 2s 861ms/step - loss: 0.0597 - mse: 0.0399 - val_loss: 0.1282 - val_mse: 0.0641 - lr: 5.0000e-04\n",
            "Epoch 47/1000\n",
            "2/2 [==============================] - 2s 860ms/step - loss: 0.0599 - mse: 0.0397 - val_loss: 0.1246 - val_mse: 0.0642 - lr: 5.0000e-04\n",
            "Epoch 48/1000\n",
            "2/2 [==============================] - 2s 858ms/step - loss: 0.0586 - mse: 0.0384 - val_loss: 0.1248 - val_mse: 0.0635 - lr: 5.0000e-04\n",
            "Epoch 49/1000\n",
            "2/2 [==============================] - 2s 886ms/step - loss: 0.0574 - mse: 0.0384 - val_loss: 0.1237 - val_mse: 0.0634 - lr: 5.0000e-04\n",
            "Epoch 50/1000\n",
            "2/2 [==============================] - 2s 895ms/step - loss: 0.0582 - mse: 0.0379 - val_loss: 0.1232 - val_mse: 0.0644 - lr: 5.0000e-04\n",
            "Epoch 51/1000\n",
            "2/2 [==============================] - 2s 872ms/step - loss: 0.0568 - mse: 0.0398 - val_loss: 0.1283 - val_mse: 0.0626 - lr: 5.0000e-04\n",
            "Epoch 52/1000\n",
            "2/2 [==============================] - 2s 892ms/step - loss: 0.0566 - mse: 0.0361 - val_loss: 0.1215 - val_mse: 0.0631 - lr: 5.0000e-04\n",
            "Epoch 53/1000\n",
            "2/2 [==============================] - 2s 875ms/step - loss: 0.0547 - mse: 0.0378 - val_loss: 0.1302 - val_mse: 0.0635 - lr: 5.0000e-04\n",
            "Epoch 54/1000\n",
            "2/2 [==============================] - 2s 863ms/step - loss: 0.0563 - mse: 0.0368 - val_loss: 0.1229 - val_mse: 0.0630 - lr: 5.0000e-04\n",
            "Epoch 55/1000\n",
            "2/2 [==============================] - 2s 858ms/step - loss: 0.0540 - mse: 0.0367 - val_loss: 0.1250 - val_mse: 0.0623 - lr: 5.0000e-04\n",
            "Epoch 56/1000\n",
            "2/2 [==============================] - 2s 864ms/step - loss: 0.0530 - mse: 0.0348 - val_loss: 0.1211 - val_mse: 0.0643 - lr: 5.0000e-04\n",
            "Epoch 57/1000\n",
            "2/2 [==============================] - 2s 861ms/step - loss: 0.0539 - mse: 0.0370 - val_loss: 0.1247 - val_mse: 0.0613 - lr: 5.0000e-04\n",
            "Epoch 58/1000\n",
            "2/2 [==============================] - 2s 891ms/step - loss: 0.0519 - mse: 0.0348 - val_loss: 0.1207 - val_mse: 0.0625 - lr: 5.0000e-04\n",
            "Epoch 59/1000\n",
            "2/2 [==============================] - 2s 875ms/step - loss: 0.0510 - mse: 0.0345 - val_loss: 0.1247 - val_mse: 0.0616 - lr: 5.0000e-04\n",
            "Epoch 60/1000\n",
            "2/2 [==============================] - 2s 868ms/step - loss: 0.0503 - mse: 0.0338 - val_loss: 0.1208 - val_mse: 0.0622 - lr: 5.0000e-04\n",
            "Epoch 61/1000\n",
            "2/2 [==============================] - 2s 870ms/step - loss: 0.0502 - mse: 0.0337 - val_loss: 0.1211 - val_mse: 0.0612 - lr: 5.0000e-04\n",
            "Epoch 62/1000\n",
            "2/2 [==============================] - 2s 860ms/step - loss: 0.0487 - mse: 0.0339 - val_loss: 0.1238 - val_mse: 0.0611 - lr: 5.0000e-04\n",
            "Epoch 63/1000\n",
            "2/2 [==============================] - 2s 872ms/step - loss: 0.0480 - mse: 0.0321 - val_loss: 0.1214 - val_mse: 0.0620 - lr: 5.0000e-04\n",
            "Epoch 64/1000\n",
            "2/2 [==============================] - 2s 998ms/step - loss: 0.0470 - mse: 0.0327 - val_loss: 0.1260 - val_mse: 0.0621 - lr: 5.0000e-04\n",
            "Epoch 65/1000\n",
            "2/2 [==============================] - 2s 1s/step - loss: 0.0483 - mse: 0.0333 - val_loss: 0.1250 - val_mse: 0.0612 - lr: 5.0000e-04\n",
            "Epoch 66/1000\n",
            "2/2 [==============================] - 2s 1s/step - loss: 0.0471 - mse: 0.0310 - val_loss: 0.1187 - val_mse: 0.0619 - lr: 5.0000e-04\n",
            "Epoch 67/1000\n",
            "2/2 [==============================] - 2s 870ms/step - loss: 0.0462 - mse: 0.0322 - val_loss: 0.1256 - val_mse: 0.0613 - lr: 5.0000e-04\n",
            "Epoch 68/1000\n",
            "2/2 [==============================] - 2s 861ms/step - loss: 0.0454 - mse: 0.0310 - val_loss: 0.1184 - val_mse: 0.0600 - lr: 5.0000e-04\n",
            "Epoch 69/1000\n",
            "2/2 [==============================] - 2s 863ms/step - loss: 0.0447 - mse: 0.0304 - val_loss: 0.1213 - val_mse: 0.0614 - lr: 5.0000e-04\n",
            "Epoch 70/1000\n",
            "2/2 [==============================] - 2s 864ms/step - loss: 0.0439 - mse: 0.0317 - val_loss: 0.1267 - val_mse: 0.0623 - lr: 5.0000e-04\n",
            "Epoch 71/1000\n",
            "2/2 [==============================] - 2s 995ms/step - loss: 0.0435 - mse: 0.0299 - val_loss: 0.1187 - val_mse: 0.0600 - lr: 5.0000e-04\n",
            "Epoch 72/1000\n",
            "2/2 [==============================] - 2s 886ms/step - loss: 0.0439 - mse: 0.0295 - val_loss: 0.1196 - val_mse: 0.0632 - lr: 5.0000e-04\n",
            "Epoch 73/1000\n",
            "2/2 [==============================] - 2s 864ms/step - loss: 0.0439 - mse: 0.0308 - val_loss: 0.1236 - val_mse: 0.0601 - lr: 5.0000e-04\n",
            "Epoch 74/1000\n",
            "2/2 [==============================] - 2s 865ms/step - loss: 0.0419 - mse: 0.0294 - val_loss: 0.1197 - val_mse: 0.0604 - lr: 5.0000e-04\n",
            "Epoch 75/1000\n",
            "2/2 [==============================] - 2s 865ms/step - loss: 0.0403 - mse: 0.0280 - val_loss: 0.1235 - val_mse: 0.0612 - lr: 5.0000e-04\n",
            "Epoch 76/1000\n",
            "2/2 [==============================] - 2s 859ms/step - loss: 0.0414 - mse: 0.0303 - val_loss: 0.1329 - val_mse: 0.0651 - lr: 5.0000e-04\n",
            "Epoch 77/1000\n",
            "2/2 [==============================] - 2s 897ms/step - loss: 0.0425 - mse: 0.0289 - val_loss: 0.1176 - val_mse: 0.0611 - lr: 5.0000e-04\n",
            "Epoch 78/1000\n",
            "2/2 [==============================] - 2s 861ms/step - loss: 0.0409 - mse: 0.0283 - val_loss: 0.1223 - val_mse: 0.0601 - lr: 5.0000e-04\n",
            "Epoch 79/1000\n",
            "2/2 [==============================] - 2s 861ms/step - loss: 0.0392 - mse: 0.0283 - val_loss: 0.1275 - val_mse: 0.0622 - lr: 5.0000e-04\n",
            "Epoch 80/1000\n",
            "2/2 [==============================] - 2s 882ms/step - loss: 0.0386 - mse: 0.0262 - val_loss: 0.1173 - val_mse: 0.0632 - lr: 5.0000e-04\n",
            "Epoch 81/1000\n",
            "2/2 [==============================] - 2s 902ms/step - loss: 0.0399 - mse: 0.0290 - val_loss: 0.1280 - val_mse: 0.0618 - lr: 5.0000e-04\n",
            "Epoch 82/1000\n",
            "2/2 [==============================] - 2s 871ms/step - loss: 0.0384 - mse: 0.0275 - val_loss: 0.1216 - val_mse: 0.0604 - lr: 5.0000e-04\n",
            "Epoch 83/1000\n",
            "2/2 [==============================] - 2s 858ms/step - loss: 0.0358 - mse: 0.0249 - val_loss: 0.1190 - val_mse: 0.0610 - lr: 5.0000e-04\n",
            "Epoch 84/1000\n",
            "2/2 [==============================] - 2s 859ms/step - loss: 0.0372 - mse: 0.0270 - val_loss: 0.1195 - val_mse: 0.0600 - lr: 5.0000e-04\n",
            "Epoch 85/1000\n",
            "2/2 [==============================] - 2s 860ms/step - loss: 0.0342 - mse: 0.0250 - val_loss: 0.1225 - val_mse: 0.0608 - lr: 5.0000e-04\n",
            "Epoch 86/1000\n",
            "2/2 [==============================] - 2s 902ms/step - loss: 0.0334 - mse: 0.0239 - val_loss: 0.1190 - val_mse: 0.0601 - lr: 5.0000e-04\n",
            "Epoch 87/1000\n",
            "2/2 [==============================] - 2s 882ms/step - loss: 0.0368 - mse: 0.0264 - val_loss: 0.1178 - val_mse: 0.0605 - lr: 5.0000e-04\n",
            "Epoch 88/1000\n",
            "2/2 [==============================] - 2s 882ms/step - loss: 0.0343 - mse: 0.0261 - val_loss: 0.1281 - val_mse: 0.0606 - lr: 5.0000e-04\n",
            "Epoch 89/1000\n",
            "2/2 [==============================] - 2s 872ms/step - loss: 0.0342 - mse: 0.0230 - val_loss: 0.1197 - val_mse: 0.0636 - lr: 5.0000e-04\n",
            "Epoch 90/1000\n",
            "2/2 [==============================] - 2s 877ms/step - loss: 0.0374 - mse: 0.0262 - val_loss: 0.1191 - val_mse: 0.0580 - lr: 5.0000e-04\n",
            "Epoch 91/1000\n",
            "2/2 [==============================] - 2s 995ms/step - loss: 0.0341 - mse: 0.0254 - val_loss: 0.1354 - val_mse: 0.0676 - lr: 5.0000e-04\n",
            "Epoch 92/1000\n",
            "2/2 [==============================] - 2s 874ms/step - loss: 0.0387 - mse: 0.0268 - val_loss: 0.1187 - val_mse: 0.0679 - lr: 5.0000e-04\n",
            "Epoch 93/1000\n",
            "2/2 [==============================] - 2s 852ms/step - loss: 0.0433 - mse: 0.0317 - val_loss: 0.1290 - val_mse: 0.0608 - lr: 5.0000e-04\n",
            "Epoch 94/1000\n",
            "2/2 [==============================] - 2s 945ms/step - loss: 0.0413 - mse: 0.0287 - val_loss: 0.1231 - val_mse: 0.0598 - lr: 5.0000e-04\n",
            "Epoch 95/1000\n",
            "2/2 [==============================] - 2s 922ms/step - loss: 0.0365 - mse: 0.0240 - val_loss: 0.1166 - val_mse: 0.0597 - lr: 5.0000e-04\n",
            "Epoch 96/1000\n",
            "2/2 [==============================] - 2s 863ms/step - loss: 0.0348 - mse: 0.0269 - val_loss: 0.1284 - val_mse: 0.0625 - lr: 5.0000e-04\n",
            "Epoch 97/1000\n",
            "2/2 [==============================] - 2s 855ms/step - loss: 0.0331 - mse: 0.0223 - val_loss: 0.1198 - val_mse: 0.0631 - lr: 5.0000e-04\n",
            "Epoch 98/1000\n",
            "2/2 [==============================] - 2s 854ms/step - loss: 0.0331 - mse: 0.0249 - val_loss: 0.1247 - val_mse: 0.0604 - lr: 5.0000e-04\n",
            "Epoch 99/1000\n",
            "2/2 [==============================] - 2s 885ms/step - loss: 0.0312 - mse: 0.0221 - val_loss: 0.1150 - val_mse: 0.0574 - lr: 5.0000e-04\n",
            "Epoch 100/1000\n",
            "2/2 [==============================] - 2s 872ms/step - loss: 0.0300 - mse: 0.0212 - val_loss: 0.1248 - val_mse: 0.0612 - lr: 5.0000e-04\n",
            "Epoch 101/1000\n",
            "2/2 [==============================] - 2s 860ms/step - loss: 0.0293 - mse: 0.0215 - val_loss: 0.1258 - val_mse: 0.0637 - lr: 5.0000e-04\n",
            "Epoch 102/1000\n",
            "2/2 [==============================] - 2s 861ms/step - loss: 0.0277 - mse: 0.0196 - val_loss: 0.1182 - val_mse: 0.0595 - lr: 5.0000e-04\n",
            "Epoch 103/1000\n",
            "2/2 [==============================] - 2s 857ms/step - loss: 0.0274 - mse: 0.0202 - val_loss: 0.1236 - val_mse: 0.0613 - lr: 5.0000e-04\n",
            "Epoch 104/1000\n",
            "2/2 [==============================] - 2s 860ms/step - loss: 0.0261 - mse: 0.0193 - val_loss: 0.1199 - val_mse: 0.0600 - lr: 5.0000e-04\n",
            "Epoch 105/1000\n",
            "2/2 [==============================] - 2s 879ms/step - loss: 0.0256 - mse: 0.0184 - val_loss: 0.1214 - val_mse: 0.0601 - lr: 5.0000e-04\n",
            "Epoch 106/1000\n",
            "2/2 [==============================] - 2s 864ms/step - loss: 0.0248 - mse: 0.0183 - val_loss: 0.1257 - val_mse: 0.0619 - lr: 5.0000e-04\n",
            "Epoch 107/1000\n",
            "2/2 [==============================] - 2s 861ms/step - loss: 0.0242 - mse: 0.0175 - val_loss: 0.1209 - val_mse: 0.0605 - lr: 5.0000e-04\n",
            "Epoch 108/1000\n",
            "2/2 [==============================] - 2s 861ms/step - loss: 0.0240 - mse: 0.0179 - val_loss: 0.1212 - val_mse: 0.0615 - lr: 5.0000e-04\n",
            "Epoch 109/1000\n",
            "2/2 [==============================] - 2s 864ms/step - loss: 0.0230 - mse: 0.0173 - val_loss: 0.1222 - val_mse: 0.0605 - lr: 5.0000e-04\n",
            "Epoch 110/1000\n",
            "2/2 [==============================] - 2s 990ms/step - loss: 0.0220 - mse: 0.0164 - val_loss: 0.1260 - val_mse: 0.0621 - lr: 5.0000e-04\n",
            "Epoch 111/1000\n",
            "2/2 [==============================] - 2s 867ms/step - loss: 0.0217 - mse: 0.0160 - val_loss: 0.1212 - val_mse: 0.0606 - lr: 5.0000e-04\n",
            "Epoch 112/1000\n",
            "2/2 [==============================] - 2s 996ms/step - loss: 0.0222 - mse: 0.0163 - val_loss: 0.1190 - val_mse: 0.0606 - lr: 5.0000e-04\n",
            "Epoch 113/1000\n",
            "2/2 [==============================] - 2s 863ms/step - loss: 0.0220 - mse: 0.0168 - val_loss: 0.1243 - val_mse: 0.0605 - lr: 5.0000e-04\n",
            "Epoch 114/1000\n",
            "2/2 [==============================] - 2s 865ms/step - loss: 0.0208 - mse: 0.0158 - val_loss: 0.1305 - val_mse: 0.0647 - lr: 5.0000e-04\n",
            "Epoch 115/1000\n",
            "2/2 [==============================] - 2s 857ms/step - loss: 0.0202 - mse: 0.0151 - val_loss: 0.1187 - val_mse: 0.0587 - lr: 5.0000e-04\n",
            "Epoch 116/1000\n",
            "2/2 [==============================] - 2s 859ms/step - loss: 0.0213 - mse: 0.0157 - val_loss: 0.1218 - val_mse: 0.0617 - lr: 5.0000e-04\n",
            "Epoch 117/1000\n",
            "2/2 [==============================] - 2s 1s/step - loss: 0.0198 - mse: 0.0154 - val_loss: 0.1280 - val_mse: 0.0616 - lr: 5.0000e-04\n",
            "Epoch 118/1000\n",
            "2/2 [==============================] - 2s 870ms/step - loss: 0.0209 - mse: 0.0153 - val_loss: 0.1289 - val_mse: 0.0638 - lr: 5.0000e-04\n",
            "Epoch 119/1000\n",
            "2/2 [==============================] - 2s 877ms/step - loss: 0.0194 - mse: 0.0143 - val_loss: 0.1192 - val_mse: 0.0599 - lr: 5.0000e-04\n",
            "Epoch 120/1000\n",
            "2/2 [==============================] - 2s 990ms/step - loss: 0.0211 - mse: 0.0162 - val_loss: 0.1216 - val_mse: 0.0617 - lr: 5.0000e-04\n",
            "Epoch 121/1000\n",
            "2/2 [==============================] - 2s 876ms/step - loss: 0.0190 - mse: 0.0148 - val_loss: 0.1265 - val_mse: 0.0606 - lr: 5.0000e-04\n",
            "Epoch 122/1000\n",
            "2/2 [==============================] - 2s 989ms/step - loss: 0.0184 - mse: 0.0138 - val_loss: 0.1287 - val_mse: 0.0644 - lr: 5.0000e-04\n",
            "Epoch 123/1000\n",
            "2/2 [==============================] - 2s 900ms/step - loss: 0.0179 - mse: 0.0138 - val_loss: 0.1198 - val_mse: 0.0595 - lr: 5.0000e-04\n",
            "Epoch 124/1000\n",
            "2/2 [==============================] - 2s 946ms/step - loss: 0.0183 - mse: 0.0139 - val_loss: 0.1236 - val_mse: 0.0613 - lr: 5.0000e-04\n",
            "Epoch 125/1000\n",
            "2/2 [==============================] - 2s 878ms/step - loss: 0.0168 - mse: 0.0132 - val_loss: 0.1279 - val_mse: 0.0622 - lr: 5.0000e-04\n",
            "Epoch 126/1000\n",
            "2/2 [==============================] - 2s 863ms/step - loss: 0.0175 - mse: 0.0134 - val_loss: 0.1274 - val_mse: 0.0625 - lr: 5.0000e-04\n",
            "Epoch 127/1000\n",
            "2/2 [==============================] - 2s 861ms/step - loss: 0.0163 - mse: 0.0126 - val_loss: 0.1234 - val_mse: 0.0607 - lr: 5.0000e-04\n",
            "Epoch 128/1000\n",
            "2/2 [==============================] - 2s 869ms/step - loss: 0.0159 - mse: 0.0121 - val_loss: 0.1221 - val_mse: 0.0607 - lr: 5.0000e-04\n",
            "Epoch 129/1000\n",
            "2/2 [==============================] - 2s 870ms/step - loss: 0.0156 - mse: 0.0123 - val_loss: 0.1269 - val_mse: 0.0621 - lr: 5.0000e-04\n",
            "Epoch 130/1000\n",
            "2/2 [==============================] - 2s 873ms/step - loss: 0.0154 - mse: 0.0122 - val_loss: 0.1285 - val_mse: 0.0625 - lr: 5.0000e-04\n",
            "Epoch 131/1000\n",
            "2/2 [==============================] - 2s 860ms/step - loss: 0.0157 - mse: 0.0122 - val_loss: 0.1255 - val_mse: 0.0613 - lr: 5.0000e-04\n",
            "Epoch 132/1000\n",
            "2/2 [==============================] - 2s 868ms/step - loss: 0.0146 - mse: 0.0113 - val_loss: 0.1224 - val_mse: 0.0610 - lr: 5.0000e-04\n",
            "Epoch 133/1000\n",
            "2/2 [==============================] - 2s 866ms/step - loss: 0.0154 - mse: 0.0121 - val_loss: 0.1202 - val_mse: 0.0597 - lr: 5.0000e-04\n",
            "Epoch 134/1000\n",
            "2/2 [==============================] - 2s 900ms/step - loss: 0.0146 - mse: 0.0117 - val_loss: 0.1277 - val_mse: 0.0626 - lr: 5.0000e-04\n",
            "Epoch 135/1000\n",
            "2/2 [==============================] - 2s 879ms/step - loss: 0.0143 - mse: 0.0115 - val_loss: 0.1301 - val_mse: 0.0627 - lr: 5.0000e-04\n",
            "Epoch 136/1000\n",
            "2/2 [==============================] - 2s 863ms/step - loss: 0.0144 - mse: 0.0112 - val_loss: 0.1248 - val_mse: 0.0609 - lr: 5.0000e-04\n",
            "Epoch 137/1000\n",
            "2/2 [==============================] - 2s 860ms/step - loss: 0.0133 - mse: 0.0105 - val_loss: 0.1245 - val_mse: 0.0619 - lr: 5.0000e-04\n",
            "Epoch 138/1000\n",
            "2/2 [==============================] - 2s 998ms/step - loss: 0.0133 - mse: 0.0106 - val_loss: 0.1216 - val_mse: 0.0596 - lr: 5.0000e-04\n",
            "Epoch 139/1000\n",
            "2/2 [==============================] - 2s 880ms/step - loss: 0.0132 - mse: 0.0106 - val_loss: 0.1244 - val_mse: 0.0614 - lr: 5.0000e-04\n",
            "Epoch 140/1000\n",
            "2/2 [==============================] - 2s 890ms/step - loss: 0.0127 - mse: 0.0103 - val_loss: 0.1263 - val_mse: 0.0613 - lr: 5.0000e-04\n",
            "Epoch 141/1000\n",
            "2/2 [==============================] - 2s 877ms/step - loss: 0.0125 - mse: 0.0101 - val_loss: 0.1275 - val_mse: 0.0619 - lr: 5.0000e-04\n",
            "Epoch 142/1000\n",
            "2/2 [==============================] - 2s 893ms/step - loss: 0.0124 - mse: 0.0100 - val_loss: 0.1294 - val_mse: 0.0635 - lr: 5.0000e-04\n",
            "Epoch 143/1000\n",
            "2/2 [==============================] - 2s 881ms/step - loss: 0.0125 - mse: 0.0102 - val_loss: 0.1270 - val_mse: 0.0610 - lr: 5.0000e-04\n",
            "Epoch 144/1000\n",
            "2/2 [==============================] - 2s 879ms/step - loss: 0.0123 - mse: 0.0098 - val_loss: 0.1294 - val_mse: 0.0631 - lr: 5.0000e-04\n",
            "Epoch 145/1000\n",
            "2/2 [==============================] - 2s 854ms/step - loss: 0.0122 - mse: 0.0099 - val_loss: 0.1295 - val_mse: 0.0630 - lr: 5.0000e-04\n",
            "Epoch 146/1000\n",
            "2/2 [==============================] - 2s 867ms/step - loss: 0.0127 - mse: 0.0102 - val_loss: 0.1295 - val_mse: 0.0618 - lr: 5.0000e-04\n",
            "Epoch 147/1000\n",
            "2/2 [==============================] - 2s 876ms/step - loss: 0.0123 - mse: 0.0098 - val_loss: 0.1297 - val_mse: 0.0642 - lr: 5.0000e-04\n",
            "Epoch 148/1000\n",
            "2/2 [==============================] - 2s 877ms/step - loss: 0.0118 - mse: 0.0096 - val_loss: 0.1248 - val_mse: 0.0601 - lr: 5.0000e-04\n",
            "Epoch 149/1000\n",
            "2/2 [==============================] - 2s 867ms/step - loss: 0.0113 - mse: 0.0092 - val_loss: 0.1273 - val_mse: 0.0627 - lr: 5.0000e-04\n",
            "Epoch 150/1000\n",
            "2/2 [==============================] - 2s 864ms/step - loss: 0.0111 - mse: 0.0090 - val_loss: 0.1254 - val_mse: 0.0613 - lr: 5.0000e-04\n",
            "Epoch 151/1000\n",
            "2/2 [==============================] - 2s 876ms/step - loss: 0.0108 - mse: 0.0089 - val_loss: 0.1252 - val_mse: 0.0611 - lr: 5.0000e-04\n",
            "Epoch 152/1000\n",
            "2/2 [==============================] - 2s 869ms/step - loss: 0.0107 - mse: 0.0087 - val_loss: 0.1259 - val_mse: 0.0624 - lr: 5.0000e-04\n",
            "Epoch 153/1000\n",
            "2/2 [==============================] - 2s 888ms/step - loss: 0.0106 - mse: 0.0088 - val_loss: 0.1242 - val_mse: 0.0605 - lr: 5.0000e-04\n",
            "Epoch 154/1000\n",
            "2/2 [==============================] - 2s 874ms/step - loss: 0.0109 - mse: 0.0090 - val_loss: 0.1212 - val_mse: 0.0605 - lr: 5.0000e-04\n",
            "Epoch 155/1000\n",
            "2/2 [==============================] - 2s 862ms/step - loss: 0.0131 - mse: 0.0101 - val_loss: 0.1180 - val_mse: 0.0610 - lr: 5.0000e-04\n",
            "Epoch 156/1000\n",
            "2/2 [==============================] - 2s 879ms/step - loss: 0.0194 - mse: 0.0150 - val_loss: 0.1189 - val_mse: 0.0599 - lr: 5.0000e-04\n",
            "Epoch 157/1000\n",
            "2/2 [==============================] - 2s 862ms/step - loss: 0.0129 - mse: 0.0105 - val_loss: 0.1301 - val_mse: 0.0625 - lr: 5.0000e-04\n",
            "Epoch 158/1000\n",
            "2/2 [==============================] - 2s 887ms/step - loss: 0.0130 - mse: 0.0104 - val_loss: 0.1291 - val_mse: 0.0617 - lr: 5.0000e-04\n",
            "Epoch 159/1000\n",
            "2/2 [==============================] - 2s 875ms/step - loss: 0.0115 - mse: 0.0089 - val_loss: 0.1216 - val_mse: 0.0623 - lr: 5.0000e-04\n",
            "Epoch 160/1000\n",
            "2/2 [==============================] - 2s 893ms/step - loss: 0.0192 - mse: 0.0143 - val_loss: 0.1191 - val_mse: 0.0583 - lr: 5.0000e-04\n",
            "Epoch 161/1000\n",
            "2/2 [==============================] - 2s 860ms/step - loss: 0.0144 - mse: 0.0118 - val_loss: 0.1464 - val_mse: 0.0718 - lr: 5.0000e-04\n",
            "Epoch 162/1000\n",
            "2/2 [==============================] - 2s 872ms/step - loss: 0.0178 - mse: 0.0133 - val_loss: 0.1181 - val_mse: 0.0623 - lr: 5.0000e-04\n",
            "Epoch 163/1000\n",
            "2/2 [==============================] - 2s 870ms/step - loss: 0.0236 - mse: 0.0183 - val_loss: 0.1260 - val_mse: 0.0608 - lr: 5.0000e-04\n",
            "Epoch 164/1000\n",
            "2/2 [==============================] - 2s 864ms/step - loss: 0.0213 - mse: 0.0170 - val_loss: 0.1315 - val_mse: 0.0601 - lr: 5.0000e-04\n",
            "Epoch 165/1000\n",
            "2/2 [==============================] - 2s 864ms/step - loss: 0.0191 - mse: 0.0125 - val_loss: 0.1245 - val_mse: 0.0667 - lr: 5.0000e-04\n",
            "Epoch 166/1000\n",
            "2/2 [==============================] - 2s 863ms/step - loss: 0.0197 - mse: 0.0140 - val_loss: 0.1284 - val_mse: 0.0592 - lr: 5.0000e-04\n",
            "Epoch 167/1000\n",
            "2/2 [==============================] - 2s 863ms/step - loss: 0.0178 - mse: 0.0120 - val_loss: 0.1261 - val_mse: 0.0650 - lr: 5.0000e-04\n",
            "Epoch 168/1000\n",
            "2/2 [==============================] - 2s 858ms/step - loss: 0.0170 - mse: 0.0132 - val_loss: 0.1189 - val_mse: 0.0584 - lr: 5.0000e-04\n",
            "Epoch 169/1000\n",
            "2/2 [==============================] - 2s 861ms/step - loss: 0.0150 - mse: 0.0121 - val_loss: 0.1280 - val_mse: 0.0617 - lr: 5.0000e-04\n",
            "Epoch 170/1000\n",
            "2/2 [==============================] - 2s 868ms/step - loss: 0.0141 - mse: 0.0107 - val_loss: 0.1216 - val_mse: 0.0624 - lr: 5.0000e-04\n",
            "Epoch 171/1000\n",
            "2/2 [==============================] - 2s 874ms/step - loss: 0.0138 - mse: 0.0108 - val_loss: 0.1278 - val_mse: 0.0611 - lr: 5.0000e-04\n",
            "Epoch 172/1000\n",
            "2/2 [==============================] - 2s 871ms/step - loss: 0.0138 - mse: 0.0106 - val_loss: 0.1253 - val_mse: 0.0623 - lr: 5.0000e-04\n",
            "Epoch 173/1000\n",
            "2/2 [==============================] - 2s 870ms/step - loss: 0.0125 - mse: 0.0100 - val_loss: 0.1224 - val_mse: 0.0606 - lr: 5.0000e-04\n",
            "Epoch 174/1000\n",
            "2/2 [==============================] - 2s 869ms/step - loss: 0.0115 - mse: 0.0094 - val_loss: 0.1279 - val_mse: 0.0616 - lr: 5.0000e-04\n",
            "Epoch 175/1000\n",
            "2/2 [==============================] - 2s 867ms/step - loss: 0.0111 - mse: 0.0088 - val_loss: 0.1231 - val_mse: 0.0620 - lr: 5.0000e-04\n",
            "Epoch 176/1000\n",
            "2/2 [==============================] - 2s 878ms/step - loss: 0.0109 - mse: 0.0089 - val_loss: 0.1261 - val_mse: 0.0610 - lr: 5.0000e-04\n",
            "Epoch 177/1000\n",
            "2/2 [==============================] - 2s 877ms/step - loss: 0.0105 - mse: 0.0086 - val_loss: 0.1246 - val_mse: 0.0613 - lr: 5.0000e-04\n",
            "Epoch 178/1000\n",
            "2/2 [==============================] - 2s 1000ms/step - loss: 0.0100 - mse: 0.0083 - val_loss: 0.1253 - val_mse: 0.0619 - lr: 5.0000e-04\n",
            "Epoch 179/1000\n",
            "2/2 [==============================] - 2s 888ms/step - loss: 0.0098 - mse: 0.0081 - val_loss: 0.1253 - val_mse: 0.0605 - lr: 5.0000e-04\n",
            "Epoch 180/1000\n",
            "2/2 [==============================] - 2s 869ms/step - loss: 0.0095 - mse: 0.0078 - val_loss: 0.1246 - val_mse: 0.0618 - lr: 5.0000e-04\n",
            "Epoch 181/1000\n",
            "2/2 [==============================] - 2s 996ms/step - loss: 0.0093 - mse: 0.0078 - val_loss: 0.1251 - val_mse: 0.0612 - lr: 5.0000e-04\n",
            "Epoch 182/1000\n",
            "2/2 [==============================] - 2s 888ms/step - loss: 0.0091 - mse: 0.0076 - val_loss: 0.1250 - val_mse: 0.0611 - lr: 5.0000e-04\n",
            "Epoch 183/1000\n",
            "2/2 [==============================] - 2s 862ms/step - loss: 0.0089 - mse: 0.0075 - val_loss: 0.1254 - val_mse: 0.0618 - lr: 5.0000e-04\n",
            "Epoch 184/1000\n",
            "2/2 [==============================] - 2s 861ms/step - loss: 0.0088 - mse: 0.0074 - val_loss: 0.1252 - val_mse: 0.0610 - lr: 5.0000e-04\n",
            "Epoch 185/1000\n",
            "2/2 [==============================] - 2s 864ms/step - loss: 0.0087 - mse: 0.0073 - val_loss: 0.1259 - val_mse: 0.0617 - lr: 5.0000e-04\n",
            "Epoch 186/1000\n",
            "2/2 [==============================] - 2s 866ms/step - loss: 0.0085 - mse: 0.0072 - val_loss: 0.1252 - val_mse: 0.0616 - lr: 5.0000e-04\n",
            "Epoch 187/1000\n",
            "2/2 [==============================] - 2s 861ms/step - loss: 0.0084 - mse: 0.0072 - val_loss: 0.1257 - val_mse: 0.0613 - lr: 5.0000e-04\n",
            "Epoch 188/1000\n",
            "2/2 [==============================] - 2s 862ms/step - loss: 0.0084 - mse: 0.0071 - val_loss: 0.1262 - val_mse: 0.0619 - lr: 5.0000e-04\n",
            "Epoch 189/1000\n",
            "2/2 [==============================] - 2s 854ms/step - loss: 0.0082 - mse: 0.0070 - val_loss: 0.1246 - val_mse: 0.0613 - lr: 5.0000e-04\n",
            "Epoch 190/1000\n",
            "2/2 [==============================] - 2s 861ms/step - loss: 0.0081 - mse: 0.0070 - val_loss: 0.1265 - val_mse: 0.0616 - lr: 5.0000e-04\n",
            "Epoch 191/1000\n",
            "2/2 [==============================] - 2s 996ms/step - loss: 0.0080 - mse: 0.0069 - val_loss: 0.1256 - val_mse: 0.0618 - lr: 5.0000e-04\n",
            "Epoch 192/1000\n",
            "2/2 [==============================] - 2s 884ms/step - loss: 0.0079 - mse: 0.0068 - val_loss: 0.1248 - val_mse: 0.0609 - lr: 5.0000e-04\n",
            "Epoch 193/1000\n",
            "2/2 [==============================] - 2s 965ms/step - loss: 0.0079 - mse: 0.0068 - val_loss: 0.1267 - val_mse: 0.0620 - lr: 5.0000e-04\n",
            "Epoch 194/1000\n",
            "2/2 [==============================] - 2s 861ms/step - loss: 0.0078 - mse: 0.0067 - val_loss: 0.1259 - val_mse: 0.0614 - lr: 5.0000e-04\n",
            "Epoch 195/1000\n",
            "2/2 [==============================] - 2s 865ms/step - loss: 0.0077 - mse: 0.0066 - val_loss: 0.1256 - val_mse: 0.0617 - lr: 5.0000e-04\n",
            "Epoch 196/1000\n",
            "2/2 [==============================] - 2s 868ms/step - loss: 0.0076 - mse: 0.0066 - val_loss: 0.1262 - val_mse: 0.0615 - lr: 5.0000e-04\n",
            "Epoch 197/1000\n",
            "2/2 [==============================] - 2s 861ms/step - loss: 0.0075 - mse: 0.0065 - val_loss: 0.1264 - val_mse: 0.0619 - lr: 5.0000e-04\n",
            "Epoch 198/1000\n",
            "2/2 [==============================] - 2s 866ms/step - loss: 0.0075 - mse: 0.0065 - val_loss: 0.1256 - val_mse: 0.0613 - lr: 5.0000e-04\n",
            "Epoch 199/1000\n",
            "2/2 [==============================] - 2s 863ms/step - loss: 0.0074 - mse: 0.0064 - val_loss: 0.1265 - val_mse: 0.0620 - lr: 5.0000e-04\n",
            "Epoch 200/1000\n",
            "2/2 [==============================] - 2s 876ms/step - loss: 0.0073 - mse: 0.0064 - val_loss: 0.1271 - val_mse: 0.0620 - lr: 5.0000e-04\n",
            "Epoch 201/1000\n",
            "2/2 [==============================] - 2s 862ms/step - loss: 0.0073 - mse: 0.0063 - val_loss: 0.1259 - val_mse: 0.0616 - lr: 5.0000e-04\n",
            "Epoch 202/1000\n",
            "2/2 [==============================] - 2s 863ms/step - loss: 0.0072 - mse: 0.0063 - val_loss: 0.1265 - val_mse: 0.0619 - lr: 5.0000e-04\n",
            "Epoch 203/1000\n",
            "2/2 [==============================] - 2s 862ms/step - loss: 0.0071 - mse: 0.0062 - val_loss: 0.1265 - val_mse: 0.0617 - lr: 5.0000e-04\n",
            "Epoch 204/1000\n",
            "2/2 [==============================] - 2s 997ms/step - loss: 0.0071 - mse: 0.0062 - val_loss: 0.1260 - val_mse: 0.0618 - lr: 5.0000e-04\n",
            "Epoch 205/1000\n",
            "2/2 [==============================] - 2s 864ms/step - loss: 0.0071 - mse: 0.0062 - val_loss: 0.1253 - val_mse: 0.0614 - lr: 5.0000e-04\n",
            "Epoch 206/1000\n",
            "2/2 [==============================] - 2s 861ms/step - loss: 0.0070 - mse: 0.0061 - val_loss: 0.1269 - val_mse: 0.0621 - lr: 5.0000e-04\n",
            "Epoch 207/1000\n",
            "2/2 [==============================] - 2s 864ms/step - loss: 0.0069 - mse: 0.0061 - val_loss: 0.1272 - val_mse: 0.0621 - lr: 5.0000e-04\n",
            "Epoch 208/1000\n",
            "2/2 [==============================] - 2s 869ms/step - loss: 0.0069 - mse: 0.0060 - val_loss: 0.1261 - val_mse: 0.0617 - lr: 5.0000e-04\n",
            "Epoch 209/1000\n",
            "2/2 [==============================] - 2s 861ms/step - loss: 0.0068 - mse: 0.0060 - val_loss: 0.1264 - val_mse: 0.0618 - lr: 5.0000e-04\n",
            "Epoch 210/1000\n",
            "2/2 [==============================] - 2s 859ms/step - loss: 0.0068 - mse: 0.0059 - val_loss: 0.1272 - val_mse: 0.0620 - lr: 5.0000e-04\n",
            "Epoch 211/1000\n",
            "2/2 [==============================] - 2s 861ms/step - loss: 0.0067 - mse: 0.0059 - val_loss: 0.1274 - val_mse: 0.0622 - lr: 5.0000e-04\n",
            "Epoch 212/1000\n",
            "2/2 [==============================] - 2s 865ms/step - loss: 0.0067 - mse: 0.0058 - val_loss: 0.1261 - val_mse: 0.0617 - lr: 5.0000e-04\n",
            "Epoch 213/1000\n",
            "2/2 [==============================] - 2s 862ms/step - loss: 0.0066 - mse: 0.0058 - val_loss: 0.1259 - val_mse: 0.0618 - lr: 5.0000e-04\n",
            "Epoch 214/1000\n",
            "2/2 [==============================] - 2s 862ms/step - loss: 0.0066 - mse: 0.0058 - val_loss: 0.1267 - val_mse: 0.0619 - lr: 5.0000e-04\n",
            "Epoch 215/1000\n",
            "2/2 [==============================] - 2s 865ms/step - loss: 0.0065 - mse: 0.0058 - val_loss: 0.1277 - val_mse: 0.0623 - lr: 5.0000e-04\n",
            "Epoch 216/1000\n",
            "2/2 [==============================] - 2s 866ms/step - loss: 0.0065 - mse: 0.0057 - val_loss: 0.1272 - val_mse: 0.0622 - lr: 5.0000e-04\n",
            "Epoch 217/1000\n",
            "2/2 [==============================] - 2s 867ms/step - loss: 0.0064 - mse: 0.0057 - val_loss: 0.1257 - val_mse: 0.0616 - lr: 5.0000e-04\n",
            "Epoch 218/1000\n",
            "2/2 [==============================] - 2s 999ms/step - loss: 0.0064 - mse: 0.0057 - val_loss: 0.1259 - val_mse: 0.0618 - lr: 5.0000e-04\n",
            "Epoch 219/1000\n",
            "2/2 [==============================] - 2s 874ms/step - loss: 0.0064 - mse: 0.0057 - val_loss: 0.1261 - val_mse: 0.0617 - lr: 5.0000e-04\n",
            "Epoch 220/1000\n",
            "2/2 [==============================] - 2s 863ms/step - loss: 0.0063 - mse: 0.0056 - val_loss: 0.1283 - val_mse: 0.0626 - lr: 5.0000e-04\n",
            "Epoch 221/1000\n",
            "2/2 [==============================] - 2s 999ms/step - loss: 0.0063 - mse: 0.0056 - val_loss: 0.1286 - val_mse: 0.0627 - lr: 5.0000e-04\n",
            "Epoch 222/1000\n",
            "2/2 [==============================] - 2s 875ms/step - loss: 0.0063 - mse: 0.0055 - val_loss: 0.1271 - val_mse: 0.0622 - lr: 5.0000e-04\n",
            "Epoch 223/1000\n",
            "2/2 [==============================] - 2s 864ms/step - loss: 0.0062 - mse: 0.0055 - val_loss: 0.1262 - val_mse: 0.0619 - lr: 5.0000e-04\n",
            "Epoch 224/1000\n",
            "2/2 [==============================] - 2s 862ms/step - loss: 0.0062 - mse: 0.0055 - val_loss: 0.1262 - val_mse: 0.0618 - lr: 5.0000e-04\n",
            "Epoch 225/1000\n",
            "2/2 [==============================] - 2s 864ms/step - loss: 0.0062 - mse: 0.0055 - val_loss: 0.1267 - val_mse: 0.0620 - lr: 5.0000e-04\n",
            "Epoch 226/1000\n",
            "2/2 [==============================] - 2s 865ms/step - loss: 0.0061 - mse: 0.0054 - val_loss: 0.1273 - val_mse: 0.0622 - lr: 5.0000e-04\n",
            "Epoch 227/1000\n",
            "2/2 [==============================] - 2s 863ms/step - loss: 0.0060 - mse: 0.0054 - val_loss: 0.1281 - val_mse: 0.0625 - lr: 5.0000e-04\n",
            "Epoch 228/1000\n",
            "2/2 [==============================] - 2s 864ms/step - loss: 0.0060 - mse: 0.0054 - val_loss: 0.1279 - val_mse: 0.0623 - lr: 5.0000e-04\n",
            "Epoch 229/1000\n",
            "2/2 [==============================] - 2s 861ms/step - loss: 0.0060 - mse: 0.0053 - val_loss: 0.1263 - val_mse: 0.0618 - lr: 5.0000e-04\n",
            "Epoch 230/1000\n",
            "2/2 [==============================] - 2s 862ms/step - loss: 0.0060 - mse: 0.0053 - val_loss: 0.1259 - val_mse: 0.0618 - lr: 5.0000e-04\n",
            "Epoch 231/1000\n",
            "2/2 [==============================] - 2s 873ms/step - loss: 0.0062 - mse: 0.0055 - val_loss: 0.1238 - val_mse: 0.0611 - lr: 5.0000e-04\n",
            "Epoch 232/1000\n",
            "2/2 [==============================] - 2s 859ms/step - loss: 0.0066 - mse: 0.0057 - val_loss: 0.1239 - val_mse: 0.0612 - lr: 5.0000e-04\n",
            "Epoch 233/1000\n",
            "2/2 [==============================] - 2s 864ms/step - loss: 0.0068 - mse: 0.0059 - val_loss: 0.1239 - val_mse: 0.0611 - lr: 5.0000e-04\n",
            "Epoch 234/1000\n",
            "2/2 [==============================] - 2s 862ms/step - loss: 0.0067 - mse: 0.0058 - val_loss: 0.1243 - val_mse: 0.0609 - lr: 5.0000e-04\n",
            "Epoch 235/1000\n",
            "2/2 [==============================] - 2s 864ms/step - loss: 0.0061 - mse: 0.0054 - val_loss: 0.1296 - val_mse: 0.0632 - lr: 5.0000e-04\n",
            "Epoch 236/1000\n",
            "2/2 [==============================] - 2s 923ms/step - loss: 0.0064 - mse: 0.0056 - val_loss: 0.1327 - val_mse: 0.0639 - lr: 5.0000e-04\n",
            "Epoch 237/1000\n",
            "2/2 [==============================] - 2s 926ms/step - loss: 0.0067 - mse: 0.0058 - val_loss: 0.1266 - val_mse: 0.0623 - lr: 5.0000e-04\n",
            "Epoch 238/1000\n",
            "2/2 [==============================] - 2s 861ms/step - loss: 0.0065 - mse: 0.0055 - val_loss: 0.1222 - val_mse: 0.0608 - lr: 5.0000e-04\n",
            "Epoch 239/1000\n",
            "2/2 [==============================] - 2s 863ms/step - loss: 0.0079 - mse: 0.0069 - val_loss: 0.1237 - val_mse: 0.0614 - lr: 5.0000e-04\n",
            "Epoch 240/1000\n",
            "2/2 [==============================] - 2s 865ms/step - loss: 0.0068 - mse: 0.0058 - val_loss: 0.1284 - val_mse: 0.0621 - lr: 5.0000e-04\n",
            "Epoch 241/1000\n",
            "2/2 [==============================] - 2s 862ms/step - loss: 0.0068 - mse: 0.0058 - val_loss: 0.1343 - val_mse: 0.0649 - lr: 5.0000e-04\n",
            "Epoch 242/1000\n",
            "2/2 [==============================] - 2s 861ms/step - loss: 0.0074 - mse: 0.0063 - val_loss: 0.1271 - val_mse: 0.0620 - lr: 5.0000e-04\n",
            "Epoch 243/1000\n",
            "2/2 [==============================] - 2s 862ms/step - loss: 0.0062 - mse: 0.0053 - val_loss: 0.1239 - val_mse: 0.0609 - lr: 5.0000e-04\n",
            "Epoch 244/1000\n",
            "2/2 [==============================] - 2s 861ms/step - loss: 0.0064 - mse: 0.0056 - val_loss: 0.1259 - val_mse: 0.0616 - lr: 5.0000e-04\n",
            "Epoch 245/1000\n",
            "2/2 [==============================] - 2s 875ms/step - loss: 0.0060 - mse: 0.0053 - val_loss: 0.1296 - val_mse: 0.0629 - lr: 5.0000e-04\n",
            "Epoch 246/1000\n",
            "2/2 [==============================] - 2s 860ms/step - loss: 0.0062 - mse: 0.0054 - val_loss: 0.1287 - val_mse: 0.0623 - lr: 5.0000e-04\n",
            "Epoch 247/1000\n",
            "2/2 [==============================] - 2s 881ms/step - loss: 0.0058 - mse: 0.0051 - val_loss: 0.1245 - val_mse: 0.0613 - lr: 5.0000e-04\n",
            "Epoch 248/1000\n",
            "2/2 [==============================] - 2s 857ms/step - loss: 0.0061 - mse: 0.0053 - val_loss: 0.1254 - val_mse: 0.0617 - lr: 5.0000e-04\n",
            "Epoch 249/1000\n",
            "2/2 [==============================] - 2s 861ms/step - loss: 0.0059 - mse: 0.0052 - val_loss: 0.1275 - val_mse: 0.0620 - lr: 5.0000e-04\n",
            "Epoch 250/1000\n",
            "2/2 [==============================] - 2s 866ms/step - loss: 0.0058 - mse: 0.0052 - val_loss: 0.1307 - val_mse: 0.0637 - lr: 5.0000e-04\n",
            "Epoch 251/1000\n",
            "2/2 [==============================] - 2s 864ms/step - loss: 0.0059 - mse: 0.0052 - val_loss: 0.1263 - val_mse: 0.0612 - lr: 5.0000e-04\n",
            "Epoch 252/1000\n",
            "2/2 [==============================] - 2s 861ms/step - loss: 0.0057 - mse: 0.0050 - val_loss: 0.1250 - val_mse: 0.0618 - lr: 5.0000e-04\n",
            "Epoch 253/1000\n",
            "2/2 [==============================] - 2s 865ms/step - loss: 0.0062 - mse: 0.0053 - val_loss: 0.1245 - val_mse: 0.0610 - lr: 5.0000e-04\n",
            "Epoch 254/1000\n",
            "2/2 [==============================] - 2s 860ms/step - loss: 0.0057 - mse: 0.0050 - val_loss: 0.1309 - val_mse: 0.0633 - lr: 5.0000e-04\n",
            "Epoch 255/1000\n",
            "2/2 [==============================] - 2s 864ms/step - loss: 0.0062 - mse: 0.0054 - val_loss: 0.1304 - val_mse: 0.0633 - lr: 5.0000e-04\n",
            "Epoch 256/1000\n",
            "2/2 [==============================] - 2s 861ms/step - loss: 0.0058 - mse: 0.0051 - val_loss: 0.1239 - val_mse: 0.0608 - lr: 5.0000e-04\n",
            "Epoch 257/1000\n",
            "2/2 [==============================] - 2s 884ms/step - loss: 0.0068 - mse: 0.0059 - val_loss: 0.1228 - val_mse: 0.0614 - lr: 5.0000e-04\n",
            "Epoch 258/1000\n",
            "2/2 [==============================] - 2s 857ms/step - loss: 0.0070 - mse: 0.0058 - val_loss: 0.1260 - val_mse: 0.0617 - lr: 5.0000e-04\n",
            "Epoch 259/1000\n",
            "2/2 [==============================] - 2s 1s/step - loss: 0.0060 - mse: 0.0052 - val_loss: 0.1339 - val_mse: 0.0645 - lr: 5.0000e-04\n",
            "Epoch 260/1000\n",
            "2/2 [==============================] - 2s 876ms/step - loss: 0.0071 - mse: 0.0060 - val_loss: 0.1290 - val_mse: 0.0629 - lr: 5.0000e-04\n",
            "Epoch 261/1000\n",
            "2/2 [==============================] - 2s 873ms/step - loss: 0.0059 - mse: 0.0051 - val_loss: 0.1235 - val_mse: 0.0605 - lr: 5.0000e-04\n",
            "Epoch 262/1000\n",
            "2/2 [==============================] - 2s 866ms/step - loss: 0.0067 - mse: 0.0058 - val_loss: 0.1240 - val_mse: 0.0615 - lr: 5.0000e-04\n",
            "Epoch 263/1000\n",
            "2/2 [==============================] - 2s 876ms/step - loss: 0.0060 - mse: 0.0051 - val_loss: 0.1279 - val_mse: 0.0619 - lr: 5.0000e-04\n",
            "Epoch 264/1000\n",
            "2/2 [==============================] - 2s 865ms/step - loss: 0.0058 - mse: 0.0050 - val_loss: 0.1290 - val_mse: 0.0627 - lr: 5.0000e-04\n",
            "Epoch 265/1000\n",
            "2/2 [==============================] - 2s 865ms/step - loss: 0.0057 - mse: 0.0050 - val_loss: 0.1254 - val_mse: 0.0614 - lr: 5.0000e-04\n",
            "Epoch 266/1000\n",
            "2/2 [==============================] - 2s 871ms/step - loss: 0.0057 - mse: 0.0050 - val_loss: 0.1248 - val_mse: 0.0611 - lr: 5.0000e-04\n",
            "Epoch 267/1000\n",
            "2/2 [==============================] - 2s 873ms/step - loss: 0.0056 - mse: 0.0049 - val_loss: 0.1279 - val_mse: 0.0626 - lr: 5.0000e-04\n",
            "Epoch 268/1000\n",
            "2/2 [==============================] - 2s 862ms/step - loss: 0.0054 - mse: 0.0048 - val_loss: 0.1269 - val_mse: 0.0612 - lr: 5.0000e-04\n",
            "Epoch 269/1000\n",
            "2/2 [==============================] - 2s 866ms/step - loss: 0.0053 - mse: 0.0047 - val_loss: 0.1276 - val_mse: 0.0628 - lr: 5.0000e-04\n",
            "Epoch 270/1000\n",
            "2/2 [==============================] - 2s 1s/step - loss: 0.0053 - mse: 0.0047 - val_loss: 0.1261 - val_mse: 0.0612 - lr: 5.0000e-04\n",
            "Epoch 271/1000\n",
            "2/2 [==============================] - 2s 878ms/step - loss: 0.0053 - mse: 0.0047 - val_loss: 0.1264 - val_mse: 0.0618 - lr: 5.0000e-04\n",
            "Epoch 272/1000\n",
            "2/2 [==============================] - 2s 868ms/step - loss: 0.0052 - mse: 0.0046 - val_loss: 0.1272 - val_mse: 0.0618 - lr: 5.0000e-04\n",
            "Epoch 273/1000\n",
            "2/2 [==============================] - 2s 871ms/step - loss: 0.0051 - mse: 0.0046 - val_loss: 0.1276 - val_mse: 0.0621 - lr: 5.0000e-04\n",
            "Epoch 274/1000\n",
            "2/2 [==============================] - 2s 1s/step - loss: 0.0051 - mse: 0.0045 - val_loss: 0.1276 - val_mse: 0.0624 - lr: 5.0000e-04\n",
            "Epoch 275/1000\n",
            "2/2 [==============================] - 2s 875ms/step - loss: 0.0050 - mse: 0.0045 - val_loss: 0.1256 - val_mse: 0.0612 - lr: 5.0000e-04\n",
            "Epoch 276/1000\n",
            "2/2 [==============================] - 2s 868ms/step - loss: 0.0050 - mse: 0.0045 - val_loss: 0.1277 - val_mse: 0.0626 - lr: 5.0000e-04\n",
            "Epoch 277/1000\n",
            "2/2 [==============================] - 2s 866ms/step - loss: 0.0050 - mse: 0.0045 - val_loss: 0.1274 - val_mse: 0.0616 - lr: 5.0000e-04\n",
            "Epoch 278/1000\n",
            "2/2 [==============================] - 2s 862ms/step - loss: 0.0051 - mse: 0.0045 - val_loss: 0.1305 - val_mse: 0.0637 - lr: 5.0000e-04\n",
            "Epoch 279/1000\n",
            "2/2 [==============================] - 2s 865ms/step - loss: 0.0052 - mse: 0.0046 - val_loss: 0.1277 - val_mse: 0.0618 - lr: 5.0000e-04\n",
            "Epoch 280/1000\n",
            "2/2 [==============================] - 2s 864ms/step - loss: 0.0049 - mse: 0.0044 - val_loss: 0.1269 - val_mse: 0.0620 - lr: 5.0000e-04\n",
            "Epoch 281/1000\n",
            "2/2 [==============================] - 2s 865ms/step - loss: 0.0050 - mse: 0.0044 - val_loss: 0.1254 - val_mse: 0.0617 - lr: 5.0000e-04\n",
            "Epoch 282/1000\n",
            "2/2 [==============================] - 2s 862ms/step - loss: 0.0051 - mse: 0.0046 - val_loss: 0.1255 - val_mse: 0.0613 - lr: 5.0000e-04\n",
            "Epoch 283/1000\n",
            "2/2 [==============================] - 2s 864ms/step - loss: 0.0050 - mse: 0.0045 - val_loss: 0.1266 - val_mse: 0.0621 - lr: 5.0000e-04\n",
            "Epoch 284/1000\n",
            "2/2 [==============================] - 2s 863ms/step - loss: 0.0048 - mse: 0.0044 - val_loss: 0.1274 - val_mse: 0.0620 - lr: 5.0000e-04\n",
            "Epoch 285/1000\n",
            "2/2 [==============================] - 2s 861ms/step - loss: 0.0048 - mse: 0.0043 - val_loss: 0.1294 - val_mse: 0.0629 - lr: 5.0000e-04\n",
            "Epoch 286/1000\n",
            "2/2 [==============================] - 2s 864ms/step - loss: 0.0050 - mse: 0.0045 - val_loss: 0.1306 - val_mse: 0.0636 - lr: 5.0000e-04\n",
            "Epoch 287/1000\n",
            "2/2 [==============================] - 2s 865ms/step - loss: 0.0049 - mse: 0.0044 - val_loss: 0.1268 - val_mse: 0.0614 - lr: 5.0000e-04\n",
            "Epoch 288/1000\n",
            "2/2 [==============================] - 2s 864ms/step - loss: 0.0047 - mse: 0.0043 - val_loss: 0.1272 - val_mse: 0.0623 - lr: 5.0000e-04\n",
            "Epoch 289/1000\n",
            "2/2 [==============================] - 2s 865ms/step - loss: 0.0048 - mse: 0.0043 - val_loss: 0.1257 - val_mse: 0.0615 - lr: 5.0000e-04\n",
            "Epoch 290/1000\n",
            "2/2 [==============================] - 2s 864ms/step - loss: 0.0049 - mse: 0.0044 - val_loss: 0.1257 - val_mse: 0.0618 - lr: 5.0000e-04\n",
            "Epoch 291/1000\n",
            "2/2 [==============================] - 2s 886ms/step - loss: 0.0050 - mse: 0.0044 - val_loss: 0.1252 - val_mse: 0.0615 - lr: 5.0000e-04\n",
            "Epoch 292/1000\n",
            "2/2 [==============================] - 2s 869ms/step - loss: 0.0050 - mse: 0.0045 - val_loss: 0.1258 - val_mse: 0.0614 - lr: 5.0000e-04\n",
            "Epoch 293/1000\n",
            "2/2 [==============================] - 2s 864ms/step - loss: 0.0048 - mse: 0.0043 - val_loss: 0.1280 - val_mse: 0.0623 - lr: 5.0000e-04\n",
            "Epoch 294/1000\n",
            "2/2 [==============================] - 2s 869ms/step - loss: 0.0047 - mse: 0.0042 - val_loss: 0.1283 - val_mse: 0.0625 - lr: 5.0000e-04\n",
            "Epoch 295/1000\n",
            "2/2 [==============================] - 2s 1s/step - loss: 0.0047 - mse: 0.0042 - val_loss: 0.1280 - val_mse: 0.0623 - lr: 5.0000e-04\n",
            "Epoch 296/1000\n",
            "2/2 [==============================] - 2s 875ms/step - loss: 0.0048 - mse: 0.0043 - val_loss: 0.1300 - val_mse: 0.0632 - lr: 5.0000e-04\n",
            "Epoch 297/1000\n",
            "2/2 [==============================] - 2s 865ms/step - loss: 0.0049 - mse: 0.0044 - val_loss: 0.1302 - val_mse: 0.0629 - lr: 5.0000e-04\n",
            "Epoch 298/1000\n",
            "2/2 [==============================] - 2s 861ms/step - loss: 0.0052 - mse: 0.0046 - val_loss: 0.1324 - val_mse: 0.0643 - lr: 5.0000e-04\n",
            "Epoch 299/1000\n",
            "2/2 [==============================] - 2s 894ms/step - loss: 0.0055 - mse: 0.0048 - val_loss: 0.1302 - val_mse: 0.0624 - lr: 5.0000e-04\n"
          ]
        }
      ],
      "source": [
        "# Learning Rate Scheduler\n",
        "def scheduler(epoch, lr):\n",
        "  if epoch == 500:\n",
        "    return lr /5\n",
        "  else:\n",
        "    return lr\n",
        "\n",
        "scheduler_cb = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
        "\n",
        "# Early stopping criteria\n",
        "early = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\",min_delta=0.0005, patience=200, verbose=0, mode=\"auto\", baseline=None, restore_best_weights=True)\n",
        "\n",
        "# Training (Fit)\n",
        "history = model.fit(train_data, train_labels, epochs=1000, validation_data=(validation_data, validation_labels), callbacks=[early, scheduler_cb])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This next plot shows loss of the trianing and validation sets."
      ],
      "metadata": {
        "id": "xIGiYmh3fwP2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "UgCp0oNUAfzt",
        "outputId": "f32fa78e-0869-4178-c3c8-56d4a7ef73e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['loss', 'mse', 'val_loss', 'val_mse', 'lr'])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deZwdZZ3v8c/vLL13p5Puzp6QBAIkQAghLAoqijiACiKyjTjqOKKOiuvcwasDjuOdcca5s6goonJdRonIIlFBFAVRWROWkBCWEBLSna3T6aT35fT53T+e6u6TTnfoDjk56dT3/Uq/ck5VnapfnTpVv3qep+opc3dERCS+EoUOQERECkuJQEQk5pQIRERiTolARCTmlAhERGJOiUBEJOaUCERGycy+b2ZfHuW0G8zsza92PiIHgxKBiEjMKRGIiMScEoEcVqIqmb8zs1Vm1m5m3zOzKWZ2t5m1mtm9ZjYxZ/oLzGyNme0ys/vNbEHOuJPM7PHocz8FSoYs621m9mT02QfNbNF+xvxBM1tnZjvNbLmZTY+Gm5n9p5ltN7MWM3vazI6Pxp1vZs9EsTWY2Wf36wsTQYlADk8XA+cARwNvB+4G/jdQR/jNXw1gZkcDNwOfjMbdBfzCzIrMrAj4OfAjYBLws2i+RJ89CbgJ+BBQA3wbWG5mxWMJ1MzeBPwLcCkwDdgILItGvwV4fbQeE6JpmqJx3wM+5O6VwPHA78eyXJFcSgRyOPq6u29z9wbgj8Aj7v6Eu3cBdwAnRdNdBvzK3X/r7r3AvwOlwGuB04E08F/u3uvutwKP5SzjKuDb7v6Iu/e5+w+A7uhzY/Fu4CZ3f9zdu4HPAa8xszlAL1AJHAuYu6919y3R53qBhWZW5e7N7v74GJcrMkCJQA5H23Jedw7zviJ6PZ1wBg6Au2eBTcCMaFyD79kr48ac10cAn4mqhXaZ2S5gVvS5sRgaQxvhrH+Gu/8e+AZwPbDdzG40s6po0ouB84GNZvYHM3vNGJcrMkCJQOJsM+GADoQ6ecLBvAHYAsyIhvWbnfN6E/B/3L0656/M3W9+lTGUE6qaGgDc/WvufjKwkFBF9HfR8Mfc/UJgMqEK65YxLldkgBKBxNktwFvN7GwzSwOfIVTvPAg8BGSAq80sbWbvBE7N+ex3gA+b2WlRo265mb3VzCrHGMPNwPvNbHHUvvDPhKqsDWZ2SjT/NNAOdAHZqA3j3WY2IarSagGyr+J7kJhTIpDYcvfngCuBrwM7CA3Lb3f3HnfvAd4JvA/YSWhPuD3nsyuADxKqbpqBddG0Y43hXuAfgNsIpZAjgcuj0VWEhNNMqD5qAr4ajXsPsMHMWoAPE9oaRPaL6cE0IiLxphKBiEjMKRGIiMScEoGISMwpEYiIxFyq0AGMVW1trc+ZM6fQYYiIjCsrV67c4e51w40bd4lgzpw5rFixotBhiIiMK2a2caRxqhoSEYk5JQIRkZhTIhARiblx10YgIjJWvb291NfX09XVVehQ8q6kpISZM2eSTqdH/RklAhE57NXX11NZWcmcOXPYs0PZw4u709TURH19PXPnzh315/JWNWRmN0WP2Fv9CtOdYmYZM3tXvmIRkXjr6uqipqbmsE4CAGZGTU3NmEs++Wwj+D5w7r4mMLMk8K/Ab/IYh4jIYZ8E+u3PeuYtEbj7A4Tue/fl44Tud7fnK45+z21t5f/+5jl2tHXne1EiIuNKwa4aMrMZwEXAtw7G8l5sbOPrv19HU1vPwViciMiAXbt28c1vfnPMnzv//PPZtWtXHiLaUyEvH/0v4O+j58Tuk5ldZWYrzGxFY2Pjfi0smQjFpUxWD3ISkYNrpESQyWT2+bm77rqL6urqfIU1oJBXDS0FlkX1WbXA+WaWcfefD53Q3W8EbgRYunTpfj1JJxUlgr6sHsQjIgfXNddcw4svvsjixYtJp9OUlJQwceJEnn32WZ5//nne8Y53sGnTJrq6uvjEJz7BVVddBQx2qdPW1sZ5553HmWeeyYMPPsiMGTO48847KS0tPSDxFSwRuPvAtU1m9n3gl8MlgQNlsESgRCASZ//4izU8s7nlgM5z4fQqrnv7cSOO/8pXvsLq1at58sknuf/++3nrW9/K6tWrBy7xvOmmm5g0aRKdnZ2ccsopXHzxxdTU1OwxjxdeeIGbb76Z73znO1x66aXcdtttXHnllQck/rwlAjO7GTgLqDWzeuA6IA3g7jfka7kjSSVCLZhKBCJSaKeeeuoe1/l/7Wtf44477gBg06ZNvPDCC3slgrlz57J48WIATj75ZDZs2HDA4slbInD3K8Yw7fvyFUe//hJBb5/aCETibF9n7gdLeXn5wOv777+fe++9l4ceeoiysjLOOuusYe8DKC4uHnidTCbp7Ow8YPHEpq+hVFJtBCJSGJWVlbS2tg47bvfu3UycOJGysjKeffZZHn744YMcXYy6mEipjUBECqSmpoYzzjiD448/ntLSUqZMmTIw7txzz+WGG25gwYIFHHPMMZx++ukHPb4YJYKojaBPiUBEDr6f/OQnww4vLi7m7rvvHnZcfztAbW0tq1cP9tbz2c9+9oDGFpuqIV01JCIyvNgkArURiIgMLzaJQHcWi4gMLzaJYKCxWG0EIiJ7iE0iSKqLCRGRYcUmEaSTYVXVWCwisqfYJILBEoHaCETk0FZRUQHA5s2bede7hn9441lnncWKFSsOyPJikwh0Q5mIjDfTp0/n1ltvzftyYpMI1EYgIoVyzTXXcP311w+8/+IXv8iXv/xlzj77bJYsWcIJJ5zAnXfeudfnNmzYwPHHHw9AZ2cnl19+OQsWLOCiiy46oH0Nxe7OYpUIRGLu7mtg69MHdp5TT4DzvjLi6Msuu4xPfvKTfPSjHwXglltu4Z577uHqq6+mqqqKHTt2cPrpp3PBBReM+Mzhb33rW5SVlbF27VpWrVrFkiVLDlj4sUkEA/cRqPdRETnITjrpJLZv387mzZtpbGxk4sSJTJ06lU996lM88MADJBIJGhoa2LZtG1OnTh12Hg888ABXX301AIsWLWLRokUHLL7YJAK1EYgIsM8z93y65JJLuPXWW9m6dSuXXXYZP/7xj2lsbGTlypWk02nmzJkzbPfTB0Ns2ggSCSNhaiMQkcK47LLLWLZsGbfeeiuXXHIJu3fvZvLkyaTTae677z42bty4z8+//vWvH+i4bvXq1axateqAxRabEgGEdgKVCESkEI477jhaW1uZMWMG06ZN493vfjdvf/vbOeGEE1i6dCnHHnvsPj//kY98hPe///0sWLCABQsWcPLJJx+w2GKVCJIJU4lARArm6acHG6lra2t56KGHhp2ura0NCA+v7+9+urS0lGXLluUlrthUDUFoJ1BfQyIie4pVIkgmTb2PiogMkbdEYGY3mdl2M1s9wvh3m9kqM3vazB40sxPzFUu/VMLURiASU+7x2Pf3Zz3zWSL4PnDuPsa/BLzB3U8A/gm4MY+xAFEbgaqGRGKnpKSEpqamwz4ZuDtNTU2UlJSM6XN5ayx29wfMbM4+xj+Y8/ZhYGa+Yumnq4ZE4mnmzJnU19fT2NhY6FDyrqSkhJkzx3Y4PVSuGvoAMPzTmwEzuwq4CmD27Nn7vZBU0tT7qEgMpdNp5s6dW+gwDlkFbyw2szcSEsHfjzSNu9/o7kvdfWldXd1+LyupNgIRkb0UtERgZouA7wLnuXtTvpeX0n0EIiJ7KViJwMxmA7cD73H35w/GMpOJBL1qLBYR2UPeSgRmdjNwFlBrZvXAdUAawN1vAK4FaoBvRt2uZtx9ab7igf4SgdoIRERy5fOqoSteYfzfAH+Tr+UPJ5VUG4GIyFAFbyw+mNRGICKyt1glAl01JCKyt1glglQioRKBiMgQsUoEKhGIiOwtVokgdEOtq4ZERHLFKhHowTQiInuLVSJIJ9XpnIjIULFKBCoRiIjsLVaJIDyYRm0EIiK5YpUI9GAaEZG9xSoRqIsJEZG9xSoR6D4CEZG9xSoRpBIJ3UcgIjJEfBJBWyNHta2kKNtZ6EhERA4p8UkEG/7Ilc9/nMnZw//h1SIiYxGfRJAIj14wzxQ4EBGRQ0vsEgHZPtzVYCwi0i9GiSAJQIo+3V0sIpIjdokgSVaXkIqI5MhbIjCzm8xsu5mtHmG8mdnXzGydma0ysyX5igUYqBpKklWJQEQkRz5LBN8Hzt3H+POA+dHfVcC38hjLQCJIWZ9KBCIiOfKWCNz9AWDnPia5EPihBw8D1WY2LV/xYINVQyoRiIgMKmQbwQxgU877+mjYXszsKjNbYWYrGhv38z6AnKoh9UAqIjJoXDQWu/uN7r7U3ZfW1dXt30wGGot11ZCISK5CJoIGYFbO+5nRsPzobyOgj4y6ohYRGVDIRLAc+Kvo6qHTgd3uviVvS9PloyIiw0rla8ZmdjNwFlBrZvXAdUAawN1vAO4CzgfWAR3A+/MVCzDk8lG1EYiI9MtbInD3K15hvAMfzdfy9zKQCHT5qIhIrnHRWHxADHQxkVUbgYhIjvgkgv77CExXDYmI5IpPItjjPgIlAhGRfvFMBHpcpYjIgBglAnVDLSIynNglAlUNiYjsKUaJYPDyUZUIREQGxS4RpFQiEBHZQ+wSQSgRqLFYRKRffBKBhVVNWpZe3VAmIjIgRonAcEvqwTQiIkPEJxEAJFKhG2olAhGRAbFKBJ5IqvdREZEhYpUIiKqGVCIQERkUr0SQSOk+AhGRIWKXCPSoShGRPcUsEfRXDamNQESkX6wSgSXURiAiMlSsEgGJVHgwjaqGREQG5DURmNm5Zvacma0zs2uGGT/bzO4zsyfMbJWZnZ/PeEIbgUoEIiK58pYIzCwJXA+cBywErjCzhUMm+wJwi7ufBFwOfDNf8QBY1Fisq4ZERAbls0RwKrDO3de7ew+wDLhwyDQOVEWvJwCb8xgPJJKkzFUiEBHJkc9EMAPYlPO+PhqW64vAlWZWD9wFfHy4GZnZVWa2wsxWNDY27n9EiSRp053FIiK5Ct1YfAXwfXefCZwP/MjM9orJ3W9096XuvrSurm7/l5ZIkbI+9T4qIpIjn4mgAZiV835mNCzXB4BbANz9IaAEqM1bRImUeh8VERkin4ngMWC+mc01syJCY/DyIdO8DJwNYGYLCIngVdT9vAJLkjJdNSQikitvicDdM8DHgHuAtYSrg9aY2ZfM7IJoss8AHzSzp4Cbgfe5e/6O0okUafU+KiKyh1Q+Z+7udxEagXOHXZvz+hngjHzGsIdEkpTpeQQiIrkK3Vh8cCVSJHG1EYiI5IhZIkiS1BPKRET2MKpEYGafMLMqC75nZo+b2VvyHdwB19/FRJ/aCERE+o22RPDX7t4CvAWYCLwH+EreosqXRFJdTIiIDDHaRGDR/+cDP3L3NTnDxo9EioQuHxUR2cNoE8FKM/sNIRHcY2aVwPirXzGVCEREhhrt5aMfABYD6929w8wmAe/PX1h5Et1ZrEdViogMGm2J4DXAc+6+y8yuJHQfvTt/YeWJHl4vIrKX0SaCbwEdZnYi4W7gF4Ef5i2qfEkkSZClV3cWi4gMGG0iyERdP1wIfMPdrwcq8xdWniSSJF0lAhGRXKNtI2g1s88RLht9XdRVdDp/YeVJVDWkNgIRkUGjLRFcBnQT7ifYSuhS+qt5iypfEikS6oZaRGQPo0oE0cH/x8AEM3sb0OXu47KNIOl9ZNRGICIyYLRdTFwKPApcAlwKPGJm78pnYHlhSRK6akhEZA+jbSP4PHCKu28HMLM64F7g1nwFlheJFAnXncUiIrlG20aQ6E8CkaYxfPbQ0d9YnFHVkIhIv9GWCH5tZvcQniIGofH4rn1Mf2hKhNXNZvsKHIiIyKFjVInA3f/OzC5m8GliN7r7HfkLK08SUSEmmylsHCIih5BRP6rS3W8DbstjLPkXlQhwJQIRkX77TARm1goM17JqgLt7VV6iypcoEbiqhkREBuyzwdfdK929api/ytEkATM718yeM7N1ZnbNCNNcambPmNkaM/vJ/q7IqPSXCJQIREQGjLpqaKzMLAlcD5wD1AOPmdlyd38mZ5r5wOeAM9y92cwm5yuesMCQ90xtBCIiA/J5CeipwDp3X+/uPcAyQqd1uT4IXO/uzQBDLlE98HJKBKEPPRERyWcimAFsynlfHw3LdTRwtJn92cweNrNzh5uRmV1lZivMbEVjY+P+RxQlgiS6qUxEpF+hbwpLAfOBs4ArgO+YWfXQidz9Rndf6u5L6+rq9n9p/YnA1M2EiEi/fCaCBmBWzvuZ0bBc9cByd+9195eA5wmJIT8SSQBS9KlEICISyWcieAyYb2ZzzawIuBxYPmSanxNKA5hZLaGqaH3eIooSQZIsfXomgYgIkMdE4O4Z4GPAPcBa4BZ3X2NmXzKzC6LJ7gGazOwZ4D7g79y9KV8x7dlGoP6GREQgj5ePArj7XQzpk8jdr8157cCno7/8S4SHqqXJqGpIRCRS6Mbig6uoDIAyupUIREQi8UoE6XIAyqxbbQQiIpF4JYKiKBHQpTYCEZFILBNBuXXpPgIRkUjMEkEF0F8iUCIQEYHYJYLBxmKVCEREgnglglQJbgnKrIud7T2FjkZE5JAQr0RgBkXllNPN4y83FzoaEZFDQrwSAWDpcqaX9bFyoxKBiAjEMBFQVM6Msj6eeHmX2glERIhpIqgrydLWneH5ba2FjkZEpOBimAgqKKcLgMbW7gIHIyJSeDFMBGWk+zoAaOvWs4tFRGKYCMpJRYmgtau3wMGIiBReDBNBBclMJwCtXSoRiIjELxGky7DedkBVQyIiEMdEUFSO9bRTXpRUiUBEhFgmggro66G62GlTIhARiWMiCF1R15Vkae1WY7GISF4TgZmda2bPmdk6M7tmH9NdbGZuZkvzGQ8w0ANpTTqjqiEREfKYCMwsCVwPnAcsBK4ws4XDTFcJfAJ4JF+x7CF6JkFNUY8SgYgI+S0RnAqsc/f17t4DLAMuHGa6fwL+FaLbffMtqhqqSffoqiEREfKbCGYAm3Le10fDBpjZEmCWu/9qXzMys6vMbIWZrWhsbHx1URVXAjAx2U1rVy/tSgYiEnMFayw2swTwH8BnXmlad7/R3Ze6+9K6urpXt+DiKgAmJrvY1tLNcdfdw4Prdry6eYqIjGP5TAQNwKyc9zOjYf0qgeOB+81sA3A6sDzvDcYlIRFUWcfAoCfrd+V1kSIih7J8JoLHgPlmNtfMioDLgeX9I919t7vXuvscd58DPAxc4O4r8hjTQImgyjoHBhUl43cVrYhIv7wdAd09A3wMuAdYC9zi7mvM7EtmdkG+lvuKokRQyWCJYEebnl8sIvGVyufM3f0u4K4hw64dYdqz8hnLgGQK0uWUZNsHBu1o03MJRCS+4lknUjIB624ZeNukRCAiMZbXEsEhq6SKOeUZPvSGeTy+sVlVQyISa/EsERRXkext5XPnLWD2pHKVCEQk1uKZCEqqoGs3ALUVRexo78HdCxyUiEhhxDMRFFdBZzM8tYy68hQ9mSytusNYRGIqnomgpAqaN8AdH+LYzicAaFI7gYjEVDwTQXQvAUAdzYAuIRWR+IpnIigZTARTki0kE8bNj75cwIBERAonpomgeuBldV8zH33jUdz+eAOPrG8qYFAiIoURz0SQLBp83baND79hHqmE8cALr7KLaxGRcSieiaCzefB12zbKilIcN2MCj73UPPJnREQOU/G8s3jJX8H2tdC6BVq3AnDqnIn84KGNdGf6KE4lCxygiMjBE88SQdkkeOe3ofZoaN8OwNI5k+jJZFlVv7vAwYmIHFzxTAT9KqaEaqJMNyfODA3Iz25peYUPiYgcXmKeCKLHXrY3MqWqmJJ0gg1NHfv+jIjIYSbmiWBK+H/nS5gZc2rK2bCjfd+fERE5zMQ7EUw9AdJl8JNLYccLzKkp56UmJQIRiZd4J4IJM+EDv4XeDtjwR46oLWPTzg76suqJVETiI96JAGDyQkiVQtOLzK0pp7fP2byr85U/JyJymMhrIjCzc83sOTNbZ2bXDDP+02b2jJmtMrPfmdkR+YxnWIkE1BwJTeuYU1sOwEtqJxCRGMlbIjCzJHA9cB6wELjCzBYOmewJYKm7LwJuBf4tX/HsU5QIjp3o/LT4n1jxyB8LEoaISCHks0RwKrDO3de7ew+wDLgwdwJ3v8/d+6/XfBiYmcd4RlZzFDRvoHrLnznN1tL93L0qFYhIbOQzEcwANuW8r4+GjeQDwN3DjTCzq8xshZmtaGzMQ8dwNUdBNgOrbgFgJtu46+ktB345IiKHoEOisdjMrgSWAl8dbry73+juS919aV1d3YEPoOao8P+zvwTg2OIdrNyoDuhEJB7ymQgagFk572dGw/ZgZm8GPg9c4O6FeUzY9CVQd+zA27nJHazcsJPsn74Gu+vh+XugZXNBQhMRybd8JoLHgPlmNtfMioDLgeW5E5jZScC3CUlgex5j2bdkCi74Rng9YymTercytXs9iXv/AZZfHW44++450LyxYCGKiORL3hKBu2eAjwH3AGuBW9x9jZl9ycwuiCb7KlAB/MzMnjSz5SPMLv9mnQL/0ARL/oqEZzg38VgY/uLvwv+dO+EPhbmoSUQkn/L6PAJ3vwu4a8iwa3Nevzmfyx+zZAomzgHgHemHB4fPfg1Mmgdrl0PXLlh4ISy6tDAxiogcYIdEY/EhpfZoAObSwAuJuVA6CU56Tzjwd7eEBuXlH4ftzxY4UBGRA0OJYKiqabD43QA82TOLpo+sgcV/CXNeBwvfAef8U+io7pefhO42+PXnYPXt4A6PfQ/W3AFtjSFRZHoKvDJyyLr9Q/DUskJHIQLE9VGVr+QtX2b39pf5n5feTNWmFv7iuDKwJFz6gzC+pAp+8Qn470XQ0QTJYnj8h7D+vjDeEuBZqD0GLvl/UD0biisLtz6FkOmBbathxpJCR3Lo2V0Pq5ZB80tw4uWFjmZ869gZnjj4amW6IZGCRDwfU6tEMJyySZT89Z1s+D+/484nG/iL46buOf6k98CL90FfT6gyuufzsPmJUFrwLHTtDgf/33wBvvVawEKX1+5h3OzTwnzmnQVFFdDeCMVVkEyHH+Kc10G6NDxTuWIKND4H0xaHfpEA+jLQsBLqjoZHvxuewVw5Zf/Xt683JLpETgExm4U7PhRiesc3oad9cGfpbIaJw3QLlemG5g1Qdww88FV44N/g/b8ODe1Hnxum2fwEzDgZzPY/3kwPPPR1mP1aOOI10PQiNDweXk+YGca//GD47iYvCCW0pnVQOx/Ka8P3t/kJyHTCEWeE7zzTDWt+Hh5duvQDUFQ2uLyu3bDpsXDgrp0Pm5+ETBdk+yBZBEveE77Dyqnh+xrKHR65IfRye+IVsP4PYXjDyvDM7Pv/BWrmw1Fvhr5umLoI2neE31K6FLpbYcKMcNDrbhloxxo19zCvRBJ6OyFVEr7/jp1QVA6p4rC9M52htJvpgrZtUD45rE9vNDyZCp/pP/gm0+G76dodqlArp4bXu16GbC9UHxG+b3fY/gw0PhtOmuqODSXvTHdYt9atYb1KqmHKwrCs3N9H61a475/hpT/A+f8O888J2/D3X4I//zec+Wk4+9rwu1z107B+Cy+E0olhO6++DWadCgsuCPvs/V+BloaQhI98E7Rsgf93btiWF38Ppi2C3Q2w8UGomAxzXw9t20MCLyoDLOzfPe0h7p628NnKaWFf3fUybHkyxDlpHlREv4tdG2HbmvDX0gDHXwyV08O8UkWDv+2GldDbHn5TDY+H40D9ivAbWPrXcOxbx7b9R8Hcx1eXy0uXLvUVK1YclGX9y91r+c4D63njMZOprSjmU+cczdQJJXtP2N0WDpDpIeN2rINND4d7EJ6/J9y9XFwZDlzZ3pAARpJIh2lSpWEHnTALSqvDj6ptG2xdBWU1oURSPRvK62DOmaFKavpi2PIUzDoNJs2F9feHneLpW8OOcPL7wrBZp8Lcs+DOj4YD5ju/A0/+T/hRN2+ER78dYnnLl+HBb4Sds2JyWMbb/yvsbJseDT/q4y4KiWPtL+Gib8OvrwkJIFUSDizHnB8SxJ/+E064NMR92ofDQbB1C7z+s2G+894QdvwHvwbHvg2mnwQ718NTN8PMU+H0j4TquE0Ph+/ijZ+Du/5XOIBWTofzvgK/vS4ctEsmwJL3wmPfDQfhVGnY+esfC6UVCJ9Z/JdQ/yi89EA0bFpoK5p2Irz4+7Djso/9JJEK2zZVGrZvT3tIGCdcAiddCS/8Fm7/mzDthNlQXBEOjCOZ87qwzj1t4cDZvj0kvZ0vhu+t9uhwUJowK0zjHkqpmS7o7Qq/G8+Gg2xxJXS1hPgqp0FLfTj5KK6C1ujemHR5+I15Niyvb7jbeSx8n127Ro572O8m+h2PRfEEmLk0lKz7emDr02H7VUyB3Ztg0WWwdTVsexomHwfb18CkI8N+lol6Di6uCgmuvTHEjof1LK6Etq1h3+ncFQ7GG/4UDuhFFeE3O++ssH/0RVW7ldPDNshmRr8O/bUC/fr3g37psrBOEE7CklEiyPbuuRxLhn136gkhkSx5L5z5yTF9nQOzMlvp7kuHHadEMLKGXZ286d/vp7osTXNHL+cfP5X/uvykAzPzbF9ICD1t4Sy2u3XwDGnjn8KZVfnkcBY17cTww8xmwkGzpwNmnhKqF069KpxhJlJhx+hPDqWTwo8awg7Q2x7ONMvrwiWx/QcvgKqZ4QAx1ElXwjPLw05SezTsfCn8UCfNCweqXP0HkNzlTl8Cmx+HI88O1WaejQ4muwc/V1IN1bPCzp6rZj40vTD4fsbJIbn1H3DPuDq6nNdh1ulwxifgZ+8NO2/1bHjDNaFE1rkzJKlFl8PqW+HZX4UYzr4uJO4nfhy+D3e44OtQNR1Wfj8cqJvWwRFnhjPC2aeF+W5/NiTY0omhBNX4fDjbnzQvHIh62sJZfMPKkHBSJWFbTzsRzvs3+J+Lwvofcz48F11Qd+XtsOmRsNMXlcFvrw3fZ9X08H2f+JfhrDZdErpN37kephwXzlL7qxy7W8OBL10Wti1ESaktrC8WDiR1C8KZc8eOMI++TDi4p0vDZzubw/QVk8MJB4R16G4Nv6uqGeGvffvg9ubyacgAABCySURBVCyuCiWYjh2hhFF9RIilaV0YnioJZ7MzlobSReOz4fOpknDwrZwW5tO+PYzbuT5s60QqHCDLauHN14Xl3nsdrPxB6Cjyjf8bjn17OHlZfVv4jZ78vvAbeeib4fOzT4Nj3gov/CacPO2uD7+H+W+BX34qJP/a+eH3MGke/PrvQ5KZdUrYtxpWwssPh5LNrNPDwbyvN+yHxZXhr6gifG8tDeHEpbgyHLhffiQk25YtYR+qOQqmHB9OuszgubvD77VpXZgnhFLbjKXh+8dg8rGD29g9KoXuX0WOEsGr0NjaTXVZmi8uX8Ntj9dz7duO44QZEzhh5oSDFsOI+s/4+rVsDmfJzS+Fs8Wd68MZ0uTjBg8IlgjtGdMXhx95sjjsGOvvg+1rQ1EZwg9y2omhFLFzPbz2anjuV9C0PpyRvPBbaFwLE+eGH+1zd4cd74xPwDM/D/M95rxwye3iK8PB9tHvwNv+IxxM+3rgzr+Fd9wAc86AP38Njn9n2FErp4aktXN9+OFXTA6loeYNoUpu9mvCDrL69rDMY84PO8dLfww75PxzwoFtx7qw3tMXD35H2egsLbcabHdDOAhNz0ny7uH7Lana/+2zZRU8+ZNQLXDah0LCb98RSodzzoSXHwoHt7mv2/Nz6+8PB8CZp4SDbap4/2M4HGWze24/GRUlggPgiZebueibDwIhmX/hrQv5wJlzD3och5VsX2wb50QOtn0lAjUWj9LiWdX87VlHMre2nN88s41/vmstD73YxMyJpbzvtXMGHmojY6AkIHJIUIlgP7R09XLxNx+ks7eP7a3dpBLGW0+YxjkLp/CWoVcYiYgcAlQ1lEdbd3fxhZ8/zcqNzTR39HLOwikcWVfBETVlXHTSDErSOusVkcJTIjgIejJZvv77F7j50U3s6ughk3XKipLMn1zBpPIiTpgxgYXTq6goTnPavEmkk2rsEpGDR4ngIOr/Ph95aSe/WrWFjTs7aGzt5rmtLWSjrzqVMKZVlzCjupQZ1WVMm1DCpPIiJpUXUVNRxKyJZUyvLqUopWQhIgeGGosPIovuiDx9Xg2nz6sZGN7Y2s22li427+rkiU272Lyrk4bmTh58cQfbWroGkkS/hMG0CaXMnlTG1AklJBPGvLpyjphUTkdPhjm15UwsK2JubTnNHT3UlBcNLFtEZCxUIjgEZLNOS1cvO9t72N7azaadHWza2cHL0d+2lm4y2SzbWva+4zOVMDJZZ+bEUo6sq6CjJ8OxU6s4anIFU6pKKC9OMm1CCbMnlauEIRJjKhEc4hIJo7qsiOqyIubVVexRksi1u7OX+uYOStNJXt7ZQVNbD89ubaGmopgnX95F/a4OSlJJbnu8no6evj0+m0wYU6tKaGrvpqI4zdzaUP1UV1FMbWUxdRXFpFMJejJZplaVsOSIakrTSba2dGEYU6qK96vE0dXbx5rNu1kwrYqyIv3cRA5F2jPHkQmlaSaUhjua59VVjDhdNus0tfewraWLjp4+6ps7WN/YTsOuTiaUpuns6WP9jjYef7mZxtZuunqze80jlTDSyQSdvSGhVJelOXZqJcWpJF29fSydM5Hjp0/AgTcdO5kdbd08v62VU+fWUN/cwTObW5hUXsSXfvEM63e0U5pO8ulzjubtJ04fvr8mESkYVQ3FnLvT3tNHY2s3mb4s6WSCjTs7ePSlJrp6s8ypLcfdWbullbVbWujJZClKJXi6YTd9UcNGUTJBn/vA+1yzJpXy8TfN5+6nt3Dfc6GTvSlVxVSWpFkwrYoTZ05g5sRSFkyroraimPJinZuI5IOuGpIDbntrF1t2ddHS1cuf1u0gnUhw8hETWbN5N5UlaY6ZWslLO9q5cPF0yopSuDurG1p4dMNO1mzeTVtXhtUNu9m8u2uP+U6uLGZKVQnVZelQXVaaDsMmlDC1qoRU0sj0OdVlaaZXl9LenaGqJE3CjKKoaiuVtHGRUNyde9Zso6u3jzceO5kJpcN0YS1ygBQsEZjZucB/A0ngu+7+lSHji4EfAicDTcBl7r5hX/NUIji8NLV107Crk2c2t7Czo4f1je00tXXT3NHLro4emjt62d05um6MkwmjL+ukk8aRdRVUl6VJJxOUF6WYWF5EwmDGxFLK0klKi5IkEwnSSaOqNE1Hdx8Ty9JUlKQoSiWYUV1KSTpJKmGv6mqsF7a18pmfPcW5x0/lI284kpbODB9f9gRzaspo7ujlF0+FrqBTCaOiJMWk8iIWzZjAMVNDaammopiyoiRVpWklCnlVCpIIzCwJPA+cA9QDjwFXuPszOdP8LbDI3T9sZpcDF7n7ZfuarxJB/HT19rGtpYttLd30ZZ1U0tjZ3sPmXZ2UFSVp7uglYdDalaEknaS1K8P6xjaaoxv7mtt7aO3K0NuXpaVrDH3KEy7jTScTpBJGImGkEkYyERJDe3cfRakEZUVJStNJStLJ8LoovO7oybBiQzPu0NMX2mHKi5J0Z7L0uZNOJvjoWUfxuqNr+e0z22jt6qWhuZPnt7XRsKtzr1hqK4oHEln/ckoHXqcoK0pSkk6QTOwdb///g6/3nMYMEmYkrP91uBQ6DAMjZ5pEeD84DTmf23tee7wnmibBsPM2g6x76HE55/+shxJU1qGjJ8PLTR1MriqmLxtu5pxeXUJJOjnw1Ij+5SQSkEokBmLtyWRJJYzebJaEGbs7w4nGhNI0vX1ZUtHJQW+fU1Waojh1+PQMUKirhk4F1rn7+iiIZcCFQO4TOS4Evhi9vhX4hpmZj7f6KsmrknSSI2rKOaLm1XXs5+509Wbp6u2jo7ePTF+Wrt4srV29VJSkaG7vpbWrl+5Mli27O+nuzdLTlw0H7mxoA8l6///hoN7bl6Wzt4+Onj46e/ro7O2jtStDY2s3RakEl58yiw++fh5/XreD+uZOGnZ1csGJ0zmyroLSoiS1FaGL6SWzJ+4R6/aWLl5sbGdnew/tPRma23tYt72NrS1ddPX2sbWld2B5/cvuTzYyvP4So1noZXw0BpNYSCQDCSYaju353nISaX/y65/PSPPvl5soCf/IZJ2eTHbg7/1nzuXT5xz9ar6GYeUzEcwANuW8rwdOG2kad8+Y2W6gBtiRx7gkpswsnEUXJZn4ypMfUJedMntM00+uKmFy1diursr0ZenKSVqZ7ODr8N6HeZ8l0+c4w5+JD3dGPvA/ue+dbJac+YRx/fMY+n6veRO9zzqJxJ4lk/4DbP/rdDLBrEmlNLX1hNJa0mho7iQTPWvCMBynL8tA4u5f15JUSN5FqQRZh8qSFBPLimju6CGVTNCbyZJ1pyiVoLUrQ1dv317fgRPiHFyfwe9vYJqcz2RHyjq+58vB7zX6jEMqaRQlExSlwt+JeXoOyqHfogaY2VXAVQCzZ49thxKJi1QyQYX6sJL9kM9fTQMwK+f9zGjYsNOYWQqYQGg03oO73+juS919aV1dXZ7CFRGJp3wmgseA+WY218yKgMuB5UOmWQ68N3r9LuD3ah8QETm48lY1FNX5fwy4h3D56E3uvsbMvgSscPflwPeAH5nZOmAnIVmIiMhBlNc2Ane/C7hryLBrc153AZfkMwYREdk3tSyJiMScEoGISMwpEYiIxJwSgYhIzI273kfNrBHYuJ8fr+XwuWv5cFoXOLzWR+tyaIr7uhzh7sPeiDXuEsGrYWYrRup0abw5nNYFDq/10bocmrQuI1PVkIhIzCkRiIjEXNwSwY2FDuAAOpzWBQ6v9dG6HJq0LiOIVRuBiIjsLW4lAhERGUKJQEQk5mKTCMzsXDN7zszWmdk1hY5nrMxsg5k9bWZPmtmKaNgkM/utmb0Q/X+wH7w1KmZ2k5ltN7PVOcOGjd2Cr0XbaZWZLSlc5HsbYV2+aGYN0bZ50szOzxn3uWhdnjOzvyhM1MMzs1lmdp+ZPWNma8zsE9Hwcbdt9rEu427bmFmJmT1qZk9F6/KP0fC5ZvZIFPNPo+79MbPi6P26aPycMS/U+x+1dhj/EbrBfhGYBxQBTwELCx3XGNdhA1A7ZNi/AddEr68B/rXQcY4Q++uBJcDqV4odOB+4m/DI1tOBRwod/yjW5YvAZ4eZdmH0WysG5ka/wWSh1yEnvmnAkuh1JfB8FPO42zb7WJdxt22i77ciep0GHom+71uAy6PhNwAfiV7/LXBD9Ppy4KdjXWZcSgSnAuvcfb279wDLgAsLHNOBcCHwg+j1D4B3FDCWEbn7A4TnTeQaKfYLgR968DBQbWbTDk6kr2yEdRnJhcAyd+9295eAdYTf4iHB3be4++PR61ZgLeE54uNu2+xjXUZyyG6b6Ptti96moz8H3gTcGg0ful36t9etwNlmZmNZZlwSwQxgU877evb9IzkUOfAbM1sZPcMZYIq7b4lebwWmFCa0/TJS7ON1W30sqi65KaeKbtysS1SdcBLh7HNcb5sh6wLjcNuYWdLMngS2A78llFh2uXsmmiQ33oF1icbvBmrGsry4JILDwZnuvgQ4D/iomb0+d6SHcuG4vBZ4PMce+RZwJLAY2AL838KGMzZmVgHcBnzS3Vtyx423bTPMuozLbePufe6+mPCs91OBY/O5vLgkggZgVs77mdGwccPdG6L/twN3EH4c2/qL5tH/2wsX4ZiNFPu421buvi3acbPAdxisYjjk18XM0oQD54/d/fZo8LjcNsOty3jeNgDuvgu4D3gNoSqu/6mSufEOrEs0fgLQNJblxCURPAbMj1rdiwgNKssLHNOomVm5mVX2vwbeAqwmrMN7o8neC9xZmAj3y0ixLwf+KrpC5XRgd041xSFpSD35RYRtA2FdLo+u6pgLzAcePdjxjSSqR/4esNbd/yNn1LjbNiOty3jcNmZWZ2bV0etS4BxCm8d9wLuiyYZul/7t9S7g91FJbvQK3UJ+sP4IVzw8T6hr+3yh4xlj7PMIVzg8Bazpj59QD/g74AXgXmBSoWMdIf6bCcXyXkLd5gdGip1wxcT10XZ6Glha6PhHsS4/imJdFe2U03Km/3y0Ls8B5xU6/iHrciah2mcV8GT0d/543Db7WJdxt22ARcATUcyrgWuj4fMIyWod8DOgOBpeEr1fF42fN9ZlqosJEZGYi0vVkIiIjECJQEQk5pQIRERiTolARCTmlAhERGJOiUDkIDKzs8zsl4WOQySXEoGISMwpEYgMw8yujPqEf9LMvh11AtZmZv8Z9RH/OzOri6ZdbGYPRx2b3ZHTf/9RZnZv1K/842Z2ZDT7CjO71cyeNbMfj7WnSJEDTYlAZAgzWwBcBpzhoeOvPuDdQDmwwt2PA/4AXBd95IfA37v7IsJdrP3Dfwxc7+4nAq8l3JEMoWfMTxL6xJ8HnJH3lRLZh9QrTyISO2cDJwOPRSfrpYSO17LAT6Np/ge43cwmANXu/odo+A+An0V9Q81w9zsA3L0LIJrfo+5eH71/EpgD/Cn/qyUyPCUCkb0Z8AN3/9weA83+Ych0+9s/S3fO6z60H0qBqWpIZG+/A95lZpNh4Bm+RxD2l/7eH/8S+JO77waazex10fD3AH/w8JSsejN7RzSPYjMrO6hrITJKOhMRGcLdnzGzLxCeCJcg9DT6UaAdODUat53QjgChC+AbogP9euD90fD3AN82sy9F87jkIK6GyKip91GRUTKzNnevKHQcIgeaqoZERGJOJQIRkZhTiUBEJOaUCEREYk6JQEQk5pQIRERiTolARCTm/j/UN5r/+rlh2wAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "print(history.history.keys())\n",
        "# # summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'valid'], loc='upper right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plotting the results\n",
        "\n",
        "After our training, we need to plot our results to verify if we are making a good prediction since the value of the loss can be heavily influenced by outliers. The following cell includes nine plots we use to evaluate performance on each of the points."
      ],
      "metadata": {
        "id": "Hb4C5FSDf-2n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for the volume plot of hotspot\n",
        "\n",
        "def area_hotspot(datapoint):\n",
        "  tinc = 5\n",
        "  tnum = int(4500/tinc)\n",
        "\n",
        "  tinc = 5\n",
        "  tnum = int(4500/tinc)\n",
        "\n",
        "  bins=np.zeros((tnum,2))\n",
        "  bins[:,0] = [ tinc*(0.5 + x) for x in list(range(tnum))]\n",
        "\n",
        "  for bin_index, temp in np.ndenumerate(datapoint):\n",
        "    theta = temp * 1000\n",
        "    tind=math.floor(theta/tinc)\n",
        "    bins[:tind, 1] += 4 # ~ Roughly 2*2*1 nm^3 (volume value from Chunyu)\n",
        "  \n",
        "  return bins"
      ],
      "metadata": {
        "id": "cVbLrXkBNWDI"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def datapoint_results_print(simulation_point, simulation_temperatures, prediction_tensor):\n",
        "\n",
        "  # Simulation point is the input train_data[<point order>,<dim 0>,<dim 1>,<dim 2>,<channel>]\n",
        "  # For example, simulation_point = train_data[i,:,:,:,:]\n",
        "\n",
        "\n",
        "\n",
        "  fig = make_subplots(rows=3, cols=3, specs=[[{\"type\": \"scatter3d\"},{\"type\": \"scatter3d\"},{\"type\": \"scatter3d\"}],\n",
        "                                             [{\"type\": \"scatter3d\"},{\"type\": \"scatter3d\"},{\"type\": \"scatter\"}],\n",
        "                                             [{\"type\": \"histogram\"},{\"type\": \"histogram\"},{\"type\": \"scatter\"}]],\n",
        "                      subplot_titles=[\"Input 1\",\"Input 2\",\"Input 3\",\n",
        "                                      \"Temp (Labels)\",\"Temp (Predictions)\",\"Parity Plot\",\n",
        "                                      \"Temp (Distributions)\", \"Residuals\", \"Hotspot volume\"], horizontal_spacing = 0.1, vertical_spacing = 0.1)\n",
        "  \n",
        "  fig.update_layout(autosize=False, width=800, height=800) \n",
        "\n",
        "  # FIRST PLOT --> INPUT 1\n",
        "\n",
        "  input_1 = simulation_point[:,:,:,0].squeeze()\n",
        "\n",
        "  X,Y,Z = np.mgrid[0:input_1.shape[0], 0:input_1.shape[1], 0:input_1.shape[2]]\n",
        "  #input_1_xz = np.swapaxes(input_1, 2, 0)\n",
        "\n",
        "  trace_1 = go.Scatter3d(x = X.flatten(), y = Y.flatten(), z=Z.flatten(),\n",
        "                          mode='markers', marker=dict(colorscale='Viridis', size=8, line=dict(width=0), symbol='square', color = input_1.flatten(), colorbar=dict(thickness=20, len=0.3, x=0.27, y=0.9)), showlegend=False)\n",
        "  fig.add_trace(trace_1, row=1, col=1)\n",
        "\n",
        "  # SECOND PLOT --> INPUT 2\n",
        "\n",
        "  input_2 = simulation_point[:,:,:,1].squeeze()\n",
        "\n",
        "  X,Y,Z = np.mgrid[0:input_2.shape[0], 0:input_2.shape[1], 0:input_2.shape[2]]\n",
        "  #input_2_xz = np.swapaxes(input_2, 2, 0)\n",
        "\n",
        "  trace_2 = go.Scatter3d(x = X.flatten(), y = Y.flatten(), z=Z.flatten(),\n",
        "                          mode='markers', marker=dict(colorscale='Viridis', size=8, line=dict(width=0), symbol='square', color = input_2.flatten(), colorbar=dict(thickness=20, len=0.3, x=0.62, y=0.9)), showlegend=False)\n",
        "  fig.add_trace(trace_2, row=1, col=2)\n",
        "\n",
        "\n",
        "  # THIRD PLOT --> INPUT 3\n",
        "\n",
        "  input_3 = simulation_point[:,:,:,2].squeeze()\n",
        "\n",
        "  X,Y,Z = np.mgrid[0:input_3.shape[0], 0:input_3.shape[1], 0:input_3.shape[2]]\n",
        "  #input_3_xz = np.swapaxes(input_3, 2, 0)\n",
        "\n",
        "  trace_3 = go.Scatter3d(x = X.flatten(), y = Y.flatten(), z=Z.flatten(),\n",
        "                          mode='markers', marker=dict(colorscale='Viridis', size=8, line=dict(width=0), symbol='square', color = input_3.flatten(), colorbar=dict(thickness=20, len=0.3, x=1, y=0.9)), showlegend=False)\n",
        "  fig.add_trace(trace_3, row=1, col=3)\n",
        "\n",
        "\n",
        "  # FOURTH PLOT --> TEMPS (LABELS)\n",
        "\n",
        "  maxval = max(np.max(prediction_tensor), np.max(simulation_temperatures))\n",
        "  \n",
        "\n",
        "  X,Y,Z = np.mgrid[0:simulation_temperatures.shape[0], 0:simulation_temperatures.shape[1], 0:simulation_temperatures.shape[2]]\n",
        "  #simulation_temperatures_xz = np.swapaxes(simulation_temperatures, 2, 0)\n",
        "\n",
        "  trace_4 = go.Scatter3d(x = X.flatten(), y = Y.flatten(), z=Z.flatten(),\n",
        "                          mode='markers', marker=dict(cmin=0, cmax=maxval, colorscale='Reds', size=8, line=dict(width=0), symbol='square', color = simulation_temperatures.flatten(), colorbar=dict(thickness=20,len=0.3, x=0.27, y=0.5)), showlegend=False)\n",
        "  fig.add_trace(trace_4, row=2, col=1)\n",
        "\n",
        "\n",
        "  # FIFTH PLOT --> TEMPS (PREDICTIONS)\n",
        "\n",
        "  X,Y,Z = np.mgrid[0:prediction_tensor.shape[0], 0:prediction_tensor.shape[1], 0:prediction_tensor.shape[2]]\n",
        "  #prediction_tensor_xz = np.swapaxes(prediction_tensor, 2, 0)\n",
        "\n",
        "  trace_5 = go.Scatter3d(x = X.flatten(), y = Y.flatten(), z=Z.flatten(),\n",
        "                          mode='markers', marker=dict(cmin=0, cmax=maxval, colorscale='Reds', size=8, line=dict(width=0), symbol='square', color = prediction_tensor.flatten(), colorbar=dict(thickness=20, len=0.3, x=0.62, y=0.5)), showlegend=False)\n",
        "  fig.add_trace(trace_5, row=2, col=2)\n",
        "\n",
        "  # SIXTH PLOT --> PARITY PLOT\n",
        "\n",
        "  trace_6 = go.Scatter(x=simulation_temperatures.flatten(), y = prediction_tensor.flatten(), mode='markers', showlegend=False)\n",
        "  fig.add_trace(trace_6, row=2, col=3)\n",
        "  fig.update_xaxes(title=\"Scaled Temperature (K) - Labels\", row=2, col=3)\n",
        "  fig.update_yaxes(title=\"Scaled Temperature (K) - Predictions\", row=2, col=3)\n",
        "\n",
        "\n",
        "  # SEVENTH PLOT --> TEMP (Distributions)\n",
        "\n",
        "  trace_7 = go.Histogram(x=prediction_tensor.flatten(), name='Predictions', marker=dict(color='red'), showlegend=True, legendgroup = '1')\n",
        "  trace_7b = go.Histogram(x=simulation_temperatures.flatten(), name='Truth', marker=dict(color='green'), showlegend=True, legendgroup = '1')\n",
        "  fig.update_xaxes(title=\"Temperature (K) - Labels\", row=3, col=1)\n",
        "  fig.add_trace(trace_7, row=3, col=1)\n",
        "  fig.add_trace(trace_7b, row=3, col=1) \n",
        "\n",
        "\n",
        "  # EIGHTH PLOT --> RESIDUALS\n",
        "\n",
        "  diff_xz = prediction_tensor - simulation_temperatures\n",
        "  flat_diff_xz = diff_xz.flatten()\n",
        "\n",
        "  trace_8 = go.Histogram(x=flat_diff_xz, showlegend=False)\n",
        "  trace_line = go.Scatter(x=[0,0], y = [0,700], mode='lines', showlegend=False)\n",
        "\n",
        "  fig.add_trace(trace_8, row=3, col=2)\n",
        "  fig.add_trace(trace_line, row=3, col=2)\n",
        "\n",
        "  # NINTH PLOT ---> VOLUME OF HOTSPOT\n",
        "  \n",
        "  bins_labels = area_hotspot(simulation_temperatures)\n",
        "  bins_predictions = area_hotspot(prediction_tensor)\n",
        "  trace_9 = go.Scatter(x=bins_predictions[:,1], y=bins_predictions[:,0], mode='lines',  name='Predictions', marker=dict(color='red'), showlegend=True, legendgroup = '2')\n",
        "  trace_9b = go.Scatter(x=bins_labels[:,1], y=bins_labels[:,0], mode='lines', name='Truth', marker=dict(color='green'), showlegend=True, legendgroup = '2')\n",
        "  \n",
        "  fig.add_trace(trace_9, row=3, col=3)\n",
        "  fig.add_trace(trace_9b, row=3, col=3) \n",
        "  fig.update_xaxes(type=\"log\", row=3, col=3)\n",
        "  fig.update_xaxes(title=\"Hotspot Volume (nm^3)\", row=3, col=3)\n",
        "  fig.update_yaxes(title=\"Temperature (K)\", row=3, col=3)  \n",
        "\n",
        "  #### \n",
        "\n",
        "  fig.update_layout(autosize=False, width=1400, height=1000, legend_tracegroupgap = 180, legend=dict(font=dict(size=16),orientation=\"h\"))   \n",
        "\n",
        "  return fig"
      ],
      "metadata": {
        "id": "p65_1oRxgTu1"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since rendering multiple plots for each system is not ideal, we will save a numpy array with the model predictions and an HTML file with the interactive plots for exploration."
      ],
      "metadata": {
        "id": "0Jx7VbNCihTX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p results/train\n",
        "!mkdir -p results/validation"
      ],
      "metadata": {
        "id": "zVpCWbXshU4h"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PREDICTIONS OVER TRAINING DATA\n",
        "\n",
        "for i, title in enumerate(paths):\n",
        "  print(i)\n",
        "  inppoint = train_data[i,:,:,:,:].squeeze()\n",
        "  inplabel = train_labels[i,:,:,:,:].squeeze()\n",
        "\n",
        "  prediction_point = model.predict(tf.expand_dims(inppoint, axis=0))\n",
        "  prediction_tensor = prediction_point.squeeze()\n",
        "\n",
        "  inppoint = inppoint[:, 1:-1, 1:-1] # NON PERIODIC GETS PLOTTED\n",
        "\n",
        "  fig = datapoint_results_print(inppoint, inplabel, prediction_tensor)\n",
        "\n",
        "  os.mkdir('results/'+str(title))\n",
        "\n",
        "  ### SAVING NUMPY PREDICTIONS\n",
        "  np.save('results/'+str(title)+'/prediction.npy', prediction_tensor)\n",
        "  ### SAVING HTML IMAGES\n",
        "  fig.write_html('results/'+str(title)+'/visualization.html')"
      ],
      "metadata": {
        "id": "nTuoEr8Khjpw",
        "outputId": "5aaea8d7-6426-4000-dea0-459fd96f435f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "2\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "3\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "4\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "5\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "6\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "7\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "8\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "9\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "10\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "11\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "12\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "13\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "14\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "15\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "16\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "17\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "18\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "19\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "20\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "21\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "22\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "23\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "24\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "25\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "26\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "27\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "28\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "29\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "30\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "31\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "32\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "33\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "34\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "35\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "36\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "37\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "38\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "39\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "40\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "41\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "42\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "43\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "44\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "45\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "46\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "47\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "48\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "49\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "50\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "51\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "52\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "53\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "54\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "55\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "56\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "57\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "58\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "59\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "60\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "61\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "62\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "63\n",
            "1/1 [==============================] - 0s 23ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PREDICTIONS OVER VALIDATION DATA\n",
        "\n",
        "for i, title in enumerate(validation_paths):\n",
        "  print(i)\n",
        "  inppoint = validation_data[i,:,:,:,:].squeeze()\n",
        "  inplabel = validation_labels[i,:,:,:,:].squeeze()\n",
        "\n",
        "  prediction_point = model.predict(tf.expand_dims(inppoint, axis=0))\n",
        "  prediction_tensor = prediction_point.squeeze()\n",
        "\n",
        "  inppoint = inppoint[:, 1:-1, 1:-1] # NON PERIODIC GETS PLOTTED\n",
        "  fig = datapoint_results_print(inppoint, inplabel, prediction_tensor)\n",
        "  \n",
        "  os.mkdir('results/'+str(title))\n",
        "\n",
        "  ### SAVING NUMPY PREDICTIONS\n",
        "  np.save('results/'+str(title)+'/prediction.npy', prediction_tensor)\n",
        "  ### SAVING HTML IMAGES\n",
        "  fig.write_html('results/'+str(title)+'/visualization.html')"
      ],
      "metadata": {
        "id": "nYhrkn0_iK52",
        "outputId": "72c91a3b-6b33-4cc2-a3dc-5705561579df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "2\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "3\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "4\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "5\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "6\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "7\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "8\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "9\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "10\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "11\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "12\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "13\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "14\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "15\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "16\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "17\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "18\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "19\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "20\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "21\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "22\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "23\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "24\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "25\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "26\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "27\n",
            "1/1 [==============================] - 0s 20ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploration GUI  (Under development)"
      ],
      "metadata": {
        "id": "8GgptMfOlBs2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def GUI(inplabel, prediction, fig):\n",
        "  #########\n",
        "\n",
        "  fig = go.FigureWidget(fig)\n",
        "\n",
        "  toggle_button = widgets.ToggleButtons(options=['MD', 'Pred'], description='Filter by:', button_style='')\n",
        "  md_tmp_slider = widgets.FloatRangeSlider(value=[0, np.max(inplabel)], min=0, max=np.max(inplabel), description='MD Temp:', disabled=False, continuous_update=False)\n",
        "  pred_tmp_slider = widgets.FloatRangeSlider(value=[0, np.max(prediction)], min=0, max=np.max(prediction), description='CNN Temp:', disabled=True, continuous_update=False)\n",
        "\n",
        "  slider_box =widgets.VBox([md_tmp_slider, pred_tmp_slider])\n",
        "  controls_box = widgets.HBox([slider_box, toggle_button])\n",
        "  endbox = widgets.VBox([controls_box, fig], layout=widgets.Layout(width='100%'))\n",
        "  \n",
        "  display(endbox)  \n",
        "\n",
        "  # OBSERVERS\n",
        "\n",
        "  def response_MD(change):\n",
        "\n",
        "    map = np.where(np.logical_and(inplabel>=md_tmp_slider.value[0], inplabel<=md_tmp_slider.value[1]), 8, 0)\n",
        "    with fig.batch_update():\n",
        "      fig.data[0].marker.size = map.flatten()\n",
        "      fig.data[1].marker.size = map.flatten()\n",
        "      fig.data[2].marker.size = map.flatten()\n",
        "      fig.data[3].marker.size = map.flatten()\n",
        "      fig.data[4].marker.size = map.flatten()\n",
        "\n",
        "\n",
        "  def response_Pred(change):\n",
        "\n",
        "    map = np.where(np.logical_and(prediction>=pred_tmp_slider.value[0], prediction<=pred_tmp_slider.value[1]), 8, 0)\n",
        "    with fig.batch_update():\n",
        "      fig.data[0].marker.size = map.flatten()\n",
        "      fig.data[1].marker.size = map.flatten()\n",
        "      fig.data[2].marker.size = map.flatten()\n",
        "      fig.data[3].marker.size = map.flatten()\n",
        "      fig.data[4].marker.size = map.flatten()\n",
        "\n",
        "  def slider_function(b):\n",
        "    md_tmp_slider.value = [0, np.max(inplabel)]\n",
        "    md_tmp_slider.disabled = (toggle_button.value == 'Pred')\n",
        "\n",
        "    pred_tmp_slider.value = [0, np.max(prediction)]\n",
        "    pred_tmp_slider.disabled = (toggle_button.value == 'MD')\n",
        "\n",
        "  toggle_button.observe(slider_function)\n",
        "  md_tmp_slider.observe(response_MD)\n",
        "  pred_tmp_slider.observe(response_Pred)\n",
        "\n",
        "  return fig\n",
        "\n",
        "\n",
        "  #fig.write_html('results/'+str(title)+'.html')"
      ],
      "metadata": {
        "id": "vUEMAz7qfj_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in [0]:\n",
        "  print(i)\n",
        "  print(paths[i])\n",
        "  inppoint = train_data[i,:,:,:,:].squeeze()\n",
        "  inplabel = train_labels[i,:,:,:,:].squeeze()\n",
        "\n",
        "  prediction_point = model.predict(tf.expand_dims(inppoint, axis=0))\n",
        "  prediction_tensor = prediction_point.squeeze()\n",
        "\n",
        "  inppoint = inppoint[:, 1:-1, 1:-1] # NON PERIODIC GETS PLOTTED\n",
        "  fig = datapoint_results_print(inppoint, inplabel, prediction_tensor)\n",
        "\n",
        "  # GUI\n",
        "\n",
        "  updated_fig = GUI(inplabel, prediction_tensor, fig)\n",
        "\n",
        "  # ### SAVING NUMPY PREDICTIONS\n",
        "  \n",
        "  # np.save('results/'+str(title)+'.npy', prediction_tensor)"
      ],
      "metadata": {
        "id": "-PCvavvNP6kj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "updated_fig.show()\n",
        "# updated_fig.write_html('test4.html')"
      ],
      "metadata": {
        "id": "MA8uxuCQ3Hj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for i, title in enumerate(validation_paths):\n",
        "#   print(i)\n",
        "#   inppoint = validation_data[i,:,:,:,:].squeeze()\n",
        "#   inplabel = validation_labels[i,:,:,:,:].squeeze()\n",
        "#   fig, prediction =  datapoint_results_print(inppoint, inplabel)\n",
        "#   ### SAVING NUMPY PREDICTIONS\n",
        "#   np.save('results/'+str(title)+'.npy', prediction)\n",
        "#   #fig.write_image('results/'+str(title)+'.pdf')\n",
        "#   fig.write_html('results/'+str(title)+'.html')"
      ],
      "metadata": {
        "id": "ClEFHwOOjvd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in [0]:\n",
        "  print(i)\n",
        "  print(validation_paths[i])\n",
        "  inppoint = validation_data[i,:,:,:,:].squeeze()\n",
        "  inplabel = validation_labels[i,:,:,:,:].squeeze()\n",
        "\n",
        "  prediction_point = model.predict(tf.expand_dims(inppoint, axis=0))\n",
        "  prediction_tensor = prediction_point.squeeze()\n",
        "\n",
        "  inppoint = inppoint[:, 1:-1, 1:-1] # NON PERIODIC GETS PLOTTED\n",
        "  fig = datapoint_results_print(inppoint, inplabel, prediction_tensor)\n",
        "\n",
        "  # GUI\n",
        "\n",
        "  updated_fig = GUI(inplabel, prediction_tensor, fig)\n",
        "\n",
        "  # ### SAVING NUMPY PREDICTIONS\n",
        "  \n",
        "  # np.save('results/'+str(title)+'.npy', prediction_tensor)"
      ],
      "metadata": {
        "id": "c52fdGLIA_W3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "updated_fig.show()"
      ],
      "metadata": {
        "id": "aAAv5I7aIfdh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OvGM6Bn8ImGo"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "pbx-local",
      "language": "python",
      "name": "pbx-local"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15"
    },
    "vscode": {
      "interpreter": {
        "hash": "f806901b044f1f905f6f73d9d34ac86b2be2e087ac41c051b6f31285dd315984"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}