{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jverduzc/CNN_PBX_Model/blob/master/CNN_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convolutional Neural Networks (CNN) for prediction of hotspots on PBX\n",
        "\n",
        "In this notebook we create a CNN model with a U-Net architecture for the prediction of temperature for a plastically bonded explosive molecular dynamics simulation."
      ],
      "metadata": {
        "id": "4QfRMy9VFrcW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Libraries\n",
        "\n",
        "First, we need to set up libraries that are not part of the default environment in Google Colab. In our case, this is only the rendering library ```kaleido```.\n",
        "\n",
        "You will need to restart the runtime to update the kernel.\n",
        "- Menu Runtime -> Restart Runtime"
      ],
      "metadata": {
        "id": "C2UgThuCGG3F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaleido"
      ],
      "metadata": {
        "id": "M79B0dH7ftZW",
        "outputId": "9618900f-8ab1-4249-b87a-fb19a014826e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: kaleido in /usr/local/lib/python3.8/dist-packages (0.2.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will then import the required libraries for our notebook to run. The following cells import:\n",
        "- Standard python libraries\n",
        "- Plotting libraries\n",
        "- Machine learning libraries (tensorflow / keras)"
      ],
      "metadata": {
        "id": "2ECMQghLGzUA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5Nd2jeT8r8uu"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import copy\n",
        "import math\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import clear_output\n",
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()"
      ],
      "metadata": {
        "id": "qrgVqntVG6YP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "V08TFba6G_nw"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import initializers\n",
        "from keras.layers import Input, Dropout, BatchNormalization, Conv3DTranspose, concatenate, Dense, Conv3D, Flatten, MaxPooling3D\n",
        "from keras.models import Sequential\n",
        "import keras.backend as K\n",
        "tf.keras.utils.set_random_seed(0)\n",
        "\n",
        "from sklearn.cluster import AgglomerativeClustering"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to verify that we are running an enviroment with a GPU in Colab. The next cell shows if you have a GPU front-end execution host allocated to the notebook. \n",
        "\n",
        "<font color='red'><b>Warning:</b></font> If you don't have one, you'll need to re-run the previous cells after going to:\n",
        "- Menu Runtime -> Change runtime type -> Hardware accelerator"
      ],
      "metadata": {
        "id": "Wh5PkIn3H9dD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "KnH0b5QqG_ny",
        "outputId": "a2ccfa96-9f08-45b6-d853-eac888238554",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  1\n"
          ]
        }
      ],
      "source": [
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data\n",
        "\n",
        "After setting up the environment, we need the repository files to create the model and access the training/validation data. \n",
        "\n",
        "<font color='orange'><b>Attention:</b></font> This is not trivial in Colab, but to access a private repository on Github like this, you need to provide Colab with your Github Key. You can get that key here: https://github.com/settings/tokens\n",
        "\n",
        "After that, you need to execute a command with this syntax:\n",
        "```\n",
        "!git clone https://username:github_key@github.com/Jverduzc/CNN_PBX_Model.git\n",
        "```\n",
        "You can fill up the blanks in the following cell. We will also change the current working directory. You should see the a new directory in your directory tree (on your left) with the files on the repository.\n"
      ],
      "metadata": {
        "id": "hYseV3r9LkME"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone XXX\n",
        "os.chdir(\"CNN_PBX_Model\")"
      ],
      "metadata": {
        "id": "Cr1dF6TJHU2J",
        "outputId": "0da273d6-41a6-46be-f788-c8f12aa3770e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CNN_PBX_Model'...\n",
            "remote: Enumerating objects: 569, done.\u001b[K\n",
            "remote: Counting objects: 100% (28/28), done.\u001b[K\n",
            "remote: Compressing objects: 100% (23/23), done.\u001b[K\n",
            "remote: Total 569 (delta 9), reused 19 (delta 5), pack-reused 541\u001b[K\n",
            "Receiving objects: 100% (569/569), 23.00 MiB | 13.50 MiB/s, done.\n",
            "Resolving deltas: 100% (262/262), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training and validation data\n",
        "\n",
        "This notebook was designed to read everything in the ```/train/``` folder as training data and everything in the ```/validation/``` folder as validation data. These directories contain individual labeled subdirectories that represent each of our simulation systems (data points).\n",
        "\n",
        "In each of the simulation systems subdirectories there are two files as numpy arrays: ```input.npy``` and ```output.npy```.\n",
        "\n",
        "\n",
        "```input.npy``` contains a (16 x 34 x 34 x 3) array with the input mappings from our simulations. The first three numbers represent the dimensions (in bins) of our system. The last number represents the number of mappings for our inputs. For each of our 3D structures, we generate the following:\n",
        "- Total density\n",
        "- HE density\n",
        "- GB interface parameter\n",
        "\n",
        "```output.npy``` contains a (16 x 32 x 32 x 1) array with the output mapping (temperature) from our simulations. The first three numbers represent the dimensions (in bins) of our system, but note that they are different from the inputs due to periodic boundary conditions. The last number represents the temperature mapping. \n",
        "- Temperature\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "B2wbmJTjOpoe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please verify that after running the next cells you get the following results:\n",
        "\n",
        "<b>Training data:</b><br>\n",
        "\n",
        "(64, 16, 34, 34, 3) <br>\n",
        "(64, 16, 32, 32, 1) <br>\n",
        "\n",
        "<b>Validation data:</b><br>\n",
        "\n",
        "(28, 16, 34, 34, 3) <br>\n",
        "(28, 16, 32, 32, 1) <br>\n",
        "\n",
        "You can see that there is a new number in these arrays. It indicates the number of datapoints in each set. You can read this as having 64 points with (16 x 34 x 34 x 3) shape."
      ],
      "metadata": {
        "id": "ogxA0zxHRUIl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "KZxqA1mmG_nz",
        "outputId": "baeb6330-14da-4e0d-f0d5-398206d60992",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 16, 34, 34, 3)\n",
            "(64, 16, 32, 32, 1)\n"
          ]
        }
      ],
      "source": [
        "# TRAINING DATA\n",
        "\n",
        "paths = [x[0] for x in os.walk('train/')][1:]\n",
        "\n",
        "train_data = []\n",
        "train_labels = []\n",
        "\n",
        "for i in paths:\n",
        "  train_ex = np.load(i + \"/input.npy\")\n",
        "\n",
        "  if train_ex.shape != (16,34,34,3):\n",
        "\n",
        "    first_axis_pad = int(0.5 * (34 - train_ex.shape[1]))\n",
        "    second_axis_pad = int(0.5 * (34 - train_ex.shape[2]))\n",
        "\n",
        "    train_ex = np.pad(train_ex, ((0, 0), (first_axis_pad, first_axis_pad), (second_axis_pad, second_axis_pad), (0,0)), 'wrap')\n",
        "\n",
        "  train_lb = np.load(i + \"/output.npy\")\n",
        "  train_data.append(train_ex)\n",
        "  train_labels.append(train_lb)\n",
        "\n",
        "train_data = np.array(train_data)\n",
        "train_labels = np.array(train_labels) / 1000\n",
        "\n",
        "print(train_data.shape)\n",
        "print(train_labels.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "02ORxH_vZNOC",
        "outputId": "0477f075-5b2d-46cc-9b72-e3a98368b151",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(28, 16, 34, 34, 3)\n",
            "(28, 16, 32, 32, 1)\n"
          ]
        }
      ],
      "source": [
        "# VALIDATION DATA\n",
        "\n",
        "validation_paths = [x[0] for x in os.walk('validation/')][1:]\n",
        "\n",
        "validation_data = []\n",
        "validation_labels = []\n",
        "\n",
        "for i in validation_paths:\n",
        "  validation_ex= np.load(i + \"/input.npy\")\n",
        "\n",
        "  if validation_ex.shape != (16,34,34,1):\n",
        "\n",
        "    first_axis_pad = int(0.5 * (34 - validation_ex.shape[1]))\n",
        "    second_axis_pad = int(0.5 * (34 - validation_ex.shape[2]))\n",
        "\n",
        "    validation_ex = np.pad(validation_ex, ((0, 0), (first_axis_pad, first_axis_pad), (second_axis_pad, second_axis_pad), (0,0)), 'wrap')\n",
        "\n",
        "\n",
        "  validation_lb = np.load(i + \"/output.npy\")\n",
        "  validation_data.append(validation_ex)\n",
        "  validation_labels.append(validation_lb)\n",
        "\n",
        "validation_data = np.array(validation_data)\n",
        "validation_labels = np.array(validation_labels) / 1000\n",
        "\n",
        "print(validation_data.shape)\n",
        "print(validation_labels.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model\n",
        "\n",
        "This notebook creates an architecture based on U-Net, a CNN algorithm for image segmentation. We will go a bit into the design of the architecture in the following cells.\n",
        "\n",
        "This first function addresses periodic padding for tensors in the model."
      ],
      "metadata": {
        "id": "xQrUeLx5R1mp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "fWvLsqQXsWGy"
      },
      "outputs": [],
      "source": [
        "# Code taken from: https://stackoverflow.com/questions/39088489/tensorflow-periodic-padding\n",
        "\n",
        "def periodic_padding_flexible(tensor, axis, padding=1):\n",
        "\n",
        "    if isinstance(axis,int):\n",
        "        axis = (axis,)\n",
        "    if isinstance(padding,int):\n",
        "        padding = (padding,)\n",
        "\n",
        "    ndim = len(tensor.shape)\n",
        "\n",
        "    for ax,p in zip(axis,padding):\n",
        "        # create a slice object that selects everything from all axes,\n",
        "        # except only 0:p for the specified for right, and -p: for left\n",
        "\n",
        "        ind_right = [slice(-p,None) if i == ax else slice(None) for i in range(ndim)]\n",
        "        ind_left = [slice(0, p) if i == ax else slice(None) for i in range(ndim)]\n",
        "        right = tensor[ind_right]\n",
        "        left = tensor[ind_left]\n",
        "        middle = tensor\n",
        "        tensor = tf.concat([right,middle,left], axis=ax)\n",
        "\n",
        "    return tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since this architecture is going to be complicated, we will start by creating mini-blocks. This first function ```DownConvBlock``` executes the following operations sequentially:\n",
        "\n",
        "- (Optional) Periodic Padding\n",
        "- 3D Convolutional Layer\n",
        "- Periodic Padding\n",
        "- 3D Convolutional Layer\n",
        "- Batch normalization\n",
        "- (Optional) MaxPooling3D Layer"
      ],
      "metadata": {
        "id": "i2sWbazsagHg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "rLhZ3NIocWJ9"
      },
      "outputs": [],
      "source": [
        "def DownConvBlock(inputs, n_filters=32, filter_size = 3, max_pooling=True, special_padding=False):\n",
        "\n",
        "  padding_size = int((filter_size-1)/2)\n",
        "  kernel_init =   tf.keras.initializers.GlorotUniform(seed=0)\n",
        "  bias_init = tf.keras.initializers.Zeros()\n",
        "\n",
        "\n",
        "  # PERIODIC PADDING\n",
        "\n",
        "  inputs = periodic_padding_flexible(inputs, axis=1,padding=padding_size)\n",
        "  if special_padding == False:\n",
        "    inputs = periodic_padding_flexible(inputs, axis=2,padding=padding_size)\n",
        "    inputs = periodic_padding_flexible(inputs, axis=3,padding=padding_size)\n",
        "  \n",
        "  conv = Conv3D(n_filters, filter_size, activation='relu', padding='valid', kernel_initializer=kernel_init, bias_initializer=bias_init)(inputs)\n",
        "  print(conv.shape)\n",
        "  conv = periodic_padding_flexible(conv, axis=(1,2,3),padding=(padding_size,padding_size,padding_size))\n",
        "  conv = Conv3D(n_filters, filter_size, activation='relu', padding='valid', kernel_initializer=kernel_init, bias_initializer=bias_init)(conv)\n",
        "  print(conv.shape)\n",
        "  conv = BatchNormalization()(conv, training=False)\n",
        "      \n",
        "  if max_pooling:\n",
        "    next_layer = tf.keras.layers.MaxPooling3D(pool_size = (2,2,2))(conv)\n",
        "  else:\n",
        "    next_layer = conv\n",
        "  \n",
        "  skip_connection = conv   \n",
        "\n",
        "  print(\"end_of_block\") \n",
        "  return next_layer, skip_connection"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This second function ```UpConvBlock``` executes the following operations sequentially:\n",
        "\n",
        "- Transpose 3D Convolutional Layer\n",
        "- Merge with <i>skip connection</i> from the corresponding ```DownConvBlock```\n",
        "- Periodic Padding\n",
        "- 3D Convolutional Layer\n",
        "- Periodic Padding\n",
        "- 3D Convolutional Layer"
      ],
      "metadata": {
        "id": "i8h2MOJHa_Aq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "YvMNFlsHseOF"
      },
      "outputs": [],
      "source": [
        "def UpConvBlock(prev_layer_input, skip_layer_input, filter_size = 3, n_filters=32):\n",
        "\n",
        "    padding_size = int((filter_size-1)/2)\n",
        "    kernel_init = tf.keras.initializers.GlorotUniform(seed=0)\n",
        "    bias_init = tf.keras.initializers.Zeros()   \n",
        "\n",
        "    up = Conv3DTranspose(n_filters, (filter_size,filter_size,filter_size),\n",
        "                         strides=(filter_size-1,filter_size-1,filter_size-1),\n",
        "                         padding='same', kernel_initializer=kernel_init, bias_initializer=bias_init)(prev_layer_input)\n",
        "\n",
        "    merge = concatenate([up, skip_layer_input], axis=4)\n",
        "    merge = periodic_padding_flexible(merge, axis=(1,2,3),padding=(padding_size,padding_size,padding_size))\n",
        "    conv = Conv3D(n_filters, filter_size, activation='relu', padding='valid', kernel_initializer=kernel_init,  bias_initializer=bias_init)(merge)\n",
        "    print(conv.shape)\n",
        "    conv = periodic_padding_flexible(conv, axis=(1,2,3),padding=(padding_size,padding_size,padding_size))\n",
        "    conv = Conv3D(n_filters, filter_size, activation='relu',padding='valid', kernel_initializer=kernel_init,  bias_initializer=bias_init)(conv)\n",
        "    print(conv.shape)\n",
        "\n",
        "    print(\"end_of_block\")\n",
        "    return conv"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The architecture will be composed of several ```DownConvBlock``` blocks followed by the same number of ```UpConvBlock``` blocks. "
      ],
      "metadata": {
        "id": "evWdWAkXbzfq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "fe-fUh99sgKg"
      },
      "outputs": [],
      "source": [
        "\n",
        "def UNet3DModel(input_size=(16, 34, 34, 3), n_filters=32, filter_size=3, n_classes=1):\n",
        "  kernel_init =  tf.keras.initializers.GlorotUniform(seed=0)\n",
        "  bias_init = tf.keras.initializers.Zeros()  \n",
        "  \n",
        "  inputs = Input(input_size)\n",
        "  print(\"Inputs\", inputs.shape)\n",
        "\n",
        "  cblock0 = DownConvBlock(inputs,     n_filters = n_filters    , filter_size = filter_size, max_pooling=False, special_padding=True)\n",
        "  print(\"CB0\", cblock0[0].shape)\n",
        "\n",
        "  cblock1 = DownConvBlock(cblock0[0],     n_filters = n_filters    , filter_size = filter_size, max_pooling=True, special_padding=False)\n",
        "  print(\"CB1\", cblock1[0].shape)\n",
        "\n",
        "  cblock2 = DownConvBlock(cblock1[0], n_filters = n_filters*2  , filter_size = filter_size, max_pooling=True, special_padding=False)\n",
        "  print(\"CB2\", cblock2[0].shape)\n",
        "    \n",
        "  cblock3 = DownConvBlock(cblock2[0], n_filters = n_filters*4  , filter_size = filter_size, max_pooling=True, special_padding=False)\n",
        "  print(\"CB3\", cblock3[0].shape)\n",
        "  \n",
        "  cblock4 = DownConvBlock(cblock3[0], n_filters = n_filters*8  , filter_size = filter_size, max_pooling=False, special_padding=False)\n",
        "  print(\"CB4\", cblock4[0].shape)\n",
        "\n",
        "\n",
        "  print(\"------------------\")\n",
        "\n",
        "  ublock7 = UpConvBlock(cblock4[0]   , cblock3[1],  n_filters = n_filters * 4, filter_size = filter_size)\n",
        "  print(\"UB7\", ublock7.shape)\n",
        "  \n",
        "  ublock8 = UpConvBlock(ublock7   , cblock2[1],  n_filters = n_filters * 2, filter_size = filter_size)\n",
        "  print(\"UB8\", ublock8.shape)\n",
        "  \n",
        "  ublock9 = UpConvBlock(ublock8   , cblock1[1],  n_filters = n_filters, filter_size = filter_size)\n",
        "  print(\"UB9\", ublock9.shape)\n",
        "\n",
        "  ublock9 = periodic_padding_flexible(ublock9, axis=(1,2,3),padding=(1,1,1))\n",
        "  \n",
        "  conv9 = Conv3D(n_filters, 3, activation='relu', padding='valid', kernel_initializer=kernel_init,  bias_initializer=bias_init)(ublock9)\n",
        "  print(\"C9\", conv9.shape)\n",
        "  \n",
        "  conv10 = Conv3D(n_classes, 1, padding='same', kernel_initializer=kernel_init,  bias_initializer=bias_init)(conv9)\n",
        "  print(\"C10\", conv10.shape)\n",
        "\n",
        "  model = tf.keras.Model(inputs=inputs, outputs=conv10)  \n",
        "\n",
        "  return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this application, we are concerned with the detection and accurate prediction of hotspots (areas with higher temperatures), which are significantly less common than the rest of the material at a lower temperature. To address this, we are using a custon loss function based on a weighted Mean Squared Error (MSE)."
      ],
      "metadata": {
        "id": "vUDnv4nLeTQH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "d2a5FVUYPHTB"
      },
      "outputs": [],
      "source": [
        "def custom_mse(y_true,y_pred):\n",
        "    w_hot = 5.0\n",
        "    w_cold = 1.0\n",
        "    cutoff = 1.8\n",
        "    weightmat = tf.cast(tf.where(tf.greater(y_true, cutoff), w_hot, w_cold),float)\n",
        "    loss = tf.cast(K.square(y_pred - y_true),float)\n",
        "    loss = loss*weightmat\n",
        "    loss = K.mean(loss)\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, this last cell calls the function to generate the model, pair it with an optimizer and compile the model object."
      ],
      "metadata": {
        "id": "-uvZjSSeekHu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "R3SJtgKps6tc",
        "outputId": "4bea6eb5-52c2-4b4c-9677-3de8422381d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs (None, 16, 34, 34, 3)\n",
            "(None, 16, 32, 32, 32)\n",
            "(None, 16, 32, 32, 32)\n",
            "end_of_block\n",
            "CB0 (None, 16, 32, 32, 32)\n",
            "(None, 16, 32, 32, 32)\n",
            "(None, 16, 32, 32, 32)\n",
            "end_of_block\n",
            "CB1 (None, 8, 16, 16, 32)\n",
            "(None, 8, 16, 16, 64)\n",
            "(None, 8, 16, 16, 64)\n",
            "end_of_block\n",
            "CB2 (None, 4, 8, 8, 64)\n",
            "(None, 4, 8, 8, 128)\n",
            "(None, 4, 8, 8, 128)\n",
            "end_of_block\n",
            "CB3 (None, 2, 4, 4, 128)\n",
            "(None, 2, 4, 4, 256)\n",
            "(None, 2, 4, 4, 256)\n",
            "end_of_block\n",
            "CB4 (None, 2, 4, 4, 256)\n",
            "------------------\n",
            "(None, 4, 8, 8, 128)\n",
            "(None, 4, 8, 8, 128)\n",
            "end_of_block\n",
            "UB7 (None, 4, 8, 8, 128)\n",
            "(None, 8, 16, 16, 64)\n",
            "(None, 8, 16, 16, 64)\n",
            "end_of_block\n",
            "UB8 (None, 8, 16, 16, 64)\n",
            "(None, 16, 32, 32, 32)\n",
            "(None, 16, 32, 32, 32)\n",
            "end_of_block\n",
            "UB9 (None, 16, 32, 32, 32)\n",
            "C9 (None, 16, 32, 32, 32)\n",
            "C10 (None, 16, 32, 32, 1)\n"
          ]
        }
      ],
      "source": [
        "model = UNet3DModel(input_size=(16, 34, 34, 3), n_filters=32, filter_size = 3, n_classes=1)\n",
        "optimizer = tf.optimizers.Adam(learning_rate = 0.0005)\n",
        "model.compile(loss=custom_mse, optimizer=optimizer, metrics=['mse'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the model\n",
        "\n",
        "This cell implements two techniques to prevent overfitting. The first one is a learning rate scheduler that decreases the learning rate after a fixed number of epochs. The second one is an early stopping criteria that monitors the validation loss to ensure the model continues to learn."
      ],
      "metadata": {
        "id": "BLFb04QIevl2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "baIEkfpntuOW",
        "outputId": "672c591f-4fb4-4215-cd13-b3be9c0829b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "2/2 [==============================] - 18s 3s/step - loss: 1.4036 - mse: 0.9416 - val_loss: 0.9612 - val_mse: 0.7003 - lr: 5.0000e-04\n",
            "Epoch 2/1000\n",
            "2/2 [==============================] - 2s 878ms/step - loss: 0.9527 - mse: 0.5757 - val_loss: 0.5626 - val_mse: 0.5288 - lr: 5.0000e-04\n",
            "Epoch 3/1000\n",
            "2/2 [==============================] - 2s 866ms/step - loss: 0.5153 - mse: 0.3723 - val_loss: 0.3528 - val_mse: 0.1869 - lr: 5.0000e-04\n",
            "Epoch 4/1000\n",
            "2/2 [==============================] - 2s 868ms/step - loss: 0.4893 - mse: 0.2156 - val_loss: 0.2463 - val_mse: 0.1089 - lr: 5.0000e-04\n",
            "Epoch 5/1000\n",
            "2/2 [==============================] - 1s 839ms/step - loss: 0.3480 - mse: 0.1388 - val_loss: 0.2966 - val_mse: 0.2283 - lr: 5.0000e-04\n",
            "Epoch 6/1000\n",
            "2/2 [==============================] - 2s 1s/step - loss: 0.3983 - mse: 0.2742 - val_loss: 0.2328 - val_mse: 0.1559 - lr: 5.0000e-04\n",
            "Epoch 7/1000\n",
            "2/2 [==============================] - 2s 1s/step - loss: 0.3088 - mse: 0.1527 - val_loss: 0.1935 - val_mse: 0.0823 - lr: 5.0000e-04\n",
            "Epoch 8/1000\n",
            "2/2 [==============================] - 2s 871ms/step - loss: 0.3025 - mse: 0.1134 - val_loss: 0.1589 - val_mse: 0.0702 - lr: 5.0000e-04\n",
            "Epoch 9/1000\n",
            "2/2 [==============================] - 1s 844ms/step - loss: 0.2582 - mse: 0.1340 - val_loss: 0.1601 - val_mse: 0.1044 - lr: 5.0000e-04\n",
            "Epoch 10/1000\n",
            "2/2 [==============================] - 2s 1s/step - loss: 0.2225 - mse: 0.1120 - val_loss: 0.1297 - val_mse: 0.0543 - lr: 5.0000e-04\n",
            "Epoch 11/1000\n",
            "2/2 [==============================] - 2s 1s/step - loss: 0.1861 - mse: 0.0744 - val_loss: 0.1115 - val_mse: 0.0793 - lr: 5.0000e-04\n",
            "Epoch 12/1000\n",
            "2/2 [==============================] - 2s 879ms/step - loss: 0.1574 - mse: 0.1039 - val_loss: 0.0894 - val_mse: 0.0428 - lr: 5.0000e-04\n",
            "Epoch 13/1000\n",
            "2/2 [==============================] - 2s 882ms/step - loss: 0.1282 - mse: 0.0607 - val_loss: 0.0706 - val_mse: 0.0453 - lr: 5.0000e-04\n",
            "Epoch 14/1000\n",
            "2/2 [==============================] - 2s 848ms/step - loss: 0.1122 - mse: 0.0797 - val_loss: 0.0798 - val_mse: 0.0527 - lr: 5.0000e-04\n",
            "Epoch 15/1000\n",
            "2/2 [==============================] - 2s 852ms/step - loss: 0.1108 - mse: 0.0707 - val_loss: 0.0788 - val_mse: 0.0560 - lr: 5.0000e-04\n",
            "Epoch 16/1000\n",
            "2/2 [==============================] - 2s 852ms/step - loss: 0.1124 - mse: 0.0845 - val_loss: 0.0827 - val_mse: 0.0577 - lr: 5.0000e-04\n",
            "Epoch 17/1000\n",
            "2/2 [==============================] - 2s 848ms/step - loss: 0.1072 - mse: 0.0693 - val_loss: 0.0701 - val_mse: 0.0486 - lr: 5.0000e-04\n",
            "Epoch 18/1000\n",
            "2/2 [==============================] - 2s 878ms/step - loss: 0.0998 - mse: 0.0730 - val_loss: 0.0628 - val_mse: 0.0425 - lr: 5.0000e-04\n",
            "Epoch 19/1000\n",
            "2/2 [==============================] - 2s 847ms/step - loss: 0.0928 - mse: 0.0574 - val_loss: 0.0650 - val_mse: 0.0387 - lr: 5.0000e-04\n",
            "Epoch 20/1000\n",
            "2/2 [==============================] - 2s 885ms/step - loss: 0.0910 - mse: 0.0577 - val_loss: 0.0604 - val_mse: 0.0430 - lr: 5.0000e-04\n",
            "Epoch 21/1000\n",
            "2/2 [==============================] - 2s 857ms/step - loss: 0.0896 - mse: 0.0567 - val_loss: 0.0632 - val_mse: 0.0356 - lr: 5.0000e-04\n",
            "Epoch 22/1000\n",
            "2/2 [==============================] - 2s 886ms/step - loss: 0.0877 - mse: 0.0479 - val_loss: 0.0579 - val_mse: 0.0418 - lr: 5.0000e-04\n",
            "Epoch 23/1000\n",
            "2/2 [==============================] - 2s 893ms/step - loss: 0.0851 - mse: 0.0595 - val_loss: 0.0570 - val_mse: 0.0362 - lr: 5.0000e-04\n",
            "Epoch 24/1000\n",
            "2/2 [==============================] - 2s 891ms/step - loss: 0.0809 - mse: 0.0482 - val_loss: 0.0549 - val_mse: 0.0380 - lr: 5.0000e-04\n",
            "Epoch 25/1000\n",
            "2/2 [==============================] - 2s 863ms/step - loss: 0.0816 - mse: 0.0579 - val_loss: 0.0553 - val_mse: 0.0388 - lr: 5.0000e-04\n",
            "Epoch 26/1000\n",
            "2/2 [==============================] - 2s 860ms/step - loss: 0.0786 - mse: 0.0504 - val_loss: 0.0561 - val_mse: 0.0384 - lr: 5.0000e-04\n",
            "Epoch 27/1000\n",
            "2/2 [==============================] - 2s 873ms/step - loss: 0.0768 - mse: 0.0526 - val_loss: 0.0509 - val_mse: 0.0370 - lr: 5.0000e-04\n",
            "Epoch 28/1000\n",
            "2/2 [==============================] - 2s 863ms/step - loss: 0.0753 - mse: 0.0499 - val_loss: 0.0524 - val_mse: 0.0333 - lr: 5.0000e-04\n",
            "Epoch 29/1000\n",
            "2/2 [==============================] - 2s 890ms/step - loss: 0.0745 - mse: 0.0478 - val_loss: 0.0481 - val_mse: 0.0335 - lr: 5.0000e-04\n",
            "Epoch 30/1000\n",
            "2/2 [==============================] - 2s 862ms/step - loss: 0.0721 - mse: 0.0472 - val_loss: 0.0501 - val_mse: 0.0315 - lr: 5.0000e-04\n",
            "Epoch 31/1000\n",
            "2/2 [==============================] - 2s 895ms/step - loss: 0.0711 - mse: 0.0440 - val_loss: 0.0468 - val_mse: 0.0330 - lr: 5.0000e-04\n",
            "Epoch 32/1000\n",
            "2/2 [==============================] - 2s 868ms/step - loss: 0.0695 - mse: 0.0473 - val_loss: 0.0492 - val_mse: 0.0325 - lr: 5.0000e-04\n",
            "Epoch 33/1000\n",
            "2/2 [==============================] - 2s 899ms/step - loss: 0.0693 - mse: 0.0435 - val_loss: 0.0459 - val_mse: 0.0330 - lr: 5.0000e-04\n",
            "Epoch 34/1000\n",
            "2/2 [==============================] - 2s 883ms/step - loss: 0.0685 - mse: 0.0478 - val_loss: 0.0473 - val_mse: 0.0309 - lr: 5.0000e-04\n",
            "Epoch 35/1000\n",
            "2/2 [==============================] - 2s 897ms/step - loss: 0.0675 - mse: 0.0415 - val_loss: 0.0443 - val_mse: 0.0305 - lr: 5.0000e-04\n",
            "Epoch 36/1000\n",
            "2/2 [==============================] - 2s 875ms/step - loss: 0.0668 - mse: 0.0451 - val_loss: 0.0443 - val_mse: 0.0292 - lr: 5.0000e-04\n",
            "Epoch 37/1000\n",
            "2/2 [==============================] - 2s 899ms/step - loss: 0.0654 - mse: 0.0409 - val_loss: 0.0435 - val_mse: 0.0299 - lr: 5.0000e-04\n",
            "Epoch 38/1000\n",
            "2/2 [==============================] - 2s 1s/step - loss: 0.0643 - mse: 0.0433 - val_loss: 0.0440 - val_mse: 0.0301 - lr: 5.0000e-04\n",
            "Epoch 39/1000\n",
            "2/2 [==============================] - 2s 912ms/step - loss: 0.0641 - mse: 0.0409 - val_loss: 0.0418 - val_mse: 0.0294 - lr: 5.0000e-04\n",
            "Epoch 40/1000\n",
            "2/2 [==============================] - 2s 888ms/step - loss: 0.0632 - mse: 0.0432 - val_loss: 0.0434 - val_mse: 0.0278 - lr: 5.0000e-04\n",
            "Epoch 41/1000\n",
            "2/2 [==============================] - 2s 906ms/step - loss: 0.0625 - mse: 0.0388 - val_loss: 0.0410 - val_mse: 0.0290 - lr: 5.0000e-04\n",
            "Epoch 42/1000\n",
            "2/2 [==============================] - 2s 880ms/step - loss: 0.0612 - mse: 0.0416 - val_loss: 0.0443 - val_mse: 0.0287 - lr: 5.0000e-04\n",
            "Epoch 43/1000\n",
            "2/2 [==============================] - 2s 877ms/step - loss: 0.0611 - mse: 0.0380 - val_loss: 0.0477 - val_mse: 0.0375 - lr: 5.0000e-04\n",
            "Epoch 44/1000\n",
            "2/2 [==============================] - 2s 873ms/step - loss: 0.0731 - mse: 0.0508 - val_loss: 0.0510 - val_mse: 0.0316 - lr: 5.0000e-04\n",
            "Epoch 45/1000\n",
            "2/2 [==============================] - 2s 869ms/step - loss: 0.0708 - mse: 0.0473 - val_loss: 0.0408 - val_mse: 0.0303 - lr: 5.0000e-04\n",
            "Epoch 46/1000\n",
            "2/2 [==============================] - 2s 870ms/step - loss: 0.0647 - mse: 0.0422 - val_loss: 0.0486 - val_mse: 0.0300 - lr: 5.0000e-04\n",
            "Epoch 47/1000\n",
            "2/2 [==============================] - 2s 871ms/step - loss: 0.0622 - mse: 0.0385 - val_loss: 0.0438 - val_mse: 0.0342 - lr: 5.0000e-04\n",
            "Epoch 48/1000\n",
            "2/2 [==============================] - 2s 873ms/step - loss: 0.0652 - mse: 0.0461 - val_loss: 0.0479 - val_mse: 0.0300 - lr: 5.0000e-04\n",
            "Epoch 49/1000\n",
            "2/2 [==============================] - 2s 897ms/step - loss: 0.0623 - mse: 0.0382 - val_loss: 0.0404 - val_mse: 0.0305 - lr: 5.0000e-04\n",
            "Epoch 50/1000\n",
            "2/2 [==============================] - 2s 875ms/step - loss: 0.0606 - mse: 0.0429 - val_loss: 0.0430 - val_mse: 0.0272 - lr: 5.0000e-04\n",
            "Epoch 51/1000\n",
            "2/2 [==============================] - 2s 899ms/step - loss: 0.0606 - mse: 0.0367 - val_loss: 0.0384 - val_mse: 0.0281 - lr: 5.0000e-04\n",
            "Epoch 52/1000\n",
            "2/2 [==============================] - 2s 870ms/step - loss: 0.0606 - mse: 0.0440 - val_loss: 0.0407 - val_mse: 0.0260 - lr: 5.0000e-04\n",
            "Epoch 53/1000\n",
            "2/2 [==============================] - 2s 893ms/step - loss: 0.0597 - mse: 0.0357 - val_loss: 0.0370 - val_mse: 0.0250 - lr: 5.0000e-04\n",
            "Epoch 54/1000\n",
            "2/2 [==============================] - 2s 902ms/step - loss: 0.0569 - mse: 0.0397 - val_loss: 0.0365 - val_mse: 0.0251 - lr: 5.0000e-04\n",
            "Epoch 55/1000\n",
            "2/2 [==============================] - 2s 862ms/step - loss: 0.0574 - mse: 0.0371 - val_loss: 0.0379 - val_mse: 0.0250 - lr: 5.0000e-04\n",
            "Epoch 56/1000\n",
            "2/2 [==============================] - 2s 863ms/step - loss: 0.0547 - mse: 0.0370 - val_loss: 0.0363 - val_mse: 0.0259 - lr: 5.0000e-04\n",
            "Epoch 57/1000\n",
            "2/2 [==============================] - 2s 873ms/step - loss: 0.0543 - mse: 0.0364 - val_loss: 0.0395 - val_mse: 0.0251 - lr: 5.0000e-04\n",
            "Epoch 58/1000\n",
            "2/2 [==============================] - 2s 896ms/step - loss: 0.0543 - mse: 0.0354 - val_loss: 0.0355 - val_mse: 0.0253 - lr: 5.0000e-04\n",
            "Epoch 59/1000\n",
            "2/2 [==============================] - 2s 861ms/step - loss: 0.0531 - mse: 0.0360 - val_loss: 0.0382 - val_mse: 0.0254 - lr: 5.0000e-04\n",
            "Epoch 60/1000\n",
            "2/2 [==============================] - 2s 895ms/step - loss: 0.0523 - mse: 0.0352 - val_loss: 0.0349 - val_mse: 0.0250 - lr: 5.0000e-04\n",
            "Epoch 61/1000\n",
            "2/2 [==============================] - 2s 868ms/step - loss: 0.0529 - mse: 0.0362 - val_loss: 0.0359 - val_mse: 0.0235 - lr: 5.0000e-04\n",
            "Epoch 62/1000\n",
            "2/2 [==============================] - 2s 857ms/step - loss: 0.0520 - mse: 0.0356 - val_loss: 0.0350 - val_mse: 0.0234 - lr: 5.0000e-04\n",
            "Epoch 63/1000\n",
            "2/2 [==============================] - 2s 992ms/step - loss: 0.0506 - mse: 0.0329 - val_loss: 0.0347 - val_mse: 0.0238 - lr: 5.0000e-04\n",
            "Epoch 64/1000\n",
            "2/2 [==============================] - 2s 869ms/step - loss: 0.0516 - mse: 0.0367 - val_loss: 0.0380 - val_mse: 0.0247 - lr: 5.0000e-04\n",
            "Epoch 65/1000\n",
            "2/2 [==============================] - 2s 862ms/step - loss: 0.0512 - mse: 0.0325 - val_loss: 0.0345 - val_mse: 0.0251 - lr: 5.0000e-04\n",
            "Epoch 66/1000\n",
            "2/2 [==============================] - 2s 859ms/step - loss: 0.0507 - mse: 0.0364 - val_loss: 0.0381 - val_mse: 0.0245 - lr: 5.0000e-04\n",
            "Epoch 67/1000\n",
            "2/2 [==============================] - 2s 891ms/step - loss: 0.0498 - mse: 0.0322 - val_loss: 0.0335 - val_mse: 0.0244 - lr: 5.0000e-04\n",
            "Epoch 68/1000\n",
            "2/2 [==============================] - 2s 857ms/step - loss: 0.0494 - mse: 0.0352 - val_loss: 0.0375 - val_mse: 0.0239 - lr: 5.0000e-04\n",
            "Epoch 69/1000\n",
            "2/2 [==============================] - 2s 863ms/step - loss: 0.0483 - mse: 0.0312 - val_loss: 0.0338 - val_mse: 0.0247 - lr: 5.0000e-04\n",
            "Epoch 70/1000\n",
            "2/2 [==============================] - 2s 864ms/step - loss: 0.0497 - mse: 0.0346 - val_loss: 0.0355 - val_mse: 0.0236 - lr: 5.0000e-04\n",
            "Epoch 71/1000\n",
            "2/2 [==============================] - 2s 1s/step - loss: 0.0469 - mse: 0.0323 - val_loss: 0.0325 - val_mse: 0.0227 - lr: 5.0000e-04\n",
            "Epoch 72/1000\n",
            "2/2 [==============================] - 2s 861ms/step - loss: 0.0461 - mse: 0.0319 - val_loss: 0.0372 - val_mse: 0.0235 - lr: 5.0000e-04\n",
            "Epoch 73/1000\n",
            "2/2 [==============================] - 2s 866ms/step - loss: 0.0480 - mse: 0.0325 - val_loss: 0.0327 - val_mse: 0.0222 - lr: 5.0000e-04\n",
            "Epoch 74/1000\n",
            "2/2 [==============================] - 2s 852ms/step - loss: 0.0451 - mse: 0.0310 - val_loss: 0.0352 - val_mse: 0.0239 - lr: 5.0000e-04\n",
            "Epoch 75/1000\n",
            "2/2 [==============================] - 2s 867ms/step - loss: 0.0451 - mse: 0.0313 - val_loss: 0.0325 - val_mse: 0.0221 - lr: 5.0000e-04\n",
            "Epoch 76/1000\n",
            "2/2 [==============================] - 2s 860ms/step - loss: 0.0446 - mse: 0.0309 - val_loss: 0.0361 - val_mse: 0.0232 - lr: 5.0000e-04\n",
            "Epoch 77/1000\n",
            "2/2 [==============================] - 2s 867ms/step - loss: 0.0453 - mse: 0.0311 - val_loss: 0.0339 - val_mse: 0.0229 - lr: 5.0000e-04\n",
            "Epoch 78/1000\n",
            "2/2 [==============================] - 2s 894ms/step - loss: 0.0438 - mse: 0.0301 - val_loss: 0.0315 - val_mse: 0.0222 - lr: 5.0000e-04\n",
            "Epoch 79/1000\n",
            "2/2 [==============================] - 2s 859ms/step - loss: 0.0437 - mse: 0.0309 - val_loss: 0.0372 - val_mse: 0.0235 - lr: 5.0000e-04\n",
            "Epoch 80/1000\n",
            "2/2 [==============================] - 2s 875ms/step - loss: 0.0436 - mse: 0.0291 - val_loss: 0.0314 - val_mse: 0.0224 - lr: 5.0000e-04\n",
            "Epoch 81/1000\n",
            "2/2 [==============================] - 2s 910ms/step - loss: 0.0456 - mse: 0.0316 - val_loss: 0.0309 - val_mse: 0.0220 - lr: 5.0000e-04\n",
            "Epoch 82/1000\n",
            "2/2 [==============================] - 2s 869ms/step - loss: 0.0430 - mse: 0.0307 - val_loss: 0.0330 - val_mse: 0.0216 - lr: 5.0000e-04\n",
            "Epoch 83/1000\n",
            "2/2 [==============================] - 2s 862ms/step - loss: 0.0423 - mse: 0.0295 - val_loss: 0.0348 - val_mse: 0.0230 - lr: 5.0000e-04\n",
            "Epoch 84/1000\n",
            "2/2 [==============================] - 2s 864ms/step - loss: 0.0419 - mse: 0.0283 - val_loss: 0.0314 - val_mse: 0.0236 - lr: 5.0000e-04\n",
            "Epoch 85/1000\n",
            "2/2 [==============================] - 2s 862ms/step - loss: 0.0444 - mse: 0.0311 - val_loss: 0.0350 - val_mse: 0.0224 - lr: 5.0000e-04\n",
            "Epoch 86/1000\n",
            "2/2 [==============================] - 2s 864ms/step - loss: 0.0436 - mse: 0.0308 - val_loss: 0.0351 - val_mse: 0.0232 - lr: 5.0000e-04\n",
            "Epoch 87/1000\n",
            "2/2 [==============================] - 2s 862ms/step - loss: 0.0414 - mse: 0.0275 - val_loss: 0.0306 - val_mse: 0.0227 - lr: 5.0000e-04\n",
            "Epoch 88/1000\n",
            "2/2 [==============================] - 2s 862ms/step - loss: 0.0410 - mse: 0.0299 - val_loss: 0.0386 - val_mse: 0.0244 - lr: 5.0000e-04\n",
            "Epoch 89/1000\n",
            "2/2 [==============================] - 2s 868ms/step - loss: 0.0427 - mse: 0.0295 - val_loss: 0.0317 - val_mse: 0.0221 - lr: 5.0000e-04\n",
            "Epoch 90/1000\n",
            "2/2 [==============================] - 2s 999ms/step - loss: 0.0416 - mse: 0.0290 - val_loss: 0.0310 - val_mse: 0.0225 - lr: 5.0000e-04\n",
            "Epoch 91/1000\n",
            "2/2 [==============================] - 2s 874ms/step - loss: 0.0387 - mse: 0.0288 - val_loss: 0.0358 - val_mse: 0.0227 - lr: 5.0000e-04\n",
            "Epoch 92/1000\n",
            "2/2 [==============================] - 2s 863ms/step - loss: 0.0389 - mse: 0.0267 - val_loss: 0.0316 - val_mse: 0.0218 - lr: 5.0000e-04\n",
            "Epoch 93/1000\n",
            "2/2 [==============================] - 2s 864ms/step - loss: 0.0369 - mse: 0.0266 - val_loss: 0.0346 - val_mse: 0.0240 - lr: 5.0000e-04\n",
            "Epoch 94/1000\n",
            "2/2 [==============================] - 2s 997ms/step - loss: 0.0364 - mse: 0.0265 - val_loss: 0.0340 - val_mse: 0.0227 - lr: 5.0000e-04\n",
            "Epoch 95/1000\n",
            "2/2 [==============================] - 2s 876ms/step - loss: 0.0359 - mse: 0.0252 - val_loss: 0.0312 - val_mse: 0.0219 - lr: 5.0000e-04\n",
            "Epoch 96/1000\n",
            "2/2 [==============================] - 2s 874ms/step - loss: 0.0356 - mse: 0.0259 - val_loss: 0.0339 - val_mse: 0.0229 - lr: 5.0000e-04\n",
            "Epoch 97/1000\n",
            "2/2 [==============================] - 2s 870ms/step - loss: 0.0355 - mse: 0.0262 - val_loss: 0.0417 - val_mse: 0.0267 - lr: 5.0000e-04\n",
            "Epoch 98/1000\n",
            "2/2 [==============================] - 2s 870ms/step - loss: 0.0364 - mse: 0.0244 - val_loss: 0.0314 - val_mse: 0.0239 - lr: 5.0000e-04\n",
            "Epoch 99/1000\n",
            "2/2 [==============================] - 2s 871ms/step - loss: 0.0417 - mse: 0.0286 - val_loss: 0.0351 - val_mse: 0.0220 - lr: 5.0000e-04\n",
            "Epoch 100/1000\n",
            "2/2 [==============================] - 2s 862ms/step - loss: 0.0405 - mse: 0.0301 - val_loss: 0.0351 - val_mse: 0.0237 - lr: 5.0000e-04\n",
            "Epoch 101/1000\n",
            "2/2 [==============================] - 2s 861ms/step - loss: 0.0366 - mse: 0.0257 - val_loss: 0.0315 - val_mse: 0.0233 - lr: 5.0000e-04\n",
            "Epoch 102/1000\n",
            "2/2 [==============================] - 2s 864ms/step - loss: 0.0348 - mse: 0.0261 - val_loss: 0.0367 - val_mse: 0.0226 - lr: 5.0000e-04\n",
            "Epoch 103/1000\n",
            "2/2 [==============================] - 2s 867ms/step - loss: 0.0348 - mse: 0.0238 - val_loss: 0.0306 - val_mse: 0.0218 - lr: 5.0000e-04\n",
            "Epoch 104/1000\n",
            "2/2 [==============================] - 2s 862ms/step - loss: 0.0351 - mse: 0.0261 - val_loss: 0.0330 - val_mse: 0.0234 - lr: 5.0000e-04\n",
            "Epoch 105/1000\n",
            "2/2 [==============================] - 2s 873ms/step - loss: 0.0361 - mse: 0.0283 - val_loss: 0.0468 - val_mse: 0.0298 - lr: 5.0000e-04\n",
            "Epoch 106/1000\n",
            "2/2 [==============================] - 2s 993ms/step - loss: 0.0390 - mse: 0.0249 - val_loss: 0.0365 - val_mse: 0.0294 - lr: 5.0000e-04\n",
            "Epoch 107/1000\n",
            "2/2 [==============================] - 2s 870ms/step - loss: 0.0391 - mse: 0.0302 - val_loss: 0.0502 - val_mse: 0.0323 - lr: 5.0000e-04\n",
            "Epoch 108/1000\n",
            "2/2 [==============================] - 2s 862ms/step - loss: 0.0381 - mse: 0.0257 - val_loss: 0.0344 - val_mse: 0.0272 - lr: 5.0000e-04\n",
            "Epoch 109/1000\n",
            "2/2 [==============================] - 2s 872ms/step - loss: 0.0432 - mse: 0.0315 - val_loss: 0.0429 - val_mse: 0.0264 - lr: 5.0000e-04\n",
            "Epoch 110/1000\n",
            "2/2 [==============================] - 2s 875ms/step - loss: 0.0444 - mse: 0.0319 - val_loss: 0.0311 - val_mse: 0.0216 - lr: 5.0000e-04\n",
            "Epoch 111/1000\n",
            "2/2 [==============================] - 2s 872ms/step - loss: 0.0330 - mse: 0.0233 - val_loss: 0.0374 - val_mse: 0.0256 - lr: 5.0000e-04\n",
            "Epoch 112/1000\n",
            "2/2 [==============================] - 2s 864ms/step - loss: 0.0339 - mse: 0.0256 - val_loss: 0.0318 - val_mse: 0.0215 - lr: 5.0000e-04\n",
            "Epoch 113/1000\n",
            "2/2 [==============================] - 2s 862ms/step - loss: 0.0312 - mse: 0.0219 - val_loss: 0.0349 - val_mse: 0.0225 - lr: 5.0000e-04\n",
            "Epoch 114/1000\n",
            "2/2 [==============================] - 2s 865ms/step - loss: 0.0312 - mse: 0.0236 - val_loss: 0.0349 - val_mse: 0.0234 - lr: 5.0000e-04\n",
            "Epoch 115/1000\n",
            "2/2 [==============================] - 2s 863ms/step - loss: 0.0312 - mse: 0.0223 - val_loss: 0.0307 - val_mse: 0.0219 - lr: 5.0000e-04\n",
            "Epoch 116/1000\n",
            "2/2 [==============================] - 2s 862ms/step - loss: 0.0299 - mse: 0.0233 - val_loss: 0.0399 - val_mse: 0.0257 - lr: 5.0000e-04\n",
            "Epoch 117/1000\n",
            "2/2 [==============================] - 2s 859ms/step - loss: 0.0297 - mse: 0.0202 - val_loss: 0.0321 - val_mse: 0.0236 - lr: 5.0000e-04\n",
            "Epoch 118/1000\n",
            "2/2 [==============================] - 2s 859ms/step - loss: 0.0294 - mse: 0.0229 - val_loss: 0.0425 - val_mse: 0.0277 - lr: 5.0000e-04\n",
            "Epoch 119/1000\n",
            "2/2 [==============================] - 2s 867ms/step - loss: 0.0282 - mse: 0.0198 - val_loss: 0.0310 - val_mse: 0.0225 - lr: 5.0000e-04\n",
            "Epoch 120/1000\n",
            "2/2 [==============================] - 2s 863ms/step - loss: 0.0282 - mse: 0.0218 - val_loss: 0.0396 - val_mse: 0.0258 - lr: 5.0000e-04\n",
            "Epoch 121/1000\n",
            "2/2 [==============================] - 2s 995ms/step - loss: 0.0271 - mse: 0.0196 - val_loss: 0.0329 - val_mse: 0.0232 - lr: 5.0000e-04\n",
            "Epoch 122/1000\n",
            "2/2 [==============================] - 2s 874ms/step - loss: 0.0261 - mse: 0.0191 - val_loss: 0.0371 - val_mse: 0.0248 - lr: 5.0000e-04\n",
            "Epoch 123/1000\n",
            "2/2 [==============================] - 2s 861ms/step - loss: 0.0260 - mse: 0.0197 - val_loss: 0.0359 - val_mse: 0.0242 - lr: 5.0000e-04\n",
            "Epoch 124/1000\n",
            "2/2 [==============================] - 2s 862ms/step - loss: 0.0240 - mse: 0.0174 - val_loss: 0.0337 - val_mse: 0.0228 - lr: 5.0000e-04\n",
            "Epoch 125/1000\n",
            "2/2 [==============================] - 2s 863ms/step - loss: 0.0231 - mse: 0.0179 - val_loss: 0.0391 - val_mse: 0.0262 - lr: 5.0000e-04\n",
            "Epoch 126/1000\n",
            "2/2 [==============================] - 2s 861ms/step - loss: 0.0226 - mse: 0.0165 - val_loss: 0.0330 - val_mse: 0.0231 - lr: 5.0000e-04\n",
            "Epoch 127/1000\n",
            "2/2 [==============================] - 2s 862ms/step - loss: 0.0229 - mse: 0.0178 - val_loss: 0.0401 - val_mse: 0.0267 - lr: 5.0000e-04\n",
            "Epoch 128/1000\n",
            "2/2 [==============================] - 2s 864ms/step - loss: 0.0216 - mse: 0.0161 - val_loss: 0.0347 - val_mse: 0.0240 - lr: 5.0000e-04\n",
            "Epoch 129/1000\n",
            "2/2 [==============================] - 2s 862ms/step - loss: 0.0218 - mse: 0.0164 - val_loss: 0.0354 - val_mse: 0.0238 - lr: 5.0000e-04\n",
            "Epoch 130/1000\n",
            "2/2 [==============================] - 2s 865ms/step - loss: 0.0208 - mse: 0.0159 - val_loss: 0.0400 - val_mse: 0.0268 - lr: 5.0000e-04\n",
            "Epoch 131/1000\n",
            "2/2 [==============================] - 2s 864ms/step - loss: 0.0204 - mse: 0.0155 - val_loss: 0.0376 - val_mse: 0.0260 - lr: 5.0000e-04\n",
            "Epoch 132/1000\n",
            "2/2 [==============================] - 2s 863ms/step - loss: 0.0193 - mse: 0.0146 - val_loss: 0.0370 - val_mse: 0.0246 - lr: 5.0000e-04\n",
            "Epoch 133/1000\n",
            "2/2 [==============================] - 2s 865ms/step - loss: 0.0191 - mse: 0.0146 - val_loss: 0.0415 - val_mse: 0.0281 - lr: 5.0000e-04\n",
            "Epoch 134/1000\n",
            "2/2 [==============================] - 2s 885ms/step - loss: 0.0189 - mse: 0.0144 - val_loss: 0.0371 - val_mse: 0.0250 - lr: 5.0000e-04\n",
            "Epoch 135/1000\n",
            "2/2 [==============================] - 2s 864ms/step - loss: 0.0184 - mse: 0.0139 - val_loss: 0.0367 - val_mse: 0.0252 - lr: 5.0000e-04\n",
            "Epoch 136/1000\n",
            "2/2 [==============================] - 2s 864ms/step - loss: 0.0181 - mse: 0.0140 - val_loss: 0.0385 - val_mse: 0.0259 - lr: 5.0000e-04\n",
            "Epoch 137/1000\n",
            "2/2 [==============================] - 2s 863ms/step - loss: 0.0174 - mse: 0.0134 - val_loss: 0.0421 - val_mse: 0.0285 - lr: 5.0000e-04\n",
            "Epoch 138/1000\n",
            "2/2 [==============================] - 2s 861ms/step - loss: 0.0175 - mse: 0.0135 - val_loss: 0.0395 - val_mse: 0.0261 - lr: 5.0000e-04\n",
            "Epoch 139/1000\n",
            "2/2 [==============================] - 2s 863ms/step - loss: 0.0167 - mse: 0.0127 - val_loss: 0.0369 - val_mse: 0.0257 - lr: 5.0000e-04\n",
            "Epoch 140/1000\n",
            "2/2 [==============================] - 2s 862ms/step - loss: 0.0180 - mse: 0.0137 - val_loss: 0.0356 - val_mse: 0.0243 - lr: 5.0000e-04\n",
            "Epoch 141/1000\n",
            "2/2 [==============================] - 2s 862ms/step - loss: 0.0165 - mse: 0.0129 - val_loss: 0.0433 - val_mse: 0.0285 - lr: 5.0000e-04\n",
            "Epoch 142/1000\n",
            "2/2 [==============================] - 2s 861ms/step - loss: 0.0167 - mse: 0.0128 - val_loss: 0.0395 - val_mse: 0.0264 - lr: 5.0000e-04\n",
            "Epoch 143/1000\n",
            "2/2 [==============================] - 2s 866ms/step - loss: 0.0156 - mse: 0.0118 - val_loss: 0.0374 - val_mse: 0.0258 - lr: 5.0000e-04\n",
            "Epoch 144/1000\n",
            "2/2 [==============================] - 2s 863ms/step - loss: 0.0156 - mse: 0.0120 - val_loss: 0.0386 - val_mse: 0.0259 - lr: 5.0000e-04\n",
            "Epoch 145/1000\n",
            "2/2 [==============================] - 2s 867ms/step - loss: 0.0151 - mse: 0.0119 - val_loss: 0.0429 - val_mse: 0.0281 - lr: 5.0000e-04\n",
            "Epoch 146/1000\n",
            "2/2 [==============================] - 2s 860ms/step - loss: 0.0153 - mse: 0.0118 - val_loss: 0.0408 - val_mse: 0.0284 - lr: 5.0000e-04\n",
            "Epoch 147/1000\n",
            "2/2 [==============================] - 2s 862ms/step - loss: 0.0162 - mse: 0.0127 - val_loss: 0.0423 - val_mse: 0.0285 - lr: 5.0000e-04\n",
            "Epoch 148/1000\n",
            "2/2 [==============================] - 2s 861ms/step - loss: 0.0149 - mse: 0.0117 - val_loss: 0.0384 - val_mse: 0.0257 - lr: 5.0000e-04\n",
            "Epoch 149/1000\n",
            "2/2 [==============================] - 2s 866ms/step - loss: 0.0147 - mse: 0.0112 - val_loss: 0.0378 - val_mse: 0.0263 - lr: 5.0000e-04\n",
            "Epoch 150/1000\n",
            "2/2 [==============================] - 2s 866ms/step - loss: 0.0143 - mse: 0.0113 - val_loss: 0.0400 - val_mse: 0.0266 - lr: 5.0000e-04\n",
            "Epoch 151/1000\n",
            "2/2 [==============================] - 2s 862ms/step - loss: 0.0138 - mse: 0.0108 - val_loss: 0.0407 - val_mse: 0.0278 - lr: 5.0000e-04\n",
            "Epoch 152/1000\n",
            "2/2 [==============================] - 2s 863ms/step - loss: 0.0135 - mse: 0.0106 - val_loss: 0.0387 - val_mse: 0.0260 - lr: 5.0000e-04\n",
            "Epoch 153/1000\n",
            "2/2 [==============================] - 2s 867ms/step - loss: 0.0137 - mse: 0.0109 - val_loss: 0.0372 - val_mse: 0.0255 - lr: 5.0000e-04\n",
            "Epoch 154/1000\n",
            "2/2 [==============================] - 2s 997ms/step - loss: 0.0144 - mse: 0.0112 - val_loss: 0.0368 - val_mse: 0.0257 - lr: 5.0000e-04\n",
            "Epoch 155/1000\n",
            "2/2 [==============================] - 2s 877ms/step - loss: 0.0140 - mse: 0.0108 - val_loss: 0.0401 - val_mse: 0.0269 - lr: 5.0000e-04\n",
            "Epoch 156/1000\n",
            "2/2 [==============================] - 2s 866ms/step - loss: 0.0131 - mse: 0.0105 - val_loss: 0.0432 - val_mse: 0.0287 - lr: 5.0000e-04\n",
            "Epoch 157/1000\n",
            "2/2 [==============================] - 2s 877ms/step - loss: 0.0136 - mse: 0.0107 - val_loss: 0.0418 - val_mse: 0.0285 - lr: 5.0000e-04\n",
            "Epoch 158/1000\n",
            "2/2 [==============================] - 2s 867ms/step - loss: 0.0129 - mse: 0.0103 - val_loss: 0.0389 - val_mse: 0.0257 - lr: 5.0000e-04\n",
            "Epoch 159/1000\n",
            "2/2 [==============================] - 2s 864ms/step - loss: 0.0128 - mse: 0.0101 - val_loss: 0.0376 - val_mse: 0.0259 - lr: 5.0000e-04\n",
            "Epoch 160/1000\n",
            "2/2 [==============================] - 2s 861ms/step - loss: 0.0130 - mse: 0.0102 - val_loss: 0.0392 - val_mse: 0.0268 - lr: 5.0000e-04\n",
            "Epoch 161/1000\n",
            "2/2 [==============================] - 2s 862ms/step - loss: 0.0122 - mse: 0.0098 - val_loss: 0.0410 - val_mse: 0.0273 - lr: 5.0000e-04\n",
            "Epoch 162/1000\n",
            "2/2 [==============================] - 2s 861ms/step - loss: 0.0122 - mse: 0.0097 - val_loss: 0.0438 - val_mse: 0.0295 - lr: 5.0000e-04\n",
            "Epoch 163/1000\n",
            "2/2 [==============================] - 2s 864ms/step - loss: 0.0124 - mse: 0.0099 - val_loss: 0.0411 - val_mse: 0.0274 - lr: 5.0000e-04\n",
            "Epoch 164/1000\n",
            "2/2 [==============================] - 2s 862ms/step - loss: 0.0119 - mse: 0.0095 - val_loss: 0.0404 - val_mse: 0.0267 - lr: 5.0000e-04\n",
            "Epoch 165/1000\n",
            "2/2 [==============================] - 2s 867ms/step - loss: 0.0116 - mse: 0.0092 - val_loss: 0.0394 - val_mse: 0.0271 - lr: 5.0000e-04\n",
            "Epoch 166/1000\n",
            "2/2 [==============================] - 2s 863ms/step - loss: 0.0116 - mse: 0.0092 - val_loss: 0.0381 - val_mse: 0.0259 - lr: 5.0000e-04\n",
            "Epoch 167/1000\n",
            "2/2 [==============================] - 2s 862ms/step - loss: 0.0118 - mse: 0.0096 - val_loss: 0.0382 - val_mse: 0.0258 - lr: 5.0000e-04\n",
            "Epoch 168/1000\n",
            "2/2 [==============================] - 2s 997ms/step - loss: 0.0113 - mse: 0.0091 - val_loss: 0.0415 - val_mse: 0.0283 - lr: 5.0000e-04\n",
            "Epoch 169/1000\n",
            "2/2 [==============================] - 2s 1s/step - loss: 0.0110 - mse: 0.0090 - val_loss: 0.0415 - val_mse: 0.0273 - lr: 5.0000e-04\n",
            "Epoch 170/1000\n",
            "2/2 [==============================] - 2s 880ms/step - loss: 0.0111 - mse: 0.0089 - val_loss: 0.0440 - val_mse: 0.0291 - lr: 5.0000e-04\n",
            "Epoch 171/1000\n",
            "2/2 [==============================] - 2s 865ms/step - loss: 0.0114 - mse: 0.0093 - val_loss: 0.0460 - val_mse: 0.0310 - lr: 5.0000e-04\n",
            "Epoch 172/1000\n",
            "2/2 [==============================] - 2s 861ms/step - loss: 0.0123 - mse: 0.0100 - val_loss: 0.0429 - val_mse: 0.0276 - lr: 5.0000e-04\n",
            "Epoch 173/1000\n",
            "2/2 [==============================] - 2s 864ms/step - loss: 0.0115 - mse: 0.0091 - val_loss: 0.0432 - val_mse: 0.0284 - lr: 5.0000e-04\n",
            "Epoch 174/1000\n",
            "2/2 [==============================] - 2s 861ms/step - loss: 0.0105 - mse: 0.0085 - val_loss: 0.0398 - val_mse: 0.0272 - lr: 5.0000e-04\n",
            "Epoch 175/1000\n",
            "2/2 [==============================] - 2s 864ms/step - loss: 0.0105 - mse: 0.0085 - val_loss: 0.0397 - val_mse: 0.0266 - lr: 5.0000e-04\n",
            "Epoch 176/1000\n",
            "2/2 [==============================] - 2s 862ms/step - loss: 0.0108 - mse: 0.0089 - val_loss: 0.0380 - val_mse: 0.0256 - lr: 5.0000e-04\n",
            "Epoch 177/1000\n",
            "2/2 [==============================] - 2s 863ms/step - loss: 0.0105 - mse: 0.0085 - val_loss: 0.0409 - val_mse: 0.0277 - lr: 5.0000e-04\n",
            "Epoch 178/1000\n",
            "2/2 [==============================] - 2s 863ms/step - loss: 0.0102 - mse: 0.0085 - val_loss: 0.0430 - val_mse: 0.0279 - lr: 5.0000e-04\n",
            "Epoch 179/1000\n",
            "2/2 [==============================] - 2s 864ms/step - loss: 0.0109 - mse: 0.0087 - val_loss: 0.0464 - val_mse: 0.0300 - lr: 5.0000e-04\n",
            "Epoch 180/1000\n",
            "2/2 [==============================] - 2s 877ms/step - loss: 0.0112 - mse: 0.0090 - val_loss: 0.0446 - val_mse: 0.0304 - lr: 5.0000e-04\n",
            "Epoch 181/1000\n",
            "2/2 [==============================] - 2s 1s/step - loss: 0.0107 - mse: 0.0089 - val_loss: 0.0413 - val_mse: 0.0270 - lr: 5.0000e-04\n",
            "Epoch 182/1000\n",
            "2/2 [==============================] - 2s 874ms/step - loss: 0.0101 - mse: 0.0082 - val_loss: 0.0379 - val_mse: 0.0259 - lr: 5.0000e-04\n",
            "Epoch 183/1000\n",
            "2/2 [==============================] - 2s 865ms/step - loss: 0.0113 - mse: 0.0087 - val_loss: 0.0380 - val_mse: 0.0261 - lr: 5.0000e-04\n",
            "Epoch 184/1000\n",
            "2/2 [==============================] - 2s 861ms/step - loss: 0.0104 - mse: 0.0086 - val_loss: 0.0423 - val_mse: 0.0271 - lr: 5.0000e-04\n",
            "Epoch 185/1000\n",
            "2/2 [==============================] - 2s 867ms/step - loss: 0.0105 - mse: 0.0084 - val_loss: 0.0465 - val_mse: 0.0319 - lr: 5.0000e-04\n",
            "Epoch 186/1000\n",
            "2/2 [==============================] - 2s 864ms/step - loss: 0.0110 - mse: 0.0092 - val_loss: 0.0400 - val_mse: 0.0259 - lr: 5.0000e-04\n",
            "Epoch 187/1000\n",
            "2/2 [==============================] - 2s 870ms/step - loss: 0.0100 - mse: 0.0081 - val_loss: 0.0374 - val_mse: 0.0260 - lr: 5.0000e-04\n",
            "Epoch 188/1000\n",
            "2/2 [==============================] - 2s 874ms/step - loss: 0.0112 - mse: 0.0086 - val_loss: 0.0391 - val_mse: 0.0265 - lr: 5.0000e-04\n",
            "Epoch 189/1000\n",
            "2/2 [==============================] - 2s 864ms/step - loss: 0.0097 - mse: 0.0081 - val_loss: 0.0409 - val_mse: 0.0270 - lr: 5.0000e-04\n",
            "Epoch 190/1000\n",
            "2/2 [==============================] - 2s 997ms/step - loss: 0.0096 - mse: 0.0079 - val_loss: 0.0433 - val_mse: 0.0291 - lr: 5.0000e-04\n",
            "Epoch 191/1000\n",
            "2/2 [==============================] - 2s 877ms/step - loss: 0.0096 - mse: 0.0080 - val_loss: 0.0410 - val_mse: 0.0268 - lr: 5.0000e-04\n",
            "Epoch 192/1000\n",
            "2/2 [==============================] - 2s 869ms/step - loss: 0.0092 - mse: 0.0075 - val_loss: 0.0394 - val_mse: 0.0270 - lr: 5.0000e-04\n",
            "Epoch 193/1000\n",
            "2/2 [==============================] - 2s 865ms/step - loss: 0.0098 - mse: 0.0080 - val_loss: 0.0384 - val_mse: 0.0260 - lr: 5.0000e-04\n",
            "Epoch 194/1000\n",
            "2/2 [==============================] - 2s 867ms/step - loss: 0.0096 - mse: 0.0081 - val_loss: 0.0399 - val_mse: 0.0268 - lr: 5.0000e-04\n",
            "Epoch 195/1000\n",
            "2/2 [==============================] - 2s 871ms/step - loss: 0.0089 - mse: 0.0075 - val_loss: 0.0438 - val_mse: 0.0290 - lr: 5.0000e-04\n",
            "Epoch 196/1000\n",
            "2/2 [==============================] - 2s 867ms/step - loss: 0.0093 - mse: 0.0077 - val_loss: 0.0431 - val_mse: 0.0281 - lr: 5.0000e-04\n",
            "Epoch 197/1000\n",
            "2/2 [==============================] - 2s 861ms/step - loss: 0.0089 - mse: 0.0073 - val_loss: 0.0408 - val_mse: 0.0277 - lr: 5.0000e-04\n",
            "Epoch 198/1000\n",
            "2/2 [==============================] - 2s 866ms/step - loss: 0.0086 - mse: 0.0072 - val_loss: 0.0398 - val_mse: 0.0265 - lr: 5.0000e-04\n",
            "Epoch 199/1000\n",
            "2/2 [==============================] - 2s 872ms/step - loss: 0.0089 - mse: 0.0075 - val_loss: 0.0399 - val_mse: 0.0270 - lr: 5.0000e-04\n",
            "Epoch 200/1000\n",
            "2/2 [==============================] - 2s 871ms/step - loss: 0.0085 - mse: 0.0071 - val_loss: 0.0417 - val_mse: 0.0276 - lr: 5.0000e-04\n",
            "Epoch 201/1000\n",
            "2/2 [==============================] - 2s 866ms/step - loss: 0.0083 - mse: 0.0070 - val_loss: 0.0424 - val_mse: 0.0280 - lr: 5.0000e-04\n",
            "Epoch 202/1000\n",
            "2/2 [==============================] - 2s 873ms/step - loss: 0.0082 - mse: 0.0069 - val_loss: 0.0418 - val_mse: 0.0282 - lr: 5.0000e-04\n",
            "Epoch 203/1000\n",
            "2/2 [==============================] - 2s 885ms/step - loss: 0.0081 - mse: 0.0069 - val_loss: 0.0409 - val_mse: 0.0268 - lr: 5.0000e-04\n",
            "Epoch 204/1000\n",
            "2/2 [==============================] - 2s 891ms/step - loss: 0.0081 - mse: 0.0068 - val_loss: 0.0402 - val_mse: 0.0273 - lr: 5.0000e-04\n",
            "Epoch 205/1000\n",
            "2/2 [==============================] - 2s 873ms/step - loss: 0.0083 - mse: 0.0069 - val_loss: 0.0403 - val_mse: 0.0269 - lr: 5.0000e-04\n",
            "Epoch 206/1000\n",
            "2/2 [==============================] - 2s 869ms/step - loss: 0.0080 - mse: 0.0069 - val_loss: 0.0430 - val_mse: 0.0280 - lr: 5.0000e-04\n",
            "Epoch 207/1000\n",
            "2/2 [==============================] - 2s 871ms/step - loss: 0.0082 - mse: 0.0069 - val_loss: 0.0457 - val_mse: 0.0309 - lr: 5.0000e-04\n",
            "Epoch 208/1000\n",
            "2/2 [==============================] - 2s 862ms/step - loss: 0.0096 - mse: 0.0081 - val_loss: 0.0472 - val_mse: 0.0298 - lr: 5.0000e-04\n",
            "Epoch 209/1000\n",
            "2/2 [==============================] - 2s 869ms/step - loss: 0.0092 - mse: 0.0073 - val_loss: 0.0416 - val_mse: 0.0280 - lr: 5.0000e-04\n",
            "Epoch 210/1000\n",
            "2/2 [==============================] - 2s 863ms/step - loss: 0.0080 - mse: 0.0066 - val_loss: 0.0385 - val_mse: 0.0263 - lr: 5.0000e-04\n",
            "Epoch 211/1000\n",
            "2/2 [==============================] - 2s 867ms/step - loss: 0.0089 - mse: 0.0076 - val_loss: 0.0394 - val_mse: 0.0266 - lr: 5.0000e-04\n",
            "Epoch 212/1000\n",
            "2/2 [==============================] - 2s 865ms/step - loss: 0.0082 - mse: 0.0070 - val_loss: 0.0433 - val_mse: 0.0291 - lr: 5.0000e-04\n",
            "Epoch 213/1000\n",
            "2/2 [==============================] - 2s 866ms/step - loss: 0.0083 - mse: 0.0072 - val_loss: 0.0478 - val_mse: 0.0309 - lr: 5.0000e-04\n",
            "Epoch 214/1000\n",
            "2/2 [==============================] - 2s 864ms/step - loss: 0.0104 - mse: 0.0082 - val_loss: 0.0444 - val_mse: 0.0284 - lr: 5.0000e-04\n",
            "Epoch 215/1000\n",
            "2/2 [==============================] - 2s 867ms/step - loss: 0.0084 - mse: 0.0068 - val_loss: 0.0407 - val_mse: 0.0276 - lr: 5.0000e-04\n",
            "Epoch 216/1000\n",
            "2/2 [==============================] - 2s 866ms/step - loss: 0.0082 - mse: 0.0067 - val_loss: 0.0400 - val_mse: 0.0269 - lr: 5.0000e-04\n",
            "Epoch 217/1000\n",
            "2/2 [==============================] - 2s 863ms/step - loss: 0.0089 - mse: 0.0075 - val_loss: 0.0390 - val_mse: 0.0264 - lr: 5.0000e-04\n",
            "Epoch 218/1000\n",
            "2/2 [==============================] - 2s 867ms/step - loss: 0.0079 - mse: 0.0067 - val_loss: 0.0460 - val_mse: 0.0298 - lr: 5.0000e-04\n",
            "Epoch 219/1000\n",
            "2/2 [==============================] - 2s 866ms/step - loss: 0.0087 - mse: 0.0071 - val_loss: 0.0447 - val_mse: 0.0289 - lr: 5.0000e-04\n",
            "Epoch 220/1000\n",
            "2/2 [==============================] - 2s 863ms/step - loss: 0.0082 - mse: 0.0067 - val_loss: 0.0408 - val_mse: 0.0276 - lr: 5.0000e-04\n",
            "Epoch 221/1000\n",
            "2/2 [==============================] - 2s 867ms/step - loss: 0.0078 - mse: 0.0065 - val_loss: 0.0397 - val_mse: 0.0264 - lr: 5.0000e-04\n",
            "Epoch 222/1000\n",
            "2/2 [==============================] - 2s 872ms/step - loss: 0.0080 - mse: 0.0069 - val_loss: 0.0412 - val_mse: 0.0278 - lr: 5.0000e-04\n",
            "Epoch 223/1000\n",
            "2/2 [==============================] - 2s 866ms/step - loss: 0.0075 - mse: 0.0064 - val_loss: 0.0441 - val_mse: 0.0292 - lr: 5.0000e-04\n",
            "Epoch 224/1000\n",
            "2/2 [==============================] - 2s 865ms/step - loss: 0.0079 - mse: 0.0067 - val_loss: 0.0429 - val_mse: 0.0277 - lr: 5.0000e-04\n",
            "Epoch 225/1000\n",
            "2/2 [==============================] - 2s 866ms/step - loss: 0.0075 - mse: 0.0063 - val_loss: 0.0421 - val_mse: 0.0283 - lr: 5.0000e-04\n",
            "Epoch 226/1000\n",
            "2/2 [==============================] - 2s 870ms/step - loss: 0.0074 - mse: 0.0063 - val_loss: 0.0391 - val_mse: 0.0261 - lr: 5.0000e-04\n",
            "Epoch 227/1000\n",
            "2/2 [==============================] - 2s 864ms/step - loss: 0.0076 - mse: 0.0066 - val_loss: 0.0416 - val_mse: 0.0276 - lr: 5.0000e-04\n",
            "Epoch 228/1000\n",
            "2/2 [==============================] - 2s 869ms/step - loss: 0.0071 - mse: 0.0061 - val_loss: 0.0435 - val_mse: 0.0291 - lr: 5.0000e-04\n",
            "Epoch 229/1000\n",
            "2/2 [==============================] - 2s 872ms/step - loss: 0.0072 - mse: 0.0062 - val_loss: 0.0430 - val_mse: 0.0277 - lr: 5.0000e-04\n",
            "Epoch 230/1000\n",
            "2/2 [==============================] - 2s 870ms/step - loss: 0.0072 - mse: 0.0060 - val_loss: 0.0432 - val_mse: 0.0289 - lr: 5.0000e-04\n",
            "Epoch 231/1000\n",
            "2/2 [==============================] - 2s 864ms/step - loss: 0.0070 - mse: 0.0060 - val_loss: 0.0409 - val_mse: 0.0270 - lr: 5.0000e-04\n",
            "Epoch 232/1000\n",
            "2/2 [==============================] - 2s 878ms/step - loss: 0.0070 - mse: 0.0060 - val_loss: 0.0413 - val_mse: 0.0272 - lr: 5.0000e-04\n",
            "Epoch 233/1000\n",
            "2/2 [==============================] - 2s 877ms/step - loss: 0.0068 - mse: 0.0058 - val_loss: 0.0427 - val_mse: 0.0287 - lr: 5.0000e-04\n",
            "Epoch 234/1000\n",
            "2/2 [==============================] - 2s 861ms/step - loss: 0.0068 - mse: 0.0059 - val_loss: 0.0427 - val_mse: 0.0277 - lr: 5.0000e-04\n",
            "Epoch 235/1000\n",
            "2/2 [==============================] - 2s 865ms/step - loss: 0.0068 - mse: 0.0058 - val_loss: 0.0423 - val_mse: 0.0283 - lr: 5.0000e-04\n",
            "Epoch 236/1000\n",
            "2/2 [==============================] - 2s 866ms/step - loss: 0.0067 - mse: 0.0058 - val_loss: 0.0414 - val_mse: 0.0274 - lr: 5.0000e-04\n",
            "Epoch 237/1000\n",
            "2/2 [==============================] - 2s 863ms/step - loss: 0.0068 - mse: 0.0059 - val_loss: 0.0406 - val_mse: 0.0270 - lr: 5.0000e-04\n",
            "Epoch 238/1000\n",
            "2/2 [==============================] - 2s 873ms/step - loss: 0.0068 - mse: 0.0058 - val_loss: 0.0427 - val_mse: 0.0283 - lr: 5.0000e-04\n",
            "Epoch 239/1000\n",
            "2/2 [==============================] - 2s 867ms/step - loss: 0.0067 - mse: 0.0058 - val_loss: 0.0430 - val_mse: 0.0282 - lr: 5.0000e-04\n",
            "Epoch 240/1000\n",
            "2/2 [==============================] - 2s 865ms/step - loss: 0.0069 - mse: 0.0059 - val_loss: 0.0450 - val_mse: 0.0296 - lr: 5.0000e-04\n",
            "Epoch 241/1000\n",
            "2/2 [==============================] - 2s 877ms/step - loss: 0.0071 - mse: 0.0060 - val_loss: 0.0434 - val_mse: 0.0283 - lr: 5.0000e-04\n",
            "Epoch 242/1000\n",
            "2/2 [==============================] - 2s 863ms/step - loss: 0.0069 - mse: 0.0059 - val_loss: 0.0445 - val_mse: 0.0289 - lr: 5.0000e-04\n",
            "Epoch 243/1000\n",
            "2/2 [==============================] - 2s 868ms/step - loss: 0.0067 - mse: 0.0058 - val_loss: 0.0438 - val_mse: 0.0290 - lr: 5.0000e-04\n",
            "Epoch 244/1000\n",
            "2/2 [==============================] - 2s 868ms/step - loss: 0.0067 - mse: 0.0058 - val_loss: 0.0433 - val_mse: 0.0281 - lr: 5.0000e-04\n",
            "Epoch 245/1000\n",
            "2/2 [==============================] - 2s 866ms/step - loss: 0.0065 - mse: 0.0056 - val_loss: 0.0420 - val_mse: 0.0279 - lr: 5.0000e-04\n",
            "Epoch 246/1000\n",
            "2/2 [==============================] - 2s 878ms/step - loss: 0.0064 - mse: 0.0056 - val_loss: 0.0416 - val_mse: 0.0275 - lr: 5.0000e-04\n",
            "Epoch 247/1000\n",
            "2/2 [==============================] - 2s 864ms/step - loss: 0.0065 - mse: 0.0056 - val_loss: 0.0402 - val_mse: 0.0268 - lr: 5.0000e-04\n",
            "Epoch 248/1000\n",
            "2/2 [==============================] - 2s 860ms/step - loss: 0.0069 - mse: 0.0060 - val_loss: 0.0397 - val_mse: 0.0270 - lr: 5.0000e-04\n",
            "Epoch 249/1000\n",
            "2/2 [==============================] - 2s 863ms/step - loss: 0.0079 - mse: 0.0067 - val_loss: 0.0383 - val_mse: 0.0260 - lr: 5.0000e-04\n",
            "Epoch 250/1000\n",
            "2/2 [==============================] - 2s 868ms/step - loss: 0.0081 - mse: 0.0068 - val_loss: 0.0399 - val_mse: 0.0271 - lr: 5.0000e-04\n",
            "Epoch 251/1000\n",
            "2/2 [==============================] - 2s 1000ms/step - loss: 0.0071 - mse: 0.0059 - val_loss: 0.0420 - val_mse: 0.0276 - lr: 5.0000e-04\n",
            "Epoch 252/1000\n",
            "2/2 [==============================] - 2s 876ms/step - loss: 0.0065 - mse: 0.0056 - val_loss: 0.0455 - val_mse: 0.0293 - lr: 5.0000e-04\n",
            "Epoch 253/1000\n",
            "2/2 [==============================] - 2s 865ms/step - loss: 0.0072 - mse: 0.0061 - val_loss: 0.0464 - val_mse: 0.0307 - lr: 5.0000e-04\n",
            "Epoch 254/1000\n",
            "2/2 [==============================] - 2s 863ms/step - loss: 0.0076 - mse: 0.0065 - val_loss: 0.0420 - val_mse: 0.0270 - lr: 5.0000e-04\n",
            "Epoch 255/1000\n",
            "2/2 [==============================] - 2s 867ms/step - loss: 0.0067 - mse: 0.0057 - val_loss: 0.0397 - val_mse: 0.0266 - lr: 5.0000e-04\n",
            "Epoch 256/1000\n",
            "2/2 [==============================] - 2s 873ms/step - loss: 0.0073 - mse: 0.0061 - val_loss: 0.0407 - val_mse: 0.0277 - lr: 5.0000e-04\n",
            "Epoch 257/1000\n",
            "2/2 [==============================] - 2s 869ms/step - loss: 0.0081 - mse: 0.0066 - val_loss: 0.0377 - val_mse: 0.0253 - lr: 5.0000e-04\n",
            "Epoch 258/1000\n",
            "2/2 [==============================] - 2s 862ms/step - loss: 0.0072 - mse: 0.0062 - val_loss: 0.0439 - val_mse: 0.0288 - lr: 5.0000e-04\n",
            "Epoch 259/1000\n",
            "2/2 [==============================] - 2s 867ms/step - loss: 0.0071 - mse: 0.0061 - val_loss: 0.0472 - val_mse: 0.0307 - lr: 5.0000e-04\n",
            "Epoch 260/1000\n",
            "2/2 [==============================] - 2s 876ms/step - loss: 0.0077 - mse: 0.0064 - val_loss: 0.0421 - val_mse: 0.0271 - lr: 5.0000e-04\n",
            "Epoch 261/1000\n",
            "2/2 [==============================] - 2s 864ms/step - loss: 0.0068 - mse: 0.0057 - val_loss: 0.0394 - val_mse: 0.0273 - lr: 5.0000e-04\n",
            "Epoch 262/1000\n",
            "2/2 [==============================] - 2s 868ms/step - loss: 0.0099 - mse: 0.0077 - val_loss: 0.0372 - val_mse: 0.0251 - lr: 5.0000e-04\n",
            "Epoch 263/1000\n",
            "2/2 [==============================] - 2s 867ms/step - loss: 0.0075 - mse: 0.0064 - val_loss: 0.0462 - val_mse: 0.0301 - lr: 5.0000e-04\n",
            "Epoch 264/1000\n",
            "2/2 [==============================] - 2s 865ms/step - loss: 0.0078 - mse: 0.0066 - val_loss: 0.0430 - val_mse: 0.0290 - lr: 5.0000e-04\n",
            "Epoch 265/1000\n",
            "2/2 [==============================] - 2s 869ms/step - loss: 0.0071 - mse: 0.0060 - val_loss: 0.0395 - val_mse: 0.0256 - lr: 5.0000e-04\n",
            "Epoch 266/1000\n",
            "2/2 [==============================] - 2s 861ms/step - loss: 0.0074 - mse: 0.0062 - val_loss: 0.0409 - val_mse: 0.0283 - lr: 5.0000e-04\n",
            "Epoch 267/1000\n",
            "2/2 [==============================] - 2s 868ms/step - loss: 0.0070 - mse: 0.0058 - val_loss: 0.0412 - val_mse: 0.0264 - lr: 5.0000e-04\n",
            "Epoch 268/1000\n",
            "2/2 [==============================] - 2s 869ms/step - loss: 0.0068 - mse: 0.0057 - val_loss: 0.0433 - val_mse: 0.0289 - lr: 5.0000e-04\n",
            "Epoch 269/1000\n",
            "2/2 [==============================] - 2s 868ms/step - loss: 0.0066 - mse: 0.0057 - val_loss: 0.0422 - val_mse: 0.0275 - lr: 5.0000e-04\n",
            "Epoch 270/1000\n",
            "2/2 [==============================] - 2s 869ms/step - loss: 0.0065 - mse: 0.0055 - val_loss: 0.0417 - val_mse: 0.0277 - lr: 5.0000e-04\n",
            "Epoch 271/1000\n",
            "2/2 [==============================] - 2s 872ms/step - loss: 0.0063 - mse: 0.0053 - val_loss: 0.0409 - val_mse: 0.0273 - lr: 5.0000e-04\n",
            "Epoch 272/1000\n",
            "2/2 [==============================] - 2s 871ms/step - loss: 0.0064 - mse: 0.0055 - val_loss: 0.0395 - val_mse: 0.0264 - lr: 5.0000e-04\n",
            "Epoch 273/1000\n",
            "2/2 [==============================] - 2s 871ms/step - loss: 0.0062 - mse: 0.0053 - val_loss: 0.0420 - val_mse: 0.0277 - lr: 5.0000e-04\n",
            "Epoch 274/1000\n",
            "2/2 [==============================] - 2s 869ms/step - loss: 0.0060 - mse: 0.0052 - val_loss: 0.0452 - val_mse: 0.0292 - lr: 5.0000e-04\n",
            "Epoch 275/1000\n",
            "2/2 [==============================] - 2s 867ms/step - loss: 0.0063 - mse: 0.0054 - val_loss: 0.0416 - val_mse: 0.0277 - lr: 5.0000e-04\n",
            "Epoch 276/1000\n",
            "2/2 [==============================] - 2s 866ms/step - loss: 0.0059 - mse: 0.0052 - val_loss: 0.0406 - val_mse: 0.0266 - lr: 5.0000e-04\n",
            "Epoch 277/1000\n",
            "2/2 [==============================] - 2s 865ms/step - loss: 0.0062 - mse: 0.0053 - val_loss: 0.0388 - val_mse: 0.0264 - lr: 5.0000e-04\n",
            "Epoch 278/1000\n",
            "2/2 [==============================] - 2s 868ms/step - loss: 0.0063 - mse: 0.0054 - val_loss: 0.0417 - val_mse: 0.0272 - lr: 5.0000e-04\n",
            "Epoch 279/1000\n",
            "2/2 [==============================] - 2s 881ms/step - loss: 0.0058 - mse: 0.0051 - val_loss: 0.0444 - val_mse: 0.0292 - lr: 5.0000e-04\n",
            "Epoch 280/1000\n",
            "2/2 [==============================] - 2s 874ms/step - loss: 0.0065 - mse: 0.0056 - val_loss: 0.0478 - val_mse: 0.0307 - lr: 5.0000e-04\n",
            "Epoch 281/1000\n",
            "2/2 [==============================] - 2s 898ms/step - loss: 0.0066 - mse: 0.0056 - val_loss: 0.0428 - val_mse: 0.0285 - lr: 5.0000e-04\n"
          ]
        }
      ],
      "source": [
        "# Learning Rate Scheduler\n",
        "def scheduler(epoch, lr):\n",
        "  if epoch == 500:\n",
        "    return lr /5\n",
        "  else:\n",
        "    return lr\n",
        "\n",
        "scheduler_cb = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
        "\n",
        "# Early stopping criteria\n",
        "early = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\",min_delta=0.0005, patience=200, verbose=0, mode=\"auto\", baseline=None, restore_best_weights=True)\n",
        "\n",
        "# Training (Fit)\n",
        "history = model.fit(train_data, train_labels, epochs=1000, validation_data=(validation_data, validation_labels), callbacks=[early, scheduler_cb])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This next plot shows loss of the trianing and validation sets."
      ],
      "metadata": {
        "id": "xIGiYmh3fwP2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "UgCp0oNUAfzt",
        "outputId": "c644301e-3028-4c6e-f5dd-859f0fd741be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['loss', 'mse', 'val_loss', 'val_mse', 'lr'])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "print(history.history.keys())\n",
        "# # summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'valid'], loc='upper right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plotting the results\n",
        "\n",
        "After our training, we need to plot our results to verify if we are making a good prediction since the value of the loss can be heavily influenced by outliers. The following cell includes nine plots we use to evaluate performance on each of the points."
      ],
      "metadata": {
        "id": "Hb4C5FSDf-2n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for the volume plot of hotspot\n",
        "\n",
        "def area_hotspot(datapoint):\n",
        "  tinc = 5\n",
        "  tnum = int(4500/tinc)\n",
        "\n",
        "  tinc = 5\n",
        "  tnum = int(4500/tinc)\n",
        "\n",
        "  bins=np.zeros((tnum,2))\n",
        "  bins[:,0] = [ tinc*(0.5 + x) for x in list(range(tnum))]\n",
        "\n",
        "  for bin_index, temp in np.ndenumerate(datapoint):\n",
        "    theta = temp * 1000\n",
        "    tind=math.floor(theta/tinc)\n",
        "    bins[:tind, 1] += 4 # ~ Roughly 2*2*1 nm^3 (volume value from Chunyu)\n",
        "  \n",
        "  return bins"
      ],
      "metadata": {
        "id": "cVbLrXkBNWDI"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def datapoint_results_print(simulation_point, simulation_temperatures, prediction_tensor):\n",
        "\n",
        "  # Simulation point is the input train_data[<point order>,<dim 0>,<dim 1>,<dim 2>,<channel>]\n",
        "  # For example, simulation_point = train_data[i,:,:,:,:]\n",
        "\n",
        "\n",
        "\n",
        "  fig = make_subplots(rows=3, cols=3, specs=[[{\"type\": \"scatter3d\"},{\"type\": \"scatter3d\"},{\"type\": \"scatter3d\"}],\n",
        "                                             [{\"type\": \"scatter3d\"},{\"type\": \"scatter3d\"},{\"type\": \"scatter\"}],\n",
        "                                             [{\"type\": \"histogram\"},{\"type\": \"histogram\"},{\"type\": \"scatter\"}]],\n",
        "                      subplot_titles=[\"Input 1\",\"Input 2\",\"Input 3\",\n",
        "                                      \"Temp (Labels)\",\"Temp (Predictions)\",\"Parity Plot\",\n",
        "                                      \"Temp (Distributions)\", \"Residuals\", \"Hotspot volume\"], horizontal_spacing = 0.1, vertical_spacing = 0.1)\n",
        "  \n",
        "  fig.update_layout(autosize=False, width=800, height=800) \n",
        "\n",
        "  # FIRST PLOT --> INPUT 1\n",
        "\n",
        "  input_1 = simulation_point[:,:,:,0].squeeze()\n",
        "\n",
        "  X,Y,Z = np.mgrid[0:input_1.shape[0], 0:input_1.shape[1], 0:input_1.shape[2]]\n",
        "  #input_1_xz = np.swapaxes(input_1, 2, 0)\n",
        "\n",
        "  trace_1 = go.Scatter3d(x = X.flatten(), y = Y.flatten(), z=Z.flatten(),\n",
        "                          mode='markers', marker=dict(colorscale='Viridis', size=8, line=dict(width=0), symbol='square', color = input_1.flatten(), colorbar=dict(thickness=20, len=0.3, x=0.27, y=0.9)), showlegend=False)\n",
        "  fig.add_trace(trace_1, row=1, col=1)\n",
        "\n",
        "  # SECOND PLOT --> INPUT 2\n",
        "\n",
        "  input_2 = simulation_point[:,:,:,1].squeeze()\n",
        "\n",
        "  X,Y,Z = np.mgrid[0:input_2.shape[0], 0:input_2.shape[1], 0:input_2.shape[2]]\n",
        "  #input_2_xz = np.swapaxes(input_2, 2, 0)\n",
        "\n",
        "  trace_2 = go.Scatter3d(x = X.flatten(), y = Y.flatten(), z=Z.flatten(),\n",
        "                          mode='markers', marker=dict(colorscale='Viridis', size=8, line=dict(width=0), symbol='square', color = input_2.flatten(), colorbar=dict(thickness=20, len=0.3, x=0.62, y=0.9)), showlegend=False)\n",
        "  fig.add_trace(trace_2, row=1, col=2)\n",
        "\n",
        "\n",
        "  # THIRD PLOT --> INPUT 3\n",
        "\n",
        "  input_3 = simulation_point[:,:,:,2].squeeze()\n",
        "\n",
        "  X,Y,Z = np.mgrid[0:input_3.shape[0], 0:input_3.shape[1], 0:input_3.shape[2]]\n",
        "  #input_3_xz = np.swapaxes(input_3, 2, 0)\n",
        "\n",
        "  trace_3 = go.Scatter3d(x = X.flatten(), y = Y.flatten(), z=Z.flatten(),\n",
        "                          mode='markers', marker=dict(colorscale='Viridis', size=8, line=dict(width=0), symbol='square', color = input_3.flatten(), colorbar=dict(thickness=20, len=0.3, x=1, y=0.9)), showlegend=False)\n",
        "  fig.add_trace(trace_3, row=1, col=3)\n",
        "\n",
        "\n",
        "  # FOURTH PLOT --> TEMPS (LABELS)\n",
        "\n",
        "  maxval = max(np.max(prediction_tensor), np.max(simulation_temperatures))\n",
        "  \n",
        "\n",
        "  X,Y,Z = np.mgrid[0:simulation_temperatures.shape[0], 0:simulation_temperatures.shape[1], 0:simulation_temperatures.shape[2]]\n",
        "  #simulation_temperatures_xz = np.swapaxes(simulation_temperatures, 2, 0)\n",
        "\n",
        "  trace_4 = go.Scatter3d(x = X.flatten(), y = Y.flatten(), z=Z.flatten(),\n",
        "                          mode='markers', marker=dict(cmin=0, cmax=maxval, colorscale='Reds', size=8, line=dict(width=0), symbol='square', color = simulation_temperatures.flatten(), colorbar=dict(thickness=20,len=0.3, x=0.27, y=0.5)), showlegend=False)\n",
        "  fig.add_trace(trace_4, row=2, col=1)\n",
        "\n",
        "\n",
        "  # FIFTH PLOT --> TEMPS (PREDICTIONS)\n",
        "\n",
        "  X,Y,Z = np.mgrid[0:prediction_tensor.shape[0], 0:prediction_tensor.shape[1], 0:prediction_tensor.shape[2]]\n",
        "  #prediction_tensor_xz = np.swapaxes(prediction_tensor, 2, 0)\n",
        "\n",
        "  trace_5 = go.Scatter3d(x = X.flatten(), y = Y.flatten(), z=Z.flatten(),\n",
        "                          mode='markers', marker=dict(cmin=0, cmax=maxval, colorscale='Reds', size=8, line=dict(width=0), symbol='square', color = prediction_tensor.flatten(), colorbar=dict(thickness=20, len=0.3, x=0.62, y=0.5)), showlegend=False)\n",
        "  fig.add_trace(trace_5, row=2, col=2)\n",
        "\n",
        "  # SIXTH PLOT --> PARITY PLOT\n",
        "\n",
        "  trace_6 = go.Scatter(x=simulation_temperatures.flatten(), y = prediction_tensor.flatten(), mode='markers', showlegend=False)\n",
        "  fig.add_trace(trace_6, row=2, col=3)\n",
        "  fig.update_xaxes(title=\"Scaled Temperature (K) - Labels\", row=2, col=3)\n",
        "  fig.update_yaxes(title=\"Scaled Temperature (K) - Predictions\", row=2, col=3)\n",
        "\n",
        "\n",
        "  # SEVENTH PLOT --> TEMP (Distributions)\n",
        "\n",
        "  trace_7 = go.Histogram(x=prediction_tensor.flatten(), name='Predictions', marker=dict(color='red'), showlegend=True, legendgroup = '1')\n",
        "  trace_7b = go.Histogram(x=simulation_temperatures.flatten(), name='Truth', marker=dict(color='green'), showlegend=True, legendgroup = '1')\n",
        "  fig.update_xaxes(title=\"Temperature (K) - Labels\", row=3, col=1)\n",
        "  fig.add_trace(trace_7, row=3, col=1)\n",
        "  fig.add_trace(trace_7b, row=3, col=1) \n",
        "\n",
        "\n",
        "  # EIGHTH PLOT --> RESIDUALS\n",
        "\n",
        "  diff_xz = prediction_tensor - simulation_temperatures\n",
        "  flat_diff_xz = diff_xz.flatten()\n",
        "\n",
        "  trace_8 = go.Histogram(x=flat_diff_xz, showlegend=False)\n",
        "  trace_line = go.Scatter(x=[0,0], y = [0,700], mode='lines', showlegend=False)\n",
        "\n",
        "  fig.add_trace(trace_8, row=3, col=2)\n",
        "  fig.add_trace(trace_line, row=3, col=2)\n",
        "\n",
        "  # NINTH PLOT ---> VOLUME OF HOTSPOT\n",
        "  \n",
        "  bins_labels = area_hotspot(simulation_temperatures)\n",
        "  bins_predictions = area_hotspot(prediction_tensor)\n",
        "  trace_9 = go.Scatter(x=bins_predictions[:,1], y=bins_predictions[:,0], mode='lines',  name='Predictions', marker=dict(color='red'), showlegend=True, legendgroup = '2')\n",
        "  trace_9b = go.Scatter(x=bins_labels[:,1], y=bins_labels[:,0], mode='lines', name='Truth', marker=dict(color='green'), showlegend=True, legendgroup = '2')\n",
        "  \n",
        "  fig.add_trace(trace_9, row=3, col=3)\n",
        "  fig.add_trace(trace_9b, row=3, col=3) \n",
        "  fig.update_xaxes(type=\"log\", row=3, col=3)\n",
        "  fig.update_xaxes(title=\"Hotspot Volume (nm^3)\", row=3, col=3)\n",
        "  fig.update_yaxes(title=\"Temperature (K)\", row=3, col=3)  \n",
        "\n",
        "  #### \n",
        "\n",
        "  fig.update_layout(autosize=False, width=1400, height=1000, legend_tracegroupgap = 180, legend=dict(font=dict(size=16),orientation=\"h\"))   \n",
        "\n",
        "  return fig"
      ],
      "metadata": {
        "id": "p65_1oRxgTu1"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since rendering multiple plots for each system is not ideal, we will save a numpy array with the model predictions and an HTML file with the interactive plots for exploration."
      ],
      "metadata": {
        "id": "0Jx7VbNCihTX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p results/train\n",
        "!mkdir -p results/validation"
      ],
      "metadata": {
        "id": "zVpCWbXshU4h"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PREDICTIONS OVER TRAINING DATA\n",
        "\n",
        "for i, title in enumerate(paths):\n",
        "  print(i)\n",
        "  inppoint = train_data[i,:,:,:,:].squeeze()\n",
        "  inplabel = train_labels[i,:,:,:,:].squeeze()\n",
        "\n",
        "  prediction_point = model.predict(tf.expand_dims(inppoint, axis=0))\n",
        "  prediction_tensor = prediction_point.squeeze()\n",
        "  #print(prediction_tensor.shape)\n",
        "\n",
        "  inppoint = inppoint[:, 1:-1, 1:-1] # NON PERIODIC GETS PLOTTED\n",
        "\n",
        "  fig = datapoint_results_print(inppoint, inplabel, prediction_tensor)\n",
        "\n",
        "  os.mkdir('results/'+str(title))\n",
        "\n",
        "  ### SAVING NUMPY PREDICTIONS\n",
        "  np.save('results/'+str(title)+'/prediction.npy', prediction_tensor)\n",
        "  ### SAVING HTML IMAGES\n",
        "  fig.write_html('results/'+str(title)+'/visualization.html')"
      ],
      "metadata": {
        "id": "nTuoEr8Khjpw",
        "outputId": "053135a0-8c02-4fd1-c4a7-c285d306c0a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "2\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "3\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "4\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "5\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "6\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "7\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "8\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "9\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "10\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "11\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "12\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "13\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "14\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "15\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "16\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "17\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "18\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "19\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "20\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "21\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "22\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "23\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "24\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "25\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "26\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "27\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "28\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "29\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "30\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "31\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "32\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "33\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "34\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "35\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "36\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "37\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "38\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "39\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "40\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "41\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "42\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "43\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "44\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "45\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "46\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "47\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "48\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "49\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "50\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "51\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "52\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "53\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "54\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "55\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "56\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "57\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "58\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "59\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "60\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "61\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "62\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "63\n",
            "1/1 [==============================] - 0s 20ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PREDICTIONS OVER VALIDATION DATA\n",
        "\n",
        "for i, title in enumerate(validation_paths):\n",
        "  print(i)\n",
        "  inppoint = validation_data[i,:,:,:,:].squeeze()\n",
        "  inplabel = validation_labels[i,:,:,:,:].squeeze()\n",
        "\n",
        "  prediction_point = model.predict(tf.expand_dims(inppoint, axis=0))\n",
        "  prediction_tensor = prediction_point.squeeze()\n",
        "\n",
        "  inppoint = inppoint[:, 1:-1, 1:-1] # NON PERIODIC GETS PLOTTED\n",
        "  fig = datapoint_results_print(inppoint, inplabel, prediction_tensor)\n",
        "  \n",
        "  os.mkdir('results/'+str(title))\n",
        "\n",
        "  ### SAVING NUMPY PREDICTIONS\n",
        "  np.save('results/'+str(title)+'/prediction.npy', prediction_tensor)\n",
        "  ### SAVING HTML IMAGES\n",
        "  fig.write_html('results/'+str(title)+'/visualization.html')"
      ],
      "metadata": {
        "id": "nYhrkn0_iK52",
        "outputId": "0b879ffb-6864-41c9-c830-bbc7fa611abf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "2\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "3\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "4\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "5\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "6\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "7\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "8\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "9\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "10\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "11\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "12\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "13\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "14\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "15\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "16\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "17\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "18\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "19\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "20\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "21\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "22\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "23\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "24\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "25\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "26\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "27\n",
            "1/1 [==============================] - 0s 21ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cluster Analysis \n",
        "In the following cells we perform cluster analysis to compare hotspots produced by the MD simulation and the predictions made by the CNN. "
      ],
      "metadata": {
        "id": "yxkN6Bos4wrU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_clusters(temp_grid):\n",
        "  x = []\n",
        "  y = []\n",
        "  z = []\n",
        "  temp = []\n",
        "  for i in range(16):\n",
        "    for j in range(32):\n",
        "      for k in range(32):\n",
        "        if temp_grid[i,j,k] >= 1.6: #temp cut-off\n",
        "          x.append(i+1)\n",
        "          y.append(j+1)\n",
        "          z.append(k+1)\n",
        "          temp.append(temp_grid[i,j,k])\n",
        "  \n",
        "  df = pd.DataFrame(temp,columns=['Temperature'])\n",
        "  df['X'] = x\n",
        "  df['Y'] = y\n",
        "  df['Z'] = z\n",
        "\n",
        "  pos = df[['X','Y','Z']].values\n",
        "  agglo = AgglomerativeClustering(n_clusters=None, distance_threshold=1.8, linkage='single').fit(pos)\n",
        "  df['Cluster ID'] = agglo.labels_\n",
        "\n",
        "  return df\n",
        "\n",
        "def overlap(df1,df2):\n",
        "  md = df1.copy()\n",
        "  pred = df2.copy()\n",
        "  overlap = []\n",
        "  for i in range(md['Cluster ID'].max()+1):\n",
        "    for j in range(pred['Cluster ID'].max()+1):\n",
        "      md_tmp = md[md['Cluster ID'] == i]\n",
        "      pred_tmp = pred[pred['Cluster ID'] == j]\n",
        "      counts = 0\n",
        "      for k in range(len(md_tmp)):\n",
        "        for l in range(len(pred_tmp)):\n",
        "          if (md_tmp[['X','Y','Z']].values[k] == pred_tmp[['X','Y','Z']].values[l]).all():\n",
        "            counts += 1\n",
        "      #if counts > 0:\n",
        "      overlap.append([i,j,counts/len(md_tmp)])\n",
        "  df_overlap = pd.DataFrame(overlap,columns=['MD id','Pred id','fraction'])\n",
        "  over_array = df_overlap[df_overlap['fraction']>0].drop_duplicates(subset=['Pred id'],keep=False).drop_duplicates(subset=['MD id'],keep=False).values\n",
        "  new_id = []\n",
        "  for i in range(len(pred)):\n",
        "    if not (pred['Cluster ID'][i] in over_array[:,1].tolist()):\n",
        "      if pred['Cluster ID'][i] in df_overlap[df_overlap['fraction']>0]['Pred id'].tolist():\n",
        "        new_id.append(100)\n",
        "      else:\n",
        "        new_id.append(None)\n",
        "    for j in range(len(over_array)):\n",
        "      if pred['Cluster ID'][i] == over_array[j,1]:\n",
        "        new_id.append(int(over_array[j,0]))\n",
        "  pred['New ID'] = new_id\n",
        "\n",
        "  new_id = []\n",
        "  for i in range(len(md)):\n",
        "    if not (md['Cluster ID'][i] in over_array[:,0].tolist()):\n",
        "      if md['Cluster ID'][i] in df_overlap[df_overlap['fraction']>0]['MD id'].tolist():\n",
        "          new_id.append(100)\n",
        "      else:\n",
        "          new_id.append(None)\n",
        "    else:\n",
        "      new_id.append(int(md['Cluster ID'][i]))\n",
        "  md['New ID'] = new_id\n",
        "  return md, pred, df_overlap\n",
        "\n",
        "def analyze_clusters(df):\n",
        "  dataframe = df.copy()\n",
        "  vols = []\n",
        "  mean_temps = []\n",
        "  std_temps = []\n",
        "  cms = []\n",
        "  mis = []\n",
        "  ids = []\n",
        "  filt = dataframe[dataframe['New ID']<100].dropna()\n",
        "  for i in range(int(filt['New ID'].max())+1):\n",
        "    if i in filt['New ID'].tolist():\n",
        "      tmp = filt[filt['New ID'] == i]\n",
        "      v = len(tmp)*4\n",
        "      mean = np.mean(tmp['Temperature']*1000)\n",
        "      std = np.std(tmp['Temperature']*1000)\n",
        "      m = len(tmp)\n",
        "      cm_x = tmp['X'].sum()/m\n",
        "      cm_y = tmp['Y'].sum()/m\n",
        "      cm_z = tmp['Z'].sum()/m\n",
        "      cm = np.array([cm_x,cm_y,cm_z])\n",
        "      r_i = tmp[['X','Y','Z']].values\n",
        "      mom_int = 0\n",
        "      for j in range(len(tmp)):\n",
        "        mom_int += np.linalg.norm(r_i[j]-cm)**2\n",
        "      vols.append(v)\n",
        "      mean_temps.append(mean)\n",
        "      std_temps.append(std)\n",
        "      cms.append(cm)\n",
        "      mis.append(mom_int)\n",
        "      ids.append(i)\n",
        "    else:\n",
        "      pass\n",
        "  \n",
        "  df_clust = pd.DataFrame(ids, columns=['cluster id'])\n",
        "  df_clust['volume'] = vols\n",
        "  df_clust['mean T (K)'] = mean_temps\n",
        "  df_clust['std T (K)'] = std_temps\n",
        "  df_clust['R center of mass'] = cms\n",
        "  df_clust['moment of inertia'] = mis\n",
        "  return df_clust"
      ],
      "metadata": {
        "id": "TfOztIGF8B3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_clusters(md,pred):\n",
        "  fig = make_subplots(rows=1, cols=2, specs=[[{\"type\": \"scatter3d\"},{\"type\": \"scatter3d\"}]],\n",
        "                    subplot_titles=[\"MD Clusters\",\"CNN Clusters\"], horizontal_spacing = 0.1, vertical_spacing = 0.1)\n",
        "  fig.update_layout(autosize=False, width=1200, height=800) \n",
        "  trace_1 = go.Scatter3d(x = md['X'], y = md['Y'], z=md['Z'], hovertemplate = 'Cluster ID: %{marker.color:.2f}<extra></extra>',\n",
        "                      mode='markers',  marker=dict(symbol='square', color = md['Cluster ID']), showlegend=False)\n",
        "  fig.add_trace(trace_1, row=1, col=1)\n",
        "  trace_2 = go.Scatter3d(x = pred['X'], y = pred['Y'], z=pred['Z'], hovertemplate = 'Cluster ID: %{marker.color:.2f}<extra></extra>',\n",
        "                      mode='markers',  marker=dict(symbol='square', color = pred['Cluster ID']), showlegend=False)\n",
        "  fig.add_trace(trace_2, row=1, col=2)\n",
        "\n",
        "  return fig\n",
        "\n"
      ],
      "metadata": {
        "id": "VaPQdkGzPaXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize(md_corr, pred_corr, md_clust, pred_clust):\n",
        "  fig = make_subplots(rows=2, cols=2, specs=[[{\"type\": \"scatter3d\"},{\"type\": \"scatter3d\"}],\n",
        "                                            [{\"type\": \"scatter\"},{\"type\": \"scatter\"}]],\n",
        "                      subplot_titles=[\"MD Clusters\",\"CNN Clusters\",\"Temp vs Volume\",\"MI Parity Plot\"], horizontal_spacing = 0.1, vertical_spacing = 0.1)\n",
        "  fig.update_layout(autosize=False, width=1200, height=800) \n",
        "\n",
        "\n",
        "  ## MD SIMULATION CLUSTERS ##\n",
        "  tmp = md_corr[md_corr['New ID']<99].dropna()\n",
        "  trace_1 = go.Scatter3d(x = tmp['X'], y = tmp['Y'], z = tmp['Z'],text=tmp['New ID'],\n",
        "                        mode='markers', marker=dict(size=8, line=dict(width=0), symbol='square', colorscale='rainbow', color = tmp['New ID']), showlegend=False, scene='scene1')\n",
        "  tmp = md_corr[md_corr['New ID']==100]\n",
        "  trace_2 = go.Scatter3d(x = tmp['X'], y = tmp['Y'], z = tmp['Z'],text=tmp['Cluster ID'],\n",
        "                        mode='markers', marker=dict(size=8, line=dict(width=0), symbol='square', color = 'grey'), showlegend=False, scene='scene1')\n",
        "  tmp = md_corr[md_corr['New ID'].isnull()]\n",
        "  trace_3 = go.Scatter3d(x = tmp['X'], y = tmp['Y'], z = tmp['Z'],text=tmp['Cluster ID'],\n",
        "                        mode='markers', marker=dict(size=8, line=dict(width=0), symbol='square', color = 'black'), showlegend=False, scene='scene1')    \n",
        "  fig.add_trace(trace_1, row=1, col=1)\n",
        "  fig.add_trace(trace_2, row=1, col=1)\n",
        "  fig.add_trace(trace_3, row=1, col=1)\n",
        "\n",
        "  ## CNN PREDICTED CLUSTERS ##\n",
        "  tmp2 = pred_corr[pred_corr['New ID']<99].dropna()\n",
        "  trace_4 = go.Scatter3d(x = tmp2['X'], y = tmp2['Y'], z = tmp2['Z'],text=tmp2['New ID'],\n",
        "                        mode='markers', marker=dict(size=8, line=dict(width=0), symbol='square', colorscale='rainbow', color = tmp2['New ID']), showlegend=False, scene='scene2')\n",
        "  tmp2 = pred_corr[pred_corr['New ID']==100]\n",
        "  trace_5 = go.Scatter3d(x = tmp2['X'], y = tmp2['Y'], z = tmp2['Z'],text=tmp2['Cluster ID'],\n",
        "                        mode='markers', marker=dict(size=8, line=dict(width=0), symbol='square', color = 'grey'), showlegend=False, scene='scene2')\n",
        "  tmp2 = pred_corr[pred_corr['New ID'].isnull()]\n",
        "  trace_6 = go.Scatter3d(x = tmp2['X'], y = tmp2['Y'], z = tmp2['Z'],text=tmp2['Cluster ID'],\n",
        "                        mode='markers', marker=dict(size=8, line=dict(width=0), symbol='square', color = 'black'), showlegend=False, scene='scene2')\n",
        "\n",
        "  fig.add_trace(trace_4, row=1, col=2)\n",
        "  fig.add_trace(trace_5, row=1, col=2)\n",
        "  fig.add_trace(trace_6, row=1, col=2)\n",
        "\n",
        "\n",
        "  ### Hotspot Temperature vs Volume ###\n",
        "  trace_7 = go.Scatter(x=md_clust['volume'],y=md_clust['mean T (K)'],error_y=dict(type='data', array=md_clust['std T (K)'],visible=True),text=md_clust['cluster id'],\n",
        "                      mode='markers',marker=dict(symbol='square',colorscale='rainbow', color = md_clust['cluster id'],size=18),name='MD')\n",
        "  trace_8 = go.Scatter(x=pred_clust['volume'],y=pred_clust['mean T (K)'],error_y=dict(type='data', array=pred_clust['std T (K)'],visible=True),text=md_clust['cluster id'],\n",
        "                        mode='markers',marker=dict(symbol='circle',colorscale='rainbow', color = md_clust['cluster id'],size=18),name='CNN')\n",
        "\n",
        "  fig.add_trace(trace_7, row=2, col=1)\n",
        "  fig.add_trace(trace_8, row=2, col=1)\n",
        "  fig.update_xaxes(title=\"Volume\")\n",
        "  fig.update_yaxes(title=\"Temperature (K)\")\n",
        "\n",
        "  ### Hotspot Moment inertia parity plot ###\n",
        "  trace_9 = go.Scatter(x=np.log10(md_clust['moment of inertia']),y=np.log10(pred_clust['moment of inertia']),\n",
        "                      mode='markers',marker=dict(symbol='square',color='green',size=14), showlegend=False)\n",
        "\n",
        "  fig.add_trace(trace_9, row=2, col=2)\n",
        "  fig.update_yaxes(title=\"Predicted log(moment of inertia)\")\n",
        "  fig.update_xaxes(title=\"MD Simulated log(moment of inertia))\")\n",
        "\n",
        "  #def cam_change(layout, camera):\n",
        "  #   fig.layout.scene2.camera = camera\n",
        "  #fig.layout.scene1.on_change(cam_change, 'camera')\n",
        "  return fig"
      ],
      "metadata": {
        "id": "UePKih3V3s9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CLUSTER ANALYSIS TRAINING DATA\n",
        "\n",
        "for i, title in enumerate(paths):\n",
        "  print(i)\n",
        "  md_temp = np.load(str(title)+'/output.npy')/1000\n",
        "  pred_temp = np.load('results/'+str(title)+'/prediction.npy')\n",
        "\n",
        "  md_1 = get_clusters(md_temp)\n",
        "  pred_1 = get_clusters(pred_temp)\n",
        "\n",
        "  md_corr, pred_corr, over = overlap(md_1,pred_1)\n",
        "  md_clust = analyze_clusters(md_corr)\n",
        "  pred_clust = analyze_clusters(pred_corr)\n",
        "\n",
        "  fig = visualize(md_corr,pred_corr,md_clust,pred_clust)\n",
        "\n",
        "  fig.write_html('results/'+str(title)+'/cluster_1.6.html')"
      ],
      "metadata": {
        "id": "RhYlpaScCVoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_0"
      ],
      "metadata": {
        "id": "kS-zq2YzD_Yx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mi = go.Scatter(x=md_mis, y=pred_mis,mode='markers',marker=dict(symbol='square',color='Blue',size=10))\n",
        "mi_fig = go.Figure(mi)\n",
        "mi_fig.show()"
      ],
      "metadata": {
        "id": "YYw6n0izpIAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploration GUI  (Under development)"
      ],
      "metadata": {
        "id": "8GgptMfOlBs2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# FUNCTIONS TO OBSERVE\n",
        "def populate_sliders(change):\n",
        "\n",
        "  with output_plot:\n",
        "    clear_output()\n",
        "  \n",
        "  example_label = np.load(main_dropdown.value + \"/output.npy\").squeeze() / 1000\n",
        "  example_pred = np.load('results/'+main_dropdown.value+\"/prediction.npy\").squeeze()\n",
        "\n",
        "  md_tmp_slider.min = np.min(example_label)\n",
        "  md_tmp_slider.max = np.max(example_label)\n",
        "  md_tmp_slider.value = [np.min(example_label), np.max(example_label)]\n",
        "\n",
        "  pred_tmp_slider.min = np.min(example_pred)\n",
        "  pred_tmp_slider.max = np.max(example_pred)\n",
        "  pred_tmp_slider.value = [np.min(example_pred), np.max(example_pred)]\n",
        "\n",
        "def display_fullplot(change):\n",
        "\n",
        "  with output_plot:\n",
        "    clear_output()\n",
        "\n",
        "  example_inp = np.load(main_dropdown.value + \"/input.npy\").squeeze()\n",
        "  example_inp = example_inp[:, 1:-1, 1:-1] # NON PERIODIC GETS PLOTTED\n",
        "  example_label = np.load(main_dropdown.value + \"/output.npy\").squeeze() / 1000\n",
        "  example_pred = np.load('results/'+main_dropdown.value+\"/prediction.npy\").squeeze()\n",
        "\n",
        "  example_fig = datapoint_results_print(example_inp, example_label, example_pred)\n",
        "  example_fig = go.FigureWidget(example_fig)\n",
        "\n",
        "  if toggle_button.value == \"MD\":\n",
        "\n",
        "    map = np.where(np.logical_and(example_label >=md_tmp_slider.value[0], example_label <=md_tmp_slider.value[1]), 8, 0)\n",
        "    with example_fig.batch_update():\n",
        "      example_fig.data[0].marker.size = map.flatten()\n",
        "      example_fig.data[1].marker.size = map.flatten()\n",
        "      example_fig.data[2].marker.size = map.flatten()\n",
        "      example_fig.data[3].marker.size = map.flatten()\n",
        "      example_fig.data[4].marker.size = map.flatten()\n",
        "\n",
        "  elif toggle_button.value == \"Pred\":\n",
        "\n",
        "    map = np.where(np.logical_and(example_pred>=pred_tmp_slider.value[0],example_pred<=pred_tmp_slider.value[1]), 8, 0)\n",
        "    with example_fig.batch_update():\n",
        "      example_fig.data[0].marker.size = map.flatten()\n",
        "      example_fig.data[1].marker.size = map.flatten()\n",
        "      example_fig.data[2].marker.size = map.flatten()\n",
        "      example_fig.data[3].marker.size = map.flatten()\n",
        "      example_fig.data[4].marker.size = map.flatten()\n",
        "\n",
        "  with output_plot:\n",
        "    example_fig.show()\n",
        "\n",
        "def slider_function(b):\n",
        "  md_tmp_slider.disabled = (toggle_button.value == 'Pred')\n",
        "  pred_tmp_slider.disabled = (toggle_button.value == 'MD')\n",
        "\n",
        "# GETTING LABELS FOR SYSTEMS\n",
        "systems = [x[0] for x in os.walk('results')]\n",
        "systems.remove('results')\n",
        "systems.remove('results/train')\n",
        "systems.remove('results/validation')\n",
        "systems = sorted(['/'.join(x.split('/')[1:]) for x in systems])\n",
        "\n",
        "# WIDGETS\n",
        "main_dropdown = widgets.Dropdown(options=systems, description = \"System: \", continuous_update=False)\n",
        "toggle_button = widgets.ToggleButtons(options=['MD', 'Pred'], description='Filter by:', button_style='')\n",
        "md_tmp_slider = widgets.FloatRangeSlider(value=[0, 1], min=0, max=1, description='MD Temp:', disabled=False, continuous_update=False)\n",
        "pred_tmp_slider = widgets.FloatRangeSlider(value=[0, 1], min=0, max=1, description='CNN Temp:', disabled=True, continuous_update=False)\n",
        "generate_button = widgets.Button(description='Generate', button_style='success', icon='fa-hand-pointer-o')\n",
        "\n",
        "output_plot = widgets.Output()\n",
        "\n",
        "# OBSERVERS\n",
        "main_dropdown.observe(populate_sliders, names='value')\n",
        "toggle_button.observe(slider_function)\n",
        "generate_button.on_click(display_fullplot)\n",
        "\n",
        "controls_box =widgets.VBox([main_dropdown, toggle_button, md_tmp_slider, pred_tmp_slider, generate_button], layout=widgets.Layout(width='100%', display='flex', flex_flow='column', align_items='flex-start'))\n",
        "frame_box = widgets.HBox([controls_box, output_plot])\n",
        "\n",
        "display(frame_box)"
      ],
      "metadata": {
        "id": "-sPms_uLlel7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ru-M1kbxIO5a"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "pbx-local",
      "language": "python",
      "name": "pbx-local"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15"
    },
    "vscode": {
      "interpreter": {
        "hash": "f806901b044f1f905f6f73d9d34ac86b2be2e087ac41c051b6f31285dd315984"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}