{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5Nd2jeT8r8uu"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "import math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "from plotly.io import write_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "V08TFba6G_nw"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import initializers\n",
        "from keras.layers import Input, Dropout, BatchNormalization, Conv3DTranspose, concatenate, Dense, Conv3D, Flatten, MaxPooling3D\n",
        "from keras.models import Sequential\n",
        "import keras.backend as K\n",
        "tf.keras.utils.set_random_seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaleido"
      ],
      "metadata": {
        "id": "M79B0dH7ftZW",
        "outputId": "51a64d4c-81f2-41be-817d-bc34f891acb2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: kaleido in /usr/local/lib/python3.7/dist-packages (0.2.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "KnH0b5QqG_ny",
        "outputId": "cfdf2f31-25b2-412e-8dc8-c08fed009c64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  1\n"
          ]
        }
      ],
      "source": [
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
        "os.chdir(\"CNN_PBX_Model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "KZxqA1mmG_nz",
        "outputId": "fc85be28-1aaf-45c9-b3be-b94772568ce8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 16, 34, 34, 3)\n",
            "(64, 16, 32, 32, 1)\n"
          ]
        }
      ],
      "source": [
        "# TRAINING DATA\n",
        "\n",
        "paths = [x[0] for x in os.walk('train/')][1:]\n",
        "\n",
        "train_data = []\n",
        "train_labels = []\n",
        "\n",
        "for i in paths:\n",
        "  train_ex = np.load(i + \"/input.npy\")\n",
        "\n",
        "  if train_ex.shape != (16,34,34):\n",
        "\n",
        "    first_axis_pad = int(0.5 * (34 - train_ex.shape[1]))\n",
        "    second_axis_pad = int(0.5 * (34 - train_ex.shape[2]))\n",
        "\n",
        "    train_ex = np.pad(train_ex, ((0, 0), (first_axis_pad, first_axis_pad), (second_axis_pad, second_axis_pad), (0,0)), 'wrap')\n",
        "\n",
        "  train_lb = np.load(i + \"/output.npy\")\n",
        "  train_data.append(train_ex)\n",
        "  train_labels.append(train_lb)\n",
        "\n",
        "train_data = np.array(train_data)\n",
        "train_labels = np.array(train_labels) / 1000\n",
        "\n",
        "print(train_data.shape)\n",
        "print(train_labels.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "02ORxH_vZNOC",
        "outputId": "d35001fc-8b08-4a5e-e1b1-848a7f0f16b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(28, 16, 34, 34, 3)\n",
            "(28, 16, 32, 32, 1)\n"
          ]
        }
      ],
      "source": [
        "# VALIDATION DATA\n",
        "\n",
        "validation_paths = [x[0] for x in os.walk('validation/')][1:]\n",
        "\n",
        "validation_data = []\n",
        "validation_labels = []\n",
        "\n",
        "for i in validation_paths:\n",
        "  validation_ex= np.load(i + \"/input.npy\")\n",
        "\n",
        "  if validation_ex.shape != (16,34,34):\n",
        "\n",
        "    first_axis_pad = int(0.5 * (34 - validation_ex.shape[1]))\n",
        "    second_axis_pad = int(0.5 * (34 - validation_ex.shape[2]))\n",
        "\n",
        "    validation_ex = np.pad(validation_ex, ((0, 0), (first_axis_pad, first_axis_pad), (second_axis_pad, second_axis_pad), (0,0)), 'wrap')\n",
        "\n",
        "\n",
        "  validation_lb = np.load(i + \"/output.npy\")\n",
        "  validation_data.append(validation_ex)\n",
        "  validation_labels.append(validation_lb)\n",
        "\n",
        "validation_data = np.array(validation_data)\n",
        "validation_labels = np.array(validation_labels) / 1000\n",
        "\n",
        "print(validation_data.shape)\n",
        "print(validation_labels.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "fWvLsqQXsWGy"
      },
      "outputs": [],
      "source": [
        "def periodic_padding_flexible(tensor, axis, padding=1):\n",
        "\n",
        "    if isinstance(axis,int):\n",
        "        axis = (axis,)\n",
        "    if isinstance(padding,int):\n",
        "        padding = (padding,)\n",
        "\n",
        "    ndim = len(tensor.shape)\n",
        "\n",
        "    for ax,p in zip(axis,padding):\n",
        "        # create a slice object that selects everything from all axes,\n",
        "        # except only 0:p for the specified for right, and -p: for left\n",
        "\n",
        "        ind_right = [slice(-p,None) if i == ax else slice(None) for i in range(ndim)]\n",
        "        ind_left = [slice(0, p) if i == ax else slice(None) for i in range(ndim)]\n",
        "        right = tensor[ind_right]\n",
        "        left = tensor[ind_left]\n",
        "        middle = tensor\n",
        "        tensor = tf.concat([right,middle,left], axis=ax)\n",
        "\n",
        "    return tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "rLhZ3NIocWJ9"
      },
      "outputs": [],
      "source": [
        "def DownConvBlock(inputs, n_filters=32, filter_size = 3, max_pooling=True, special_padding=False):\n",
        "\n",
        "  padding_size = int((filter_size-1)/2)\n",
        "  kernel_init =   tf.keras.initializers.GlorotUniform(seed=0)\n",
        "  bias_init = tf.keras.initializers.Zeros()\n",
        "\n",
        "\n",
        "  # PERIODIC PADDING\n",
        "\n",
        "  inputs = periodic_padding_flexible(inputs, axis=1,padding=padding_size)\n",
        "  if special_padding == False:\n",
        "    inputs = periodic_padding_flexible(inputs, axis=2,padding=padding_size)\n",
        "    inputs = periodic_padding_flexible(inputs, axis=3,padding=padding_size)\n",
        "  \n",
        "  conv = Conv3D(n_filters, filter_size, activation='relu', padding='valid', kernel_initializer=kernel_init, bias_initializer=bias_init)(inputs)\n",
        "  print(conv.shape)\n",
        "  conv = periodic_padding_flexible(conv, axis=(1,2,3),padding=(padding_size,padding_size,padding_size))\n",
        "  conv = Conv3D(n_filters, filter_size, activation='relu', padding='valid', kernel_initializer=kernel_init, bias_initializer=bias_init)(conv)\n",
        "  print(conv.shape)\n",
        "  conv = BatchNormalization()(conv, training=False)\n",
        "      \n",
        "  if max_pooling:\n",
        "    next_layer = tf.keras.layers.MaxPooling3D(pool_size = (2,2,2))(conv)\n",
        "  else:\n",
        "    next_layer = conv\n",
        "  \n",
        "  skip_connection = conv   \n",
        "\n",
        "  print(\"end_of_block\") \n",
        "  return next_layer, skip_connection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "YvMNFlsHseOF"
      },
      "outputs": [],
      "source": [
        "def UpConvBlock(prev_layer_input, skip_layer_input, filter_size = 3, n_filters=32):\n",
        "\n",
        "    padding_size = int((filter_size-1)/2)\n",
        "    kernel_init = tf.keras.initializers.GlorotUniform(seed=0)\n",
        "    bias_init = tf.keras.initializers.Zeros()   \n",
        "\n",
        "    up = Conv3DTranspose(n_filters, (filter_size,filter_size,filter_size),\n",
        "                         strides=(filter_size-1,filter_size-1,filter_size-1),\n",
        "                         padding='same', kernel_initializer=kernel_init, bias_initializer=bias_init)(prev_layer_input)\n",
        "\n",
        "    merge = concatenate([up, skip_layer_input], axis=4)\n",
        "    merge = periodic_padding_flexible(merge, axis=(1,2,3),padding=(padding_size,padding_size,padding_size))\n",
        "    conv = Conv3D(n_filters, filter_size, activation='relu', padding='valid', kernel_initializer=kernel_init,  bias_initializer=bias_init)(merge)\n",
        "    print(conv.shape)\n",
        "    conv = periodic_padding_flexible(conv, axis=(1,2,3),padding=(padding_size,padding_size,padding_size))\n",
        "    conv = Conv3D(n_filters, filter_size, activation='relu',padding='valid', kernel_initializer=kernel_init,  bias_initializer=bias_init)(conv)\n",
        "    print(conv.shape)\n",
        "\n",
        "    print(\"end_of_block\")\n",
        "    return conv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "fe-fUh99sgKg"
      },
      "outputs": [],
      "source": [
        "\n",
        "def UNet3DModel(input_size=(16, 34, 34, 3), n_filters=32, filter_size=3, n_classes=1):\n",
        "  kernel_init =  tf.keras.initializers.GlorotUniform(seed=0)\n",
        "  bias_init = tf.keras.initializers.Zeros()  \n",
        "  \n",
        "  inputs = Input(input_size)\n",
        "  print(\"Inputs\", inputs.shape)\n",
        "\n",
        "  cblock0 = DownConvBlock(inputs,     n_filters = n_filters    , filter_size = filter_size, max_pooling=False, special_padding=True)\n",
        "  print(\"CB0\", cblock0[0].shape)\n",
        "\n",
        "  cblock1 = DownConvBlock(inputs,     n_filters = n_filters    , filter_size = filter_size, max_pooling=True, special_padding=True)\n",
        "  print(\"CB1\", cblock1[0].shape)\n",
        "\n",
        "  cblock2 = DownConvBlock(cblock1[0], n_filters = n_filters*2  , filter_size = filter_size, max_pooling=True, special_padding=False)\n",
        "  print(\"CB2\", cblock2[0].shape)\n",
        "    \n",
        "  cblock3 = DownConvBlock(cblock2[0], n_filters = n_filters*4  , filter_size = filter_size, max_pooling=True, special_padding=False)\n",
        "  print(\"CB3\", cblock3[0].shape)\n",
        "  \n",
        "  cblock4 = DownConvBlock(cblock3[0], n_filters = n_filters*8  , filter_size = filter_size, max_pooling=False, special_padding=False)\n",
        "  print(\"CB4\", cblock4[0].shape)\n",
        "\n",
        "\n",
        "  print(\"------------------\")\n",
        "\n",
        "  ublock7 = UpConvBlock(cblock4[0]   , cblock3[1],  n_filters = n_filters * 4, filter_size = filter_size)\n",
        "  print(\"UB7\", ublock7.shape)\n",
        "  \n",
        "  ublock8 = UpConvBlock(ublock7   , cblock2[1],  n_filters = n_filters * 2, filter_size = filter_size)\n",
        "  print(\"UB8\", ublock8.shape)\n",
        "  \n",
        "  ublock9 = UpConvBlock(ublock8   , cblock1[1],  n_filters = n_filters, filter_size = filter_size)\n",
        "  print(\"UB9\", ublock9.shape)\n",
        "\n",
        "  ublock9 = periodic_padding_flexible(ublock9, axis=(1,2,3),padding=(1,1,1))\n",
        "  \n",
        "  conv9 = Conv3D(n_filters, 3, activation='relu', padding='valid', kernel_initializer=kernel_init,  bias_initializer=bias_init)(ublock9)\n",
        "  print(\"C9\", conv9.shape)\n",
        "  \n",
        "  conv10 = Conv3D(n_classes, 1, padding='same', kernel_initializer=kernel_init,  bias_initializer=bias_init)(conv9)\n",
        "  print(\"C10\", conv10.shape)\n",
        "\n",
        "  model = tf.keras.Model(inputs=inputs, outputs=conv10)  \n",
        "\n",
        "  return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "d2a5FVUYPHTB"
      },
      "outputs": [],
      "source": [
        "\n",
        "def custom_mse(y_true,y_pred):\n",
        "    w_hot = 5.0\n",
        "    w_cold = 1.0\n",
        "    cutoff = 1.8\n",
        "    weightmat = tf.cast(tf.where(tf.greater(y_true, cutoff), w_hot, w_cold),float)\n",
        "    loss = tf.cast(K.square(y_pred - y_true),float)\n",
        "    loss = loss*weightmat\n",
        "    loss = K.mean(loss)\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "R3SJtgKps6tc",
        "outputId": "c2b8e4b5-59ce-4f0c-9530-1bc50b50b6a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs (None, 16, 34, 34, 3)\n",
            "(None, 16, 32, 32, 32)\n",
            "(None, 16, 32, 32, 32)\n",
            "end_of_block\n",
            "CB0 (None, 16, 32, 32, 32)\n",
            "(None, 16, 32, 32, 32)\n",
            "(None, 16, 32, 32, 32)\n",
            "end_of_block\n",
            "CB1 (None, 8, 16, 16, 32)\n",
            "(None, 8, 16, 16, 64)\n",
            "(None, 8, 16, 16, 64)\n",
            "end_of_block\n",
            "CB2 (None, 4, 8, 8, 64)\n",
            "(None, 4, 8, 8, 128)\n",
            "(None, 4, 8, 8, 128)\n",
            "end_of_block\n",
            "CB3 (None, 2, 4, 4, 128)\n",
            "(None, 2, 4, 4, 256)\n",
            "(None, 2, 4, 4, 256)\n",
            "end_of_block\n",
            "CB4 (None, 2, 4, 4, 256)\n",
            "------------------\n",
            "(None, 4, 8, 8, 128)\n",
            "(None, 4, 8, 8, 128)\n",
            "end_of_block\n",
            "UB7 (None, 4, 8, 8, 128)\n",
            "(None, 8, 16, 16, 64)\n",
            "(None, 8, 16, 16, 64)\n",
            "end_of_block\n",
            "UB8 (None, 8, 16, 16, 64)\n",
            "(None, 16, 32, 32, 32)\n",
            "(None, 16, 32, 32, 32)\n",
            "end_of_block\n",
            "UB9 (None, 16, 32, 32, 32)\n",
            "C9 (None, 16, 32, 32, 32)\n",
            "C10 (None, 16, 32, 32, 1)\n"
          ]
        }
      ],
      "source": [
        "model = UNet3DModel(input_size=(16, 34, 34, 3), n_filters=32, filter_size = 3, n_classes=1)\n",
        "optimizer = tf.optimizers.Adam(learning_rate = 0.0005)\n",
        "model.compile(loss=custom_mse, optimizer=optimizer, metrics=['mse'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "baIEkfpntuOW",
        "outputId": "1b294c78-8497-4253-9f6a-4383987a02a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "2/2 [==============================] - 18s 3s/step - loss: 1.7163 - mse: 1.2085 - val_loss: 0.9396 - val_mse: 0.6231 - lr: 5.0000e-04\n",
            "Epoch 2/1000\n",
            "2/2 [==============================] - 1s 732ms/step - loss: 0.7435 - mse: 0.4189 - val_loss: 18.3230 - val_mse: 17.6340 - lr: 5.0000e-04\n",
            "Epoch 3/1000\n",
            "2/2 [==============================] - 1s 684ms/step - loss: 9.4486 - mse: 8.9766 - val_loss: 0.6523 - val_mse: 0.3868 - lr: 5.0000e-04\n",
            "Epoch 4/1000\n",
            "2/2 [==============================] - 1s 656ms/step - loss: 0.8536 - mse: 0.4902 - val_loss: 0.9399 - val_mse: 0.6220 - lr: 5.0000e-04\n",
            "Epoch 5/1000\n",
            "2/2 [==============================] - 1s 653ms/step - loss: 1.0751 - mse: 0.6679 - val_loss: 1.0296 - val_mse: 0.6972 - lr: 5.0000e-04\n",
            "Epoch 6/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 1.1499 - mse: 0.7294 - val_loss: 1.0715 - val_mse: 0.7327 - lr: 5.0000e-04\n",
            "Epoch 7/1000\n",
            "2/2 [==============================] - 1s 657ms/step - loss: 1.1845 - mse: 0.7581 - val_loss: 1.0888 - val_mse: 0.7476 - lr: 5.0000e-04\n",
            "Epoch 8/1000\n",
            "2/2 [==============================] - 1s 654ms/step - loss: 1.1967 - mse: 0.7686 - val_loss: 1.0891 - val_mse: 0.7481 - lr: 5.0000e-04\n",
            "Epoch 9/1000\n",
            "2/2 [==============================] - 1s 657ms/step - loss: 1.1930 - mse: 0.7661 - val_loss: 1.0767 - val_mse: 0.7380 - lr: 5.0000e-04\n",
            "Epoch 10/1000\n",
            "2/2 [==============================] - 1s 669ms/step - loss: 1.1769 - mse: 0.7536 - val_loss: 1.0532 - val_mse: 0.7185 - lr: 5.0000e-04\n",
            "Epoch 11/1000\n",
            "2/2 [==============================] - 1s 673ms/step - loss: 1.1494 - mse: 0.7316 - val_loss: 1.0160 - val_mse: 0.6874 - lr: 5.0000e-04\n",
            "Epoch 12/1000\n",
            "2/2 [==============================] - 1s 655ms/step - loss: 1.1067 - mse: 0.6970 - val_loss: 0.9580 - val_mse: 0.6389 - lr: 5.0000e-04\n",
            "Epoch 13/1000\n",
            "2/2 [==============================] - 1s 657ms/step - loss: 1.0403 - mse: 0.6432 - val_loss: 0.8618 - val_mse: 0.5589 - lr: 5.0000e-04\n",
            "Epoch 14/1000\n",
            "2/2 [==============================] - 1s 683ms/step - loss: 0.9226 - mse: 0.5480 - val_loss: 0.6340 - val_mse: 0.3728 - lr: 5.0000e-04\n",
            "Epoch 15/1000\n",
            "2/2 [==============================] - 1s 659ms/step - loss: 0.5990 - mse: 0.3026 - val_loss: 2.6060 - val_mse: 2.5456 - lr: 5.0000e-04\n",
            "Epoch 16/1000\n",
            "2/2 [==============================] - 1s 690ms/step - loss: 1.5652 - mse: 1.4010 - val_loss: 0.5876 - val_mse: 0.3332 - lr: 5.0000e-04\n",
            "Epoch 17/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.6990 - mse: 0.3700 - val_loss: 0.6523 - val_mse: 0.3880 - lr: 5.0000e-04\n",
            "Epoch 18/1000\n",
            "2/2 [==============================] - 1s 654ms/step - loss: 0.7600 - mse: 0.4197 - val_loss: 0.6632 - val_mse: 0.3966 - lr: 5.0000e-04\n",
            "Epoch 19/1000\n",
            "2/2 [==============================] - 1s 660ms/step - loss: 0.7513 - mse: 0.4125 - val_loss: 0.6138 - val_mse: 0.3565 - lr: 5.0000e-04\n",
            "Epoch 20/1000\n",
            "2/2 [==============================] - 1s 685ms/step - loss: 0.6986 - mse: 0.3714 - val_loss: 0.5535 - val_mse: 0.3084 - lr: 5.0000e-04\n",
            "Epoch 21/1000\n",
            "2/2 [==============================] - 1s 703ms/step - loss: 0.6336 - mse: 0.3213 - val_loss: 0.4726 - val_mse: 0.2456 - lr: 5.0000e-04\n",
            "Epoch 22/1000\n",
            "2/2 [==============================] - 1s 692ms/step - loss: 0.5447 - mse: 0.2561 - val_loss: 0.3822 - val_mse: 0.1791 - lr: 5.0000e-04\n",
            "Epoch 23/1000\n",
            "2/2 [==============================] - 1s 696ms/step - loss: 0.4512 - mse: 0.1918 - val_loss: 0.2981 - val_mse: 0.1223 - lr: 5.0000e-04\n",
            "Epoch 24/1000\n",
            "2/2 [==============================] - 1s 691ms/step - loss: 0.3651 - mse: 0.1403 - val_loss: 0.2377 - val_mse: 0.0928 - lr: 5.0000e-04\n",
            "Epoch 25/1000\n",
            "2/2 [==============================] - 1s 692ms/step - loss: 0.3103 - mse: 0.1223 - val_loss: 0.2277 - val_mse: 0.1138 - lr: 5.0000e-04\n",
            "Epoch 26/1000\n",
            "2/2 [==============================] - 1s 677ms/step - loss: 0.3044 - mse: 0.1534 - val_loss: 0.2681 - val_mse: 0.1784 - lr: 5.0000e-04\n",
            "Epoch 27/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.3435 - mse: 0.2175 - val_loss: 0.3031 - val_mse: 0.2244 - lr: 5.0000e-04\n",
            "Epoch 28/1000\n",
            "2/2 [==============================] - 1s 679ms/step - loss: 0.3609 - mse: 0.2452 - val_loss: 0.2847 - val_mse: 0.2026 - lr: 5.0000e-04\n",
            "Epoch 29/1000\n",
            "2/2 [==============================] - 1s 660ms/step - loss: 0.3347 - mse: 0.2125 - val_loss: 0.2430 - val_mse: 0.1479 - lr: 5.0000e-04\n",
            "Epoch 30/1000\n",
            "2/2 [==============================] - 1s 691ms/step - loss: 0.2987 - mse: 0.1604 - val_loss: 0.2174 - val_mse: 0.1062 - lr: 5.0000e-04\n",
            "Epoch 31/1000\n",
            "2/2 [==============================] - 1s 707ms/step - loss: 0.2788 - mse: 0.1243 - val_loss: 0.2121 - val_mse: 0.0904 - lr: 5.0000e-04\n",
            "Epoch 32/1000\n",
            "2/2 [==============================] - 1s 701ms/step - loss: 0.2757 - mse: 0.1127 - val_loss: 0.2112 - val_mse: 0.0889 - lr: 5.0000e-04\n",
            "Epoch 33/1000\n",
            "2/2 [==============================] - 1s 701ms/step - loss: 0.2720 - mse: 0.1111 - val_loss: 0.2069 - val_mse: 0.0891 - lr: 5.0000e-04\n",
            "Epoch 34/1000\n",
            "2/2 [==============================] - 1s 710ms/step - loss: 0.2642 - mse: 0.1110 - val_loss: 0.2011 - val_mse: 0.0884 - lr: 5.0000e-04\n",
            "Epoch 35/1000\n",
            "2/2 [==============================] - 1s 693ms/step - loss: 0.2540 - mse: 0.1092 - val_loss: 0.1952 - val_mse: 0.0866 - lr: 5.0000e-04\n",
            "Epoch 36/1000\n",
            "2/2 [==============================] - 1s 699ms/step - loss: 0.2437 - mse: 0.1037 - val_loss: 0.1908 - val_mse: 0.0819 - lr: 5.0000e-04\n",
            "Epoch 37/1000\n",
            "2/2 [==============================] - 1s 696ms/step - loss: 0.2361 - mse: 0.0991 - val_loss: 0.1876 - val_mse: 0.0811 - lr: 5.0000e-04\n",
            "Epoch 38/1000\n",
            "2/2 [==============================] - 1s 696ms/step - loss: 0.2288 - mse: 0.0968 - val_loss: 0.1831 - val_mse: 0.0798 - lr: 5.0000e-04\n",
            "Epoch 39/1000\n",
            "2/2 [==============================] - 1s 699ms/step - loss: 0.2200 - mse: 0.0950 - val_loss: 0.1798 - val_mse: 0.0824 - lr: 5.0000e-04\n",
            "Epoch 40/1000\n",
            "2/2 [==============================] - 1s 703ms/step - loss: 0.2132 - mse: 0.0967 - val_loss: 0.1776 - val_mse: 0.0844 - lr: 5.0000e-04\n",
            "Epoch 41/1000\n",
            "2/2 [==============================] - 1s 702ms/step - loss: 0.2073 - mse: 0.0962 - val_loss: 0.1741 - val_mse: 0.0806 - lr: 5.0000e-04\n",
            "Epoch 42/1000\n",
            "2/2 [==============================] - 1s 697ms/step - loss: 0.2004 - mse: 0.0914 - val_loss: 0.1709 - val_mse: 0.0775 - lr: 5.0000e-04\n",
            "Epoch 43/1000\n",
            "2/2 [==============================] - 1s 699ms/step - loss: 0.1936 - mse: 0.0872 - val_loss: 0.1677 - val_mse: 0.0758 - lr: 5.0000e-04\n",
            "Epoch 44/1000\n",
            "2/2 [==============================] - 1s 705ms/step - loss: 0.1866 - mse: 0.0850 - val_loss: 0.1647 - val_mse: 0.0763 - lr: 5.0000e-04\n",
            "Epoch 45/1000\n",
            "2/2 [==============================] - 1s 703ms/step - loss: 0.1800 - mse: 0.0859 - val_loss: 0.1620 - val_mse: 0.0772 - lr: 5.0000e-04\n",
            "Epoch 46/1000\n",
            "2/2 [==============================] - 1s 701ms/step - loss: 0.1729 - mse: 0.0834 - val_loss: 0.1589 - val_mse: 0.0722 - lr: 5.0000e-04\n",
            "Epoch 47/1000\n",
            "2/2 [==============================] - 1s 708ms/step - loss: 0.1656 - mse: 0.0773 - val_loss: 0.1563 - val_mse: 0.0704 - lr: 5.0000e-04\n",
            "Epoch 48/1000\n",
            "2/2 [==============================] - 1s 706ms/step - loss: 0.1587 - mse: 0.0749 - val_loss: 0.1537 - val_mse: 0.0711 - lr: 5.0000e-04\n",
            "Epoch 49/1000\n",
            "2/2 [==============================] - 1s 714ms/step - loss: 0.1517 - mse: 0.0748 - val_loss: 0.1514 - val_mse: 0.0708 - lr: 5.0000e-04\n",
            "Epoch 50/1000\n",
            "2/2 [==============================] - 1s 710ms/step - loss: 0.1452 - mse: 0.0728 - val_loss: 0.1497 - val_mse: 0.0688 - lr: 5.0000e-04\n",
            "Epoch 51/1000\n",
            "2/2 [==============================] - 1s 710ms/step - loss: 0.1385 - mse: 0.0680 - val_loss: 0.1486 - val_mse: 0.0682 - lr: 5.0000e-04\n",
            "Epoch 52/1000\n",
            "2/2 [==============================] - 1s 722ms/step - loss: 0.1323 - mse: 0.0662 - val_loss: 0.1481 - val_mse: 0.0708 - lr: 5.0000e-04\n",
            "Epoch 53/1000\n",
            "2/2 [==============================] - 1s 683ms/step - loss: 0.1261 - mse: 0.0663 - val_loss: 0.1510 - val_mse: 0.0720 - lr: 5.0000e-04\n",
            "Epoch 54/1000\n",
            "2/2 [==============================] - 1s 685ms/step - loss: 0.1209 - mse: 0.0650 - val_loss: 0.1499 - val_mse: 0.0761 - lr: 5.0000e-04\n",
            "Epoch 55/1000\n",
            "2/2 [==============================] - 1s 684ms/step - loss: 0.1160 - mse: 0.0669 - val_loss: 0.1569 - val_mse: 0.0797 - lr: 5.0000e-04\n",
            "Epoch 56/1000\n",
            "2/2 [==============================] - 1s 718ms/step - loss: 0.1135 - mse: 0.0658 - val_loss: 0.1463 - val_mse: 0.0766 - lr: 5.0000e-04\n",
            "Epoch 57/1000\n",
            "2/2 [==============================] - 1s 683ms/step - loss: 0.1077 - mse: 0.0620 - val_loss: 0.1503 - val_mse: 0.0733 - lr: 5.0000e-04\n",
            "Epoch 58/1000\n",
            "2/2 [==============================] - 1s 720ms/step - loss: 0.1049 - mse: 0.0576 - val_loss: 0.1455 - val_mse: 0.0747 - lr: 5.0000e-04\n",
            "Epoch 59/1000\n",
            "2/2 [==============================] - 1s 687ms/step - loss: 0.1019 - mse: 0.0617 - val_loss: 0.1453 - val_mse: 0.0758 - lr: 5.0000e-04\n",
            "Epoch 60/1000\n",
            "2/2 [==============================] - 1s 686ms/step - loss: 0.0988 - mse: 0.0589 - val_loss: 0.1495 - val_mse: 0.0756 - lr: 5.0000e-04\n",
            "Epoch 61/1000\n",
            "2/2 [==============================] - 1s 692ms/step - loss: 0.0966 - mse: 0.0564 - val_loss: 0.1460 - val_mse: 0.0775 - lr: 5.0000e-04\n",
            "Epoch 62/1000\n",
            "2/2 [==============================] - 1s 702ms/step - loss: 0.0948 - mse: 0.0606 - val_loss: 0.1467 - val_mse: 0.0782 - lr: 5.0000e-04\n",
            "Epoch 63/1000\n",
            "2/2 [==============================] - 1s 688ms/step - loss: 0.0929 - mse: 0.0574 - val_loss: 0.1506 - val_mse: 0.0784 - lr: 5.0000e-04\n",
            "Epoch 64/1000\n",
            "2/2 [==============================] - 1s 693ms/step - loss: 0.0913 - mse: 0.0562 - val_loss: 0.1464 - val_mse: 0.0797 - lr: 5.0000e-04\n",
            "Epoch 65/1000\n",
            "2/2 [==============================] - 1s 690ms/step - loss: 0.0902 - mse: 0.0596 - val_loss: 0.1482 - val_mse: 0.0790 - lr: 5.0000e-04\n",
            "Epoch 66/1000\n",
            "2/2 [==============================] - 1s 685ms/step - loss: 0.0888 - mse: 0.0552 - val_loss: 0.1502 - val_mse: 0.0790 - lr: 5.0000e-04\n",
            "Epoch 67/1000\n",
            "2/2 [==============================] - 1s 695ms/step - loss: 0.0877 - mse: 0.0559 - val_loss: 0.1455 - val_mse: 0.0796 - lr: 5.0000e-04\n",
            "Epoch 68/1000\n",
            "2/2 [==============================] - 1s 689ms/step - loss: 0.0864 - mse: 0.0572 - val_loss: 0.1487 - val_mse: 0.0787 - lr: 5.0000e-04\n",
            "Epoch 69/1000\n",
            "2/2 [==============================] - 1s 690ms/step - loss: 0.0861 - mse: 0.0536 - val_loss: 0.1473 - val_mse: 0.0784 - lr: 5.0000e-04\n",
            "Epoch 70/1000\n",
            "2/2 [==============================] - 1s 727ms/step - loss: 0.0836 - mse: 0.0540 - val_loss: 0.1432 - val_mse: 0.0791 - lr: 5.0000e-04\n",
            "Epoch 71/1000\n",
            "2/2 [==============================] - 1s 698ms/step - loss: 0.0840 - mse: 0.0574 - val_loss: 0.1474 - val_mse: 0.0774 - lr: 5.0000e-04\n",
            "Epoch 72/1000\n",
            "2/2 [==============================] - 1s 697ms/step - loss: 0.0822 - mse: 0.0507 - val_loss: 0.1478 - val_mse: 0.0770 - lr: 5.0000e-04\n",
            "Epoch 73/1000\n",
            "2/2 [==============================] - 1s 728ms/step - loss: 0.0808 - mse: 0.0508 - val_loss: 0.1411 - val_mse: 0.0774 - lr: 5.0000e-04\n",
            "Epoch 74/1000\n",
            "2/2 [==============================] - 1s 700ms/step - loss: 0.0804 - mse: 0.0539 - val_loss: 0.1470 - val_mse: 0.0773 - lr: 5.0000e-04\n",
            "Epoch 75/1000\n",
            "2/2 [==============================] - 1s 703ms/step - loss: 0.0800 - mse: 0.0491 - val_loss: 0.1422 - val_mse: 0.0756 - lr: 5.0000e-04\n",
            "Epoch 76/1000\n",
            "2/2 [==============================] - 1s 823ms/step - loss: 0.0781 - mse: 0.0516 - val_loss: 0.1393 - val_mse: 0.0750 - lr: 5.0000e-04\n",
            "Epoch 77/1000\n",
            "2/2 [==============================] - 1s 780ms/step - loss: 0.0777 - mse: 0.0503 - val_loss: 0.1441 - val_mse: 0.0744 - lr: 5.0000e-04\n",
            "Epoch 78/1000\n",
            "2/2 [==============================] - 1s 790ms/step - loss: 0.0768 - mse: 0.0479 - val_loss: 0.1392 - val_mse: 0.0743 - lr: 5.0000e-04\n",
            "Epoch 79/1000\n",
            "2/2 [==============================] - 1s 705ms/step - loss: 0.0757 - mse: 0.0499 - val_loss: 0.1406 - val_mse: 0.0743 - lr: 5.0000e-04\n",
            "Epoch 80/1000\n",
            "2/2 [==============================] - 1s 706ms/step - loss: 0.0745 - mse: 0.0479 - val_loss: 0.1419 - val_mse: 0.0742 - lr: 5.0000e-04\n",
            "Epoch 81/1000\n",
            "2/2 [==============================] - 1s 702ms/step - loss: 0.0740 - mse: 0.0474 - val_loss: 0.1394 - val_mse: 0.0736 - lr: 5.0000e-04\n",
            "Epoch 82/1000\n",
            "2/2 [==============================] - 1s 705ms/step - loss: 0.0730 - mse: 0.0468 - val_loss: 0.1397 - val_mse: 0.0734 - lr: 5.0000e-04\n",
            "Epoch 83/1000\n",
            "2/2 [==============================] - 1s 706ms/step - loss: 0.0726 - mse: 0.0474 - val_loss: 0.1399 - val_mse: 0.0735 - lr: 5.0000e-04\n",
            "Epoch 84/1000\n",
            "2/2 [==============================] - 1s 743ms/step - loss: 0.0720 - mse: 0.0456 - val_loss: 0.1378 - val_mse: 0.0730 - lr: 5.0000e-04\n",
            "Epoch 85/1000\n",
            "2/2 [==============================] - 1s 739ms/step - loss: 0.0708 - mse: 0.0468 - val_loss: 0.1360 - val_mse: 0.0721 - lr: 5.0000e-04\n",
            "Epoch 86/1000\n",
            "2/2 [==============================] - 1s 714ms/step - loss: 0.0702 - mse: 0.0455 - val_loss: 0.1373 - val_mse: 0.0715 - lr: 5.0000e-04\n",
            "Epoch 87/1000\n",
            "2/2 [==============================] - 1s 740ms/step - loss: 0.0692 - mse: 0.0446 - val_loss: 0.1337 - val_mse: 0.0710 - lr: 5.0000e-04\n",
            "Epoch 88/1000\n",
            "2/2 [==============================] - 1s 712ms/step - loss: 0.0686 - mse: 0.0449 - val_loss: 0.1409 - val_mse: 0.0721 - lr: 5.0000e-04\n",
            "Epoch 89/1000\n",
            "2/2 [==============================] - 1s 738ms/step - loss: 0.0685 - mse: 0.0431 - val_loss: 0.1313 - val_mse: 0.0721 - lr: 5.0000e-04\n",
            "Epoch 90/1000\n",
            "2/2 [==============================] - 1s 704ms/step - loss: 0.0685 - mse: 0.0465 - val_loss: 0.1432 - val_mse: 0.0733 - lr: 5.0000e-04\n",
            "Epoch 91/1000\n",
            "2/2 [==============================] - 1s 707ms/step - loss: 0.0686 - mse: 0.0440 - val_loss: 0.1315 - val_mse: 0.0706 - lr: 5.0000e-04\n",
            "Epoch 92/1000\n",
            "2/2 [==============================] - 1s 701ms/step - loss: 0.0666 - mse: 0.0449 - val_loss: 0.1391 - val_mse: 0.0711 - lr: 5.0000e-04\n",
            "Epoch 93/1000\n",
            "2/2 [==============================] - 1s 714ms/step - loss: 0.0667 - mse: 0.0430 - val_loss: 0.1317 - val_mse: 0.0693 - lr: 5.0000e-04\n",
            "Epoch 94/1000\n",
            "2/2 [==============================] - 1s 714ms/step - loss: 0.0660 - mse: 0.0430 - val_loss: 0.1326 - val_mse: 0.0686 - lr: 5.0000e-04\n",
            "Epoch 95/1000\n",
            "2/2 [==============================] - 1s 708ms/step - loss: 0.0645 - mse: 0.0425 - val_loss: 0.1310 - val_mse: 0.0677 - lr: 5.0000e-04\n",
            "Epoch 96/1000\n",
            "2/2 [==============================] - 1s 708ms/step - loss: 0.0640 - mse: 0.0416 - val_loss: 0.1332 - val_mse: 0.0680 - lr: 5.0000e-04\n",
            "Epoch 97/1000\n",
            "2/2 [==============================] - 1s 733ms/step - loss: 0.0634 - mse: 0.0409 - val_loss: 0.1295 - val_mse: 0.0688 - lr: 5.0000e-04\n",
            "Epoch 98/1000\n",
            "2/2 [==============================] - 1s 696ms/step - loss: 0.0631 - mse: 0.0426 - val_loss: 0.1406 - val_mse: 0.0710 - lr: 5.0000e-04\n",
            "Epoch 99/1000\n",
            "2/2 [==============================] - 1s 727ms/step - loss: 0.0637 - mse: 0.0408 - val_loss: 0.1287 - val_mse: 0.0700 - lr: 5.0000e-04\n",
            "Epoch 100/1000\n",
            "2/2 [==============================] - 1s 692ms/step - loss: 0.0652 - mse: 0.0449 - val_loss: 0.1349 - val_mse: 0.0686 - lr: 5.0000e-04\n",
            "Epoch 101/1000\n",
            "2/2 [==============================] - 1s 723ms/step - loss: 0.0624 - mse: 0.0411 - val_loss: 0.1277 - val_mse: 0.0667 - lr: 5.0000e-04\n",
            "Epoch 102/1000\n",
            "2/2 [==============================] - 1s 692ms/step - loss: 0.0616 - mse: 0.0408 - val_loss: 0.1343 - val_mse: 0.0678 - lr: 5.0000e-04\n",
            "Epoch 103/1000\n",
            "2/2 [==============================] - 1s 705ms/step - loss: 0.0626 - mse: 0.0422 - val_loss: 0.1314 - val_mse: 0.0674 - lr: 5.0000e-04\n",
            "Epoch 104/1000\n",
            "2/2 [==============================] - 1s 718ms/step - loss: 0.0605 - mse: 0.0393 - val_loss: 0.1307 - val_mse: 0.0664 - lr: 5.0000e-04\n",
            "Epoch 105/1000\n",
            "2/2 [==============================] - 1s 704ms/step - loss: 0.0609 - mse: 0.0411 - val_loss: 0.1320 - val_mse: 0.0669 - lr: 5.0000e-04\n",
            "Epoch 106/1000\n",
            "2/2 [==============================] - 1s 694ms/step - loss: 0.0603 - mse: 0.0387 - val_loss: 0.1295 - val_mse: 0.0672 - lr: 5.0000e-04\n",
            "Epoch 107/1000\n",
            "2/2 [==============================] - 1s 692ms/step - loss: 0.0598 - mse: 0.0411 - val_loss: 0.1307 - val_mse: 0.0663 - lr: 5.0000e-04\n",
            "Epoch 108/1000\n",
            "2/2 [==============================] - 1s 714ms/step - loss: 0.0604 - mse: 0.0386 - val_loss: 0.1269 - val_mse: 0.0658 - lr: 5.0000e-04\n",
            "Epoch 109/1000\n",
            "2/2 [==============================] - 1s 694ms/step - loss: 0.0599 - mse: 0.0418 - val_loss: 0.1317 - val_mse: 0.0664 - lr: 5.0000e-04\n",
            "Epoch 110/1000\n",
            "2/2 [==============================] - 1s 694ms/step - loss: 0.0598 - mse: 0.0379 - val_loss: 0.1280 - val_mse: 0.0664 - lr: 5.0000e-04\n",
            "Epoch 111/1000\n",
            "2/2 [==============================] - 1s 686ms/step - loss: 0.0588 - mse: 0.0411 - val_loss: 0.1301 - val_mse: 0.0657 - lr: 5.0000e-04\n",
            "Epoch 112/1000\n",
            "2/2 [==============================] - 1s 687ms/step - loss: 0.0577 - mse: 0.0372 - val_loss: 0.1288 - val_mse: 0.0649 - lr: 5.0000e-04\n",
            "Epoch 113/1000\n",
            "2/2 [==============================] - 1s 687ms/step - loss: 0.0574 - mse: 0.0385 - val_loss: 0.1272 - val_mse: 0.0648 - lr: 5.0000e-04\n",
            "Epoch 114/1000\n",
            "2/2 [==============================] - 1s 691ms/step - loss: 0.0573 - mse: 0.0376 - val_loss: 0.1302 - val_mse: 0.0660 - lr: 5.0000e-04\n",
            "Epoch 115/1000\n",
            "2/2 [==============================] - 1s 695ms/step - loss: 0.0573 - mse: 0.0393 - val_loss: 0.1299 - val_mse: 0.0658 - lr: 5.0000e-04\n",
            "Epoch 116/1000\n",
            "2/2 [==============================] - 1s 688ms/step - loss: 0.0560 - mse: 0.0366 - val_loss: 0.1286 - val_mse: 0.0648 - lr: 5.0000e-04\n",
            "Epoch 117/1000\n",
            "2/2 [==============================] - 1s 687ms/step - loss: 0.0558 - mse: 0.0377 - val_loss: 0.1288 - val_mse: 0.0651 - lr: 5.0000e-04\n",
            "Epoch 118/1000\n",
            "2/2 [==============================] - 1s 692ms/step - loss: 0.0552 - mse: 0.0364 - val_loss: 0.1290 - val_mse: 0.0657 - lr: 5.0000e-04\n",
            "Epoch 119/1000\n",
            "2/2 [==============================] - 1s 698ms/step - loss: 0.0547 - mse: 0.0372 - val_loss: 0.1295 - val_mse: 0.0654 - lr: 5.0000e-04\n",
            "Epoch 120/1000\n",
            "2/2 [==============================] - 1s 726ms/step - loss: 0.0553 - mse: 0.0361 - val_loss: 0.1248 - val_mse: 0.0654 - lr: 5.0000e-04\n",
            "Epoch 121/1000\n",
            "2/2 [==============================] - 1s 696ms/step - loss: 0.0549 - mse: 0.0386 - val_loss: 0.1366 - val_mse: 0.0678 - lr: 5.0000e-04\n",
            "Epoch 122/1000\n",
            "2/2 [==============================] - 1s 689ms/step - loss: 0.0551 - mse: 0.0360 - val_loss: 0.1258 - val_mse: 0.0649 - lr: 5.0000e-04\n",
            "Epoch 123/1000\n",
            "2/2 [==============================] - 1s 691ms/step - loss: 0.0534 - mse: 0.0370 - val_loss: 0.1383 - val_mse: 0.0686 - lr: 5.0000e-04\n",
            "Epoch 124/1000\n",
            "2/2 [==============================] - 1s 690ms/step - loss: 0.0541 - mse: 0.0347 - val_loss: 0.1244 - val_mse: 0.0678 - lr: 5.0000e-04\n",
            "Epoch 125/1000\n",
            "2/2 [==============================] - 1s 700ms/step - loss: 0.0556 - mse: 0.0394 - val_loss: 0.1424 - val_mse: 0.0707 - lr: 5.0000e-04\n",
            "Epoch 126/1000\n",
            "2/2 [==============================] - 1s 747ms/step - loss: 0.0551 - mse: 0.0361 - val_loss: 0.1235 - val_mse: 0.0658 - lr: 5.0000e-04\n",
            "Epoch 127/1000\n",
            "2/2 [==============================] - 1s 693ms/step - loss: 0.0559 - mse: 0.0390 - val_loss: 0.1344 - val_mse: 0.0663 - lr: 5.0000e-04\n",
            "Epoch 128/1000\n",
            "2/2 [==============================] - 1s 694ms/step - loss: 0.0545 - mse: 0.0365 - val_loss: 0.1233 - val_mse: 0.0640 - lr: 5.0000e-04\n",
            "Epoch 129/1000\n",
            "2/2 [==============================] - 1s 700ms/step - loss: 0.0530 - mse: 0.0358 - val_loss: 0.1326 - val_mse: 0.0654 - lr: 5.0000e-04\n",
            "Epoch 130/1000\n",
            "2/2 [==============================] - 1s 691ms/step - loss: 0.0521 - mse: 0.0345 - val_loss: 0.1259 - val_mse: 0.0657 - lr: 5.0000e-04\n",
            "Epoch 131/1000\n",
            "2/2 [==============================] - 1s 705ms/step - loss: 0.0516 - mse: 0.0360 - val_loss: 0.1354 - val_mse: 0.0678 - lr: 5.0000e-04\n",
            "Epoch 132/1000\n",
            "2/2 [==============================] - 1s 699ms/step - loss: 0.0515 - mse: 0.0342 - val_loss: 0.1252 - val_mse: 0.0646 - lr: 5.0000e-04\n",
            "Epoch 133/1000\n",
            "2/2 [==============================] - 1s 698ms/step - loss: 0.0511 - mse: 0.0350 - val_loss: 0.1289 - val_mse: 0.0649 - lr: 5.0000e-04\n",
            "Epoch 134/1000\n",
            "2/2 [==============================] - 1s 688ms/step - loss: 0.0496 - mse: 0.0335 - val_loss: 0.1269 - val_mse: 0.0643 - lr: 5.0000e-04\n",
            "Epoch 135/1000\n",
            "2/2 [==============================] - 1s 692ms/step - loss: 0.0493 - mse: 0.0331 - val_loss: 0.1289 - val_mse: 0.0649 - lr: 5.0000e-04\n",
            "Epoch 136/1000\n",
            "2/2 [==============================] - 1s 696ms/step - loss: 0.0492 - mse: 0.0338 - val_loss: 0.1333 - val_mse: 0.0670 - lr: 5.0000e-04\n",
            "Epoch 137/1000\n",
            "2/2 [==============================] - 1s 693ms/step - loss: 0.0488 - mse: 0.0319 - val_loss: 0.1240 - val_mse: 0.0654 - lr: 5.0000e-04\n",
            "Epoch 138/1000\n",
            "2/2 [==============================] - 1s 700ms/step - loss: 0.0508 - mse: 0.0355 - val_loss: 0.1318 - val_mse: 0.0661 - lr: 5.0000e-04\n",
            "Epoch 139/1000\n",
            "2/2 [==============================] - 1s 695ms/step - loss: 0.0498 - mse: 0.0351 - val_loss: 0.1327 - val_mse: 0.0664 - lr: 5.0000e-04\n",
            "Epoch 140/1000\n",
            "2/2 [==============================] - 1s 695ms/step - loss: 0.0479 - mse: 0.0315 - val_loss: 0.1278 - val_mse: 0.0649 - lr: 5.0000e-04\n",
            "Epoch 141/1000\n",
            "2/2 [==============================] - 1s 700ms/step - loss: 0.0471 - mse: 0.0321 - val_loss: 0.1281 - val_mse: 0.0644 - lr: 5.0000e-04\n",
            "Epoch 142/1000\n",
            "2/2 [==============================] - 1s 695ms/step - loss: 0.0464 - mse: 0.0312 - val_loss: 0.1306 - val_mse: 0.0656 - lr: 5.0000e-04\n",
            "Epoch 143/1000\n",
            "2/2 [==============================] - 1s 701ms/step - loss: 0.0462 - mse: 0.0312 - val_loss: 0.1312 - val_mse: 0.0662 - lr: 5.0000e-04\n",
            "Epoch 144/1000\n",
            "2/2 [==============================] - 1s 707ms/step - loss: 0.0454 - mse: 0.0304 - val_loss: 0.1282 - val_mse: 0.0650 - lr: 5.0000e-04\n",
            "Epoch 145/1000\n",
            "2/2 [==============================] - 1s 699ms/step - loss: 0.0451 - mse: 0.0309 - val_loss: 0.1370 - val_mse: 0.0679 - lr: 5.0000e-04\n",
            "Epoch 146/1000\n",
            "2/2 [==============================] - 1s 698ms/step - loss: 0.0460 - mse: 0.0307 - val_loss: 0.1287 - val_mse: 0.0655 - lr: 5.0000e-04\n",
            "Epoch 147/1000\n",
            "2/2 [==============================] - 1s 701ms/step - loss: 0.0450 - mse: 0.0299 - val_loss: 0.1292 - val_mse: 0.0660 - lr: 5.0000e-04\n",
            "Epoch 148/1000\n",
            "2/2 [==============================] - 1s 704ms/step - loss: 0.0445 - mse: 0.0298 - val_loss: 0.1286 - val_mse: 0.0644 - lr: 5.0000e-04\n",
            "Epoch 149/1000\n",
            "2/2 [==============================] - 1s 699ms/step - loss: 0.0437 - mse: 0.0290 - val_loss: 0.1303 - val_mse: 0.0663 - lr: 5.0000e-04\n",
            "Epoch 150/1000\n",
            "2/2 [==============================] - 1s 701ms/step - loss: 0.0430 - mse: 0.0292 - val_loss: 0.1317 - val_mse: 0.0671 - lr: 5.0000e-04\n",
            "Epoch 151/1000\n",
            "2/2 [==============================] - 1s 704ms/step - loss: 0.0427 - mse: 0.0293 - val_loss: 0.1362 - val_mse: 0.0682 - lr: 5.0000e-04\n",
            "Epoch 152/1000\n",
            "2/2 [==============================] - 1s 703ms/step - loss: 0.0450 - mse: 0.0311 - val_loss: 0.1485 - val_mse: 0.0745 - lr: 5.0000e-04\n",
            "Epoch 153/1000\n",
            "2/2 [==============================] - 1s 704ms/step - loss: 0.0496 - mse: 0.0332 - val_loss: 0.1231 - val_mse: 0.0624 - lr: 5.0000e-04\n",
            "Epoch 154/1000\n",
            "2/2 [==============================] - 1s 699ms/step - loss: 0.0477 - mse: 0.0318 - val_loss: 0.1266 - val_mse: 0.0631 - lr: 5.0000e-04\n",
            "Epoch 155/1000\n",
            "2/2 [==============================] - 1s 700ms/step - loss: 0.0449 - mse: 0.0315 - val_loss: 0.1332 - val_mse: 0.0673 - lr: 5.0000e-04\n",
            "Epoch 156/1000\n",
            "2/2 [==============================] - 1s 698ms/step - loss: 0.0453 - mse: 0.0302 - val_loss: 0.1271 - val_mse: 0.0652 - lr: 5.0000e-04\n",
            "Epoch 157/1000\n",
            "2/2 [==============================] - 1s 695ms/step - loss: 0.0435 - mse: 0.0308 - val_loss: 0.1323 - val_mse: 0.0656 - lr: 5.0000e-04\n",
            "Epoch 158/1000\n",
            "2/2 [==============================] - 1s 695ms/step - loss: 0.0435 - mse: 0.0277 - val_loss: 0.1240 - val_mse: 0.0642 - lr: 5.0000e-04\n",
            "Epoch 159/1000\n",
            "2/2 [==============================] - 1s 714ms/step - loss: 0.0432 - mse: 0.0305 - val_loss: 0.1383 - val_mse: 0.0688 - lr: 5.0000e-04\n",
            "Epoch 160/1000\n",
            "2/2 [==============================] - 1s 719ms/step - loss: 0.0431 - mse: 0.0287 - val_loss: 0.1255 - val_mse: 0.0645 - lr: 5.0000e-04\n",
            "Epoch 161/1000\n",
            "2/2 [==============================] - 1s 709ms/step - loss: 0.0419 - mse: 0.0288 - val_loss: 0.1349 - val_mse: 0.0672 - lr: 5.0000e-04\n",
            "Epoch 162/1000\n",
            "2/2 [==============================] - 1s 695ms/step - loss: 0.0413 - mse: 0.0280 - val_loss: 0.1263 - val_mse: 0.0651 - lr: 5.0000e-04\n",
            "Epoch 163/1000\n",
            "2/2 [==============================] - 1s 704ms/step - loss: 0.0413 - mse: 0.0282 - val_loss: 0.1299 - val_mse: 0.0656 - lr: 5.0000e-04\n",
            "Epoch 164/1000\n",
            "2/2 [==============================] - 1s 701ms/step - loss: 0.0403 - mse: 0.0282 - val_loss: 0.1300 - val_mse: 0.0654 - lr: 5.0000e-04\n",
            "Epoch 165/1000\n",
            "2/2 [==============================] - 1s 701ms/step - loss: 0.0397 - mse: 0.0264 - val_loss: 0.1271 - val_mse: 0.0646 - lr: 5.0000e-04\n",
            "Epoch 166/1000\n",
            "2/2 [==============================] - 1s 693ms/step - loss: 0.0396 - mse: 0.0273 - val_loss: 0.1337 - val_mse: 0.0668 - lr: 5.0000e-04\n",
            "Epoch 167/1000\n",
            "2/2 [==============================] - 1s 693ms/step - loss: 0.0394 - mse: 0.0266 - val_loss: 0.1283 - val_mse: 0.0652 - lr: 5.0000e-04\n",
            "Epoch 168/1000\n",
            "2/2 [==============================] - 1s 695ms/step - loss: 0.0394 - mse: 0.0267 - val_loss: 0.1275 - val_mse: 0.0651 - lr: 5.0000e-04\n",
            "Epoch 169/1000\n",
            "2/2 [==============================] - 1s 704ms/step - loss: 0.0385 - mse: 0.0270 - val_loss: 0.1342 - val_mse: 0.0672 - lr: 5.0000e-04\n",
            "Epoch 170/1000\n",
            "2/2 [==============================] - 1s 697ms/step - loss: 0.0386 - mse: 0.0261 - val_loss: 0.1284 - val_mse: 0.0655 - lr: 5.0000e-04\n",
            "Epoch 171/1000\n",
            "2/2 [==============================] - 1s 700ms/step - loss: 0.0383 - mse: 0.0261 - val_loss: 0.1280 - val_mse: 0.0661 - lr: 5.0000e-04\n",
            "Epoch 172/1000\n",
            "2/2 [==============================] - 1s 706ms/step - loss: 0.0376 - mse: 0.0264 - val_loss: 0.1344 - val_mse: 0.0672 - lr: 5.0000e-04\n",
            "Epoch 173/1000\n",
            "2/2 [==============================] - 1s 704ms/step - loss: 0.0374 - mse: 0.0256 - val_loss: 0.1279 - val_mse: 0.0654 - lr: 5.0000e-04\n",
            "Epoch 174/1000\n",
            "2/2 [==============================] - 1s 706ms/step - loss: 0.0374 - mse: 0.0258 - val_loss: 0.1285 - val_mse: 0.0655 - lr: 5.0000e-04\n",
            "Epoch 175/1000\n",
            "2/2 [==============================] - 1s 694ms/step - loss: 0.0367 - mse: 0.0260 - val_loss: 0.1357 - val_mse: 0.0677 - lr: 5.0000e-04\n",
            "Epoch 176/1000\n",
            "2/2 [==============================] - 1s 700ms/step - loss: 0.0374 - mse: 0.0255 - val_loss: 0.1286 - val_mse: 0.0648 - lr: 5.0000e-04\n",
            "Epoch 177/1000\n",
            "2/2 [==============================] - 1s 694ms/step - loss: 0.0370 - mse: 0.0248 - val_loss: 0.1251 - val_mse: 0.0647 - lr: 5.0000e-04\n",
            "Epoch 178/1000\n",
            "2/2 [==============================] - 1s 693ms/step - loss: 0.0392 - mse: 0.0276 - val_loss: 0.1271 - val_mse: 0.0637 - lr: 5.0000e-04\n",
            "Epoch 179/1000\n",
            "2/2 [==============================] - 1s 701ms/step - loss: 0.0365 - mse: 0.0258 - val_loss: 0.1326 - val_mse: 0.0659 - lr: 5.0000e-04\n",
            "Epoch 180/1000\n",
            "2/2 [==============================] - 1s 694ms/step - loss: 0.0364 - mse: 0.0249 - val_loss: 0.1302 - val_mse: 0.0662 - lr: 5.0000e-04\n",
            "Epoch 181/1000\n",
            "2/2 [==============================] - 1s 697ms/step - loss: 0.0353 - mse: 0.0247 - val_loss: 0.1277 - val_mse: 0.0647 - lr: 5.0000e-04\n",
            "Epoch 182/1000\n",
            "2/2 [==============================] - 1s 695ms/step - loss: 0.0352 - mse: 0.0246 - val_loss: 0.1308 - val_mse: 0.0662 - lr: 5.0000e-04\n",
            "Epoch 183/1000\n",
            "2/2 [==============================] - 1s 696ms/step - loss: 0.0346 - mse: 0.0244 - val_loss: 0.1331 - val_mse: 0.0670 - lr: 5.0000e-04\n",
            "Epoch 184/1000\n",
            "2/2 [==============================] - 1s 693ms/step - loss: 0.0342 - mse: 0.0238 - val_loss: 0.1300 - val_mse: 0.0657 - lr: 5.0000e-04\n",
            "Epoch 185/1000\n",
            "2/2 [==============================] - 1s 698ms/step - loss: 0.0339 - mse: 0.0236 - val_loss: 0.1291 - val_mse: 0.0659 - lr: 5.0000e-04\n",
            "Epoch 186/1000\n",
            "2/2 [==============================] - 1s 701ms/step - loss: 0.0337 - mse: 0.0238 - val_loss: 0.1284 - val_mse: 0.0653 - lr: 5.0000e-04\n",
            "Epoch 187/1000\n",
            "2/2 [==============================] - 1s 701ms/step - loss: 0.0334 - mse: 0.0236 - val_loss: 0.1297 - val_mse: 0.0658 - lr: 5.0000e-04\n",
            "Epoch 188/1000\n",
            "2/2 [==============================] - 1s 695ms/step - loss: 0.0330 - mse: 0.0236 - val_loss: 0.1326 - val_mse: 0.0666 - lr: 5.0000e-04\n",
            "Epoch 189/1000\n",
            "2/2 [==============================] - 1s 699ms/step - loss: 0.0333 - mse: 0.0239 - val_loss: 0.1400 - val_mse: 0.0701 - lr: 5.0000e-04\n",
            "Epoch 190/1000\n",
            "2/2 [==============================] - 1s 698ms/step - loss: 0.0355 - mse: 0.0255 - val_loss: 0.1406 - val_mse: 0.0694 - lr: 5.0000e-04\n",
            "Epoch 191/1000\n",
            "2/2 [==============================] - 1s 697ms/step - loss: 0.0349 - mse: 0.0241 - val_loss: 0.1283 - val_mse: 0.0652 - lr: 5.0000e-04\n",
            "Epoch 192/1000\n",
            "2/2 [==============================] - 1s 706ms/step - loss: 0.0339 - mse: 0.0238 - val_loss: 0.1256 - val_mse: 0.0651 - lr: 5.0000e-04\n",
            "Epoch 193/1000\n",
            "2/2 [==============================] - 1s 722ms/step - loss: 0.0335 - mse: 0.0245 - val_loss: 0.1318 - val_mse: 0.0654 - lr: 5.0000e-04\n",
            "Epoch 194/1000\n",
            "2/2 [==============================] - 1s 694ms/step - loss: 0.0327 - mse: 0.0233 - val_loss: 0.1347 - val_mse: 0.0675 - lr: 5.0000e-04\n",
            "Epoch 195/1000\n",
            "2/2 [==============================] - 1s 696ms/step - loss: 0.0320 - mse: 0.0224 - val_loss: 0.1266 - val_mse: 0.0650 - lr: 5.0000e-04\n",
            "Epoch 196/1000\n",
            "2/2 [==============================] - 1s 696ms/step - loss: 0.0331 - mse: 0.0237 - val_loss: 0.1277 - val_mse: 0.0653 - lr: 5.0000e-04\n",
            "Epoch 197/1000\n",
            "2/2 [==============================] - 1s 700ms/step - loss: 0.0313 - mse: 0.0227 - val_loss: 0.1333 - val_mse: 0.0664 - lr: 5.0000e-04\n",
            "Epoch 198/1000\n",
            "2/2 [==============================] - 1s 706ms/step - loss: 0.0309 - mse: 0.0222 - val_loss: 0.1315 - val_mse: 0.0658 - lr: 5.0000e-04\n",
            "Epoch 199/1000\n",
            "2/2 [==============================] - 1s 699ms/step - loss: 0.0304 - mse: 0.0215 - val_loss: 0.1298 - val_mse: 0.0654 - lr: 5.0000e-04\n",
            "Epoch 200/1000\n",
            "2/2 [==============================] - 1s 700ms/step - loss: 0.0301 - mse: 0.0217 - val_loss: 0.1285 - val_mse: 0.0652 - lr: 5.0000e-04\n",
            "Epoch 201/1000\n",
            "2/2 [==============================] - 1s 701ms/step - loss: 0.0300 - mse: 0.0215 - val_loss: 0.1308 - val_mse: 0.0663 - lr: 5.0000e-04\n",
            "Epoch 202/1000\n",
            "2/2 [==============================] - 1s 700ms/step - loss: 0.0296 - mse: 0.0216 - val_loss: 0.1320 - val_mse: 0.0664 - lr: 5.0000e-04\n",
            "Epoch 203/1000\n",
            "2/2 [==============================] - 1s 707ms/step - loss: 0.0293 - mse: 0.0214 - val_loss: 0.1359 - val_mse: 0.0676 - lr: 5.0000e-04\n",
            "Epoch 204/1000\n",
            "2/2 [==============================] - 1s 704ms/step - loss: 0.0301 - mse: 0.0218 - val_loss: 0.1376 - val_mse: 0.0688 - lr: 5.0000e-04\n",
            "Epoch 205/1000\n",
            "2/2 [==============================] - 1s 697ms/step - loss: 0.0295 - mse: 0.0215 - val_loss: 0.1309 - val_mse: 0.0655 - lr: 5.0000e-04\n",
            "Epoch 206/1000\n",
            "2/2 [==============================] - 1s 703ms/step - loss: 0.0287 - mse: 0.0205 - val_loss: 0.1289 - val_mse: 0.0657 - lr: 5.0000e-04\n",
            "Epoch 207/1000\n",
            "2/2 [==============================] - 1s 713ms/step - loss: 0.0307 - mse: 0.0219 - val_loss: 0.1248 - val_mse: 0.0645 - lr: 5.0000e-04\n",
            "Epoch 208/1000\n",
            "2/2 [==============================] - 1s 700ms/step - loss: 0.0321 - mse: 0.0235 - val_loss: 0.1271 - val_mse: 0.0657 - lr: 5.0000e-04\n",
            "Epoch 209/1000\n",
            "2/2 [==============================] - 1s 701ms/step - loss: 0.0321 - mse: 0.0235 - val_loss: 0.1251 - val_mse: 0.0618 - lr: 5.0000e-04\n",
            "Epoch 210/1000\n",
            "2/2 [==============================] - 1s 703ms/step - loss: 0.0309 - mse: 0.0221 - val_loss: 0.1334 - val_mse: 0.0659 - lr: 5.0000e-04\n",
            "Epoch 211/1000\n",
            "2/2 [==============================] - 1s 703ms/step - loss: 0.0295 - mse: 0.0213 - val_loss: 0.1294 - val_mse: 0.0669 - lr: 5.0000e-04\n",
            "Epoch 212/1000\n",
            "2/2 [==============================] - 1s 699ms/step - loss: 0.0296 - mse: 0.0213 - val_loss: 0.1301 - val_mse: 0.0640 - lr: 5.0000e-04\n",
            "Epoch 213/1000\n",
            "2/2 [==============================] - 1s 698ms/step - loss: 0.0290 - mse: 0.0213 - val_loss: 0.1383 - val_mse: 0.0685 - lr: 5.0000e-04\n",
            "Epoch 214/1000\n",
            "2/2 [==============================] - 1s 709ms/step - loss: 0.0290 - mse: 0.0208 - val_loss: 0.1298 - val_mse: 0.0655 - lr: 5.0000e-04\n",
            "Epoch 215/1000\n",
            "2/2 [==============================] - 1s 714ms/step - loss: 0.0287 - mse: 0.0208 - val_loss: 0.1260 - val_mse: 0.0646 - lr: 5.0000e-04\n",
            "Epoch 216/1000\n",
            "2/2 [==============================] - 1s 705ms/step - loss: 0.0290 - mse: 0.0214 - val_loss: 0.1308 - val_mse: 0.0651 - lr: 5.0000e-04\n",
            "Epoch 217/1000\n",
            "2/2 [==============================] - 1s 707ms/step - loss: 0.0275 - mse: 0.0205 - val_loss: 0.1377 - val_mse: 0.0684 - lr: 5.0000e-04\n",
            "Epoch 218/1000\n",
            "2/2 [==============================] - 1s 703ms/step - loss: 0.0278 - mse: 0.0204 - val_loss: 0.1292 - val_mse: 0.0655 - lr: 5.0000e-04\n",
            "Epoch 219/1000\n",
            "2/2 [==============================] - 1s 706ms/step - loss: 0.0278 - mse: 0.0201 - val_loss: 0.1272 - val_mse: 0.0645 - lr: 5.0000e-04\n",
            "Epoch 220/1000\n",
            "2/2 [==============================] - 1s 706ms/step - loss: 0.0270 - mse: 0.0204 - val_loss: 0.1376 - val_mse: 0.0686 - lr: 5.0000e-04\n",
            "Epoch 221/1000\n",
            "2/2 [==============================] - 1s 702ms/step - loss: 0.0276 - mse: 0.0207 - val_loss: 0.1358 - val_mse: 0.0665 - lr: 5.0000e-04\n",
            "Epoch 222/1000\n",
            "2/2 [==============================] - 1s 714ms/step - loss: 0.0267 - mse: 0.0190 - val_loss: 0.1279 - val_mse: 0.0649 - lr: 5.0000e-04\n",
            "Epoch 223/1000\n",
            "2/2 [==============================] - 1s 707ms/step - loss: 0.0272 - mse: 0.0201 - val_loss: 0.1284 - val_mse: 0.0647 - lr: 5.0000e-04\n",
            "Epoch 224/1000\n",
            "2/2 [==============================] - 1s 724ms/step - loss: 0.0259 - mse: 0.0196 - val_loss: 0.1348 - val_mse: 0.0667 - lr: 5.0000e-04\n",
            "Epoch 225/1000\n",
            "2/2 [==============================] - 1s 721ms/step - loss: 0.0261 - mse: 0.0193 - val_loss: 0.1354 - val_mse: 0.0667 - lr: 5.0000e-04\n",
            "Epoch 226/1000\n",
            "2/2 [==============================] - 1s 702ms/step - loss: 0.0254 - mse: 0.0186 - val_loss: 0.1284 - val_mse: 0.0652 - lr: 5.0000e-04\n",
            "Epoch 227/1000\n",
            "2/2 [==============================] - 1s 698ms/step - loss: 0.0258 - mse: 0.0190 - val_loss: 0.1286 - val_mse: 0.0636 - lr: 5.0000e-04\n",
            "Epoch 228/1000\n",
            "2/2 [==============================] - 1s 695ms/step - loss: 0.0248 - mse: 0.0189 - val_loss: 0.1370 - val_mse: 0.0679 - lr: 5.0000e-04\n",
            "Epoch 229/1000\n",
            "2/2 [==============================] - 1s 713ms/step - loss: 0.0251 - mse: 0.0189 - val_loss: 0.1337 - val_mse: 0.0659 - lr: 5.0000e-04\n",
            "Epoch 230/1000\n",
            "2/2 [==============================] - 1s 696ms/step - loss: 0.0242 - mse: 0.0178 - val_loss: 0.1301 - val_mse: 0.0655 - lr: 5.0000e-04\n",
            "Epoch 231/1000\n",
            "2/2 [==============================] - 1s 703ms/step - loss: 0.0244 - mse: 0.0183 - val_loss: 0.1290 - val_mse: 0.0646 - lr: 5.0000e-04\n",
            "Epoch 232/1000\n",
            "2/2 [==============================] - 1s 702ms/step - loss: 0.0240 - mse: 0.0181 - val_loss: 0.1328 - val_mse: 0.0658 - lr: 5.0000e-04\n",
            "Epoch 233/1000\n",
            "2/2 [==============================] - 1s 705ms/step - loss: 0.0238 - mse: 0.0182 - val_loss: 0.1392 - val_mse: 0.0682 - lr: 5.0000e-04\n",
            "Epoch 234/1000\n",
            "2/2 [==============================] - 1s 703ms/step - loss: 0.0251 - mse: 0.0191 - val_loss: 0.1372 - val_mse: 0.0675 - lr: 5.0000e-04\n",
            "Epoch 235/1000\n",
            "2/2 [==============================] - 1s 707ms/step - loss: 0.0238 - mse: 0.0176 - val_loss: 0.1300 - val_mse: 0.0649 - lr: 5.0000e-04\n",
            "Epoch 236/1000\n",
            "2/2 [==============================] - 1s 698ms/step - loss: 0.0240 - mse: 0.0178 - val_loss: 0.1263 - val_mse: 0.0645 - lr: 5.0000e-04\n",
            "Epoch 237/1000\n",
            "2/2 [==============================] - 1s 696ms/step - loss: 0.0249 - mse: 0.0189 - val_loss: 0.1298 - val_mse: 0.0645 - lr: 5.0000e-04\n",
            "Epoch 238/1000\n",
            "2/2 [==============================] - 1s 700ms/step - loss: 0.0232 - mse: 0.0180 - val_loss: 0.1395 - val_mse: 0.0685 - lr: 5.0000e-04\n",
            "Epoch 239/1000\n",
            "2/2 [==============================] - 1s 703ms/step - loss: 0.0248 - mse: 0.0189 - val_loss: 0.1345 - val_mse: 0.0656 - lr: 5.0000e-04\n",
            "Epoch 240/1000\n",
            "2/2 [==============================] - 1s 694ms/step - loss: 0.0230 - mse: 0.0169 - val_loss: 0.1277 - val_mse: 0.0649 - lr: 5.0000e-04\n",
            "Epoch 241/1000\n",
            "2/2 [==============================] - 1s 700ms/step - loss: 0.0249 - mse: 0.0188 - val_loss: 0.1272 - val_mse: 0.0642 - lr: 5.0000e-04\n",
            "Epoch 242/1000\n",
            "2/2 [==============================] - 1s 694ms/step - loss: 0.0230 - mse: 0.0179 - val_loss: 0.1353 - val_mse: 0.0664 - lr: 5.0000e-04\n",
            "Epoch 243/1000\n",
            "2/2 [==============================] - 1s 704ms/step - loss: 0.0227 - mse: 0.0175 - val_loss: 0.1348 - val_mse: 0.0664 - lr: 5.0000e-04\n",
            "Epoch 244/1000\n",
            "2/2 [==============================] - 1s 716ms/step - loss: 0.0222 - mse: 0.0166 - val_loss: 0.1296 - val_mse: 0.0646 - lr: 5.0000e-04\n",
            "Epoch 245/1000\n",
            "2/2 [==============================] - 1s 710ms/step - loss: 0.0226 - mse: 0.0171 - val_loss: 0.1286 - val_mse: 0.0647 - lr: 5.0000e-04\n",
            "Epoch 246/1000\n",
            "2/2 [==============================] - 1s 707ms/step - loss: 0.0220 - mse: 0.0170 - val_loss: 0.1330 - val_mse: 0.0649 - lr: 5.0000e-04\n",
            "Epoch 247/1000\n",
            "2/2 [==============================] - 1s 703ms/step - loss: 0.0220 - mse: 0.0171 - val_loss: 0.1396 - val_mse: 0.0690 - lr: 5.0000e-04\n",
            "Epoch 248/1000\n",
            "2/2 [==============================] - 1s 706ms/step - loss: 0.0231 - mse: 0.0177 - val_loss: 0.1341 - val_mse: 0.0650 - lr: 5.0000e-04\n",
            "Epoch 249/1000\n",
            "2/2 [==============================] - 1s 703ms/step - loss: 0.0219 - mse: 0.0164 - val_loss: 0.1285 - val_mse: 0.0650 - lr: 5.0000e-04\n",
            "Epoch 250/1000\n",
            "2/2 [==============================] - 1s 696ms/step - loss: 0.0223 - mse: 0.0171 - val_loss: 0.1296 - val_mse: 0.0642 - lr: 5.0000e-04\n",
            "Epoch 251/1000\n",
            "2/2 [==============================] - 1s 699ms/step - loss: 0.0213 - mse: 0.0165 - val_loss: 0.1340 - val_mse: 0.0660 - lr: 5.0000e-04\n",
            "Epoch 252/1000\n",
            "2/2 [==============================] - 1s 697ms/step - loss: 0.0210 - mse: 0.0162 - val_loss: 0.1346 - val_mse: 0.0658 - lr: 5.0000e-04\n",
            "Epoch 253/1000\n",
            "2/2 [==============================] - 1s 698ms/step - loss: 0.0209 - mse: 0.0160 - val_loss: 0.1320 - val_mse: 0.0653 - lr: 5.0000e-04\n",
            "Epoch 254/1000\n",
            "2/2 [==============================] - 1s 707ms/step - loss: 0.0206 - mse: 0.0159 - val_loss: 0.1314 - val_mse: 0.0649 - lr: 5.0000e-04\n",
            "Epoch 255/1000\n",
            "2/2 [==============================] - 1s 706ms/step - loss: 0.0206 - mse: 0.0160 - val_loss: 0.1337 - val_mse: 0.0657 - lr: 5.0000e-04\n",
            "Epoch 256/1000\n",
            "2/2 [==============================] - 1s 708ms/step - loss: 0.0205 - mse: 0.0160 - val_loss: 0.1382 - val_mse: 0.0678 - lr: 5.0000e-04\n",
            "Epoch 257/1000\n",
            "2/2 [==============================] - 1s 725ms/step - loss: 0.0213 - mse: 0.0166 - val_loss: 0.1399 - val_mse: 0.0683 - lr: 5.0000e-04\n",
            "Epoch 258/1000\n",
            "2/2 [==============================] - 1s 706ms/step - loss: 0.0212 - mse: 0.0163 - val_loss: 0.1347 - val_mse: 0.0663 - lr: 5.0000e-04\n",
            "Epoch 259/1000\n",
            "2/2 [==============================] - 1s 706ms/step - loss: 0.0201 - mse: 0.0153 - val_loss: 0.1294 - val_mse: 0.0648 - lr: 5.0000e-04\n",
            "Epoch 260/1000\n",
            "2/2 [==============================] - 1s 701ms/step - loss: 0.0212 - mse: 0.0163 - val_loss: 0.1268 - val_mse: 0.0640 - lr: 5.0000e-04\n",
            "Epoch 261/1000\n",
            "2/2 [==============================] - 1s 705ms/step - loss: 0.0212 - mse: 0.0164 - val_loss: 0.1303 - val_mse: 0.0644 - lr: 5.0000e-04\n",
            "Epoch 262/1000\n",
            "2/2 [==============================] - 1s 699ms/step - loss: 0.0199 - mse: 0.0157 - val_loss: 0.1383 - val_mse: 0.0675 - lr: 5.0000e-04\n",
            "Epoch 263/1000\n",
            "2/2 [==============================] - 1s 706ms/step - loss: 0.0209 - mse: 0.0162 - val_loss: 0.1371 - val_mse: 0.0661 - lr: 5.0000e-04\n",
            "Epoch 264/1000\n",
            "2/2 [==============================] - 1s 703ms/step - loss: 0.0205 - mse: 0.0157 - val_loss: 0.1313 - val_mse: 0.0650 - lr: 5.0000e-04\n",
            "Epoch 265/1000\n",
            "2/2 [==============================] - 1s 714ms/step - loss: 0.0197 - mse: 0.0151 - val_loss: 0.1284 - val_mse: 0.0642 - lr: 5.0000e-04\n",
            "Epoch 266/1000\n",
            "2/2 [==============================] - 1s 712ms/step - loss: 0.0203 - mse: 0.0159 - val_loss: 0.1287 - val_mse: 0.0638 - lr: 5.0000e-04\n",
            "Epoch 267/1000\n",
            "2/2 [==============================] - 1s 706ms/step - loss: 0.0197 - mse: 0.0154 - val_loss: 0.1329 - val_mse: 0.0654 - lr: 5.0000e-04\n",
            "Epoch 268/1000\n",
            "2/2 [==============================] - 1s 706ms/step - loss: 0.0195 - mse: 0.0154 - val_loss: 0.1385 - val_mse: 0.0672 - lr: 5.0000e-04\n",
            "Epoch 269/1000\n",
            "2/2 [==============================] - 1s 705ms/step - loss: 0.0201 - mse: 0.0157 - val_loss: 0.1375 - val_mse: 0.0676 - lr: 5.0000e-04\n",
            "Epoch 270/1000\n",
            "2/2 [==============================] - 1s 699ms/step - loss: 0.0198 - mse: 0.0153 - val_loss: 0.1330 - val_mse: 0.0644 - lr: 5.0000e-04\n",
            "Epoch 271/1000\n",
            "2/2 [==============================] - 1s 704ms/step - loss: 0.0191 - mse: 0.0148 - val_loss: 0.1311 - val_mse: 0.0651 - lr: 5.0000e-04\n",
            "Epoch 272/1000\n",
            "2/2 [==============================] - 1s 697ms/step - loss: 0.0190 - mse: 0.0148 - val_loss: 0.1317 - val_mse: 0.0648 - lr: 5.0000e-04\n",
            "Epoch 273/1000\n",
            "2/2 [==============================] - 1s 703ms/step - loss: 0.0188 - mse: 0.0148 - val_loss: 0.1324 - val_mse: 0.0653 - lr: 5.0000e-04\n",
            "Epoch 274/1000\n",
            "2/2 [==============================] - 1s 709ms/step - loss: 0.0185 - mse: 0.0146 - val_loss: 0.1340 - val_mse: 0.0654 - lr: 5.0000e-04\n",
            "Epoch 275/1000\n",
            "2/2 [==============================] - 1s 697ms/step - loss: 0.0185 - mse: 0.0146 - val_loss: 0.1365 - val_mse: 0.0667 - lr: 5.0000e-04\n",
            "Epoch 276/1000\n",
            "2/2 [==============================] - 1s 703ms/step - loss: 0.0187 - mse: 0.0147 - val_loss: 0.1349 - val_mse: 0.0658 - lr: 5.0000e-04\n",
            "Epoch 277/1000\n",
            "2/2 [==============================] - 1s 705ms/step - loss: 0.0183 - mse: 0.0143 - val_loss: 0.1334 - val_mse: 0.0652 - lr: 5.0000e-04\n",
            "Epoch 278/1000\n",
            "2/2 [==============================] - 1s 699ms/step - loss: 0.0180 - mse: 0.0141 - val_loss: 0.1332 - val_mse: 0.0655 - lr: 5.0000e-04\n",
            "Epoch 279/1000\n",
            "2/2 [==============================] - 1s 708ms/step - loss: 0.0178 - mse: 0.0141 - val_loss: 0.1327 - val_mse: 0.0650 - lr: 5.0000e-04\n",
            "Epoch 280/1000\n",
            "2/2 [==============================] - 1s 702ms/step - loss: 0.0177 - mse: 0.0140 - val_loss: 0.1325 - val_mse: 0.0655 - lr: 5.0000e-04\n",
            "Epoch 281/1000\n",
            "2/2 [==============================] - 1s 700ms/step - loss: 0.0177 - mse: 0.0140 - val_loss: 0.1304 - val_mse: 0.0646 - lr: 5.0000e-04\n",
            "Epoch 282/1000\n",
            "2/2 [==============================] - 1s 699ms/step - loss: 0.0184 - mse: 0.0145 - val_loss: 0.1272 - val_mse: 0.0641 - lr: 5.0000e-04\n",
            "Epoch 283/1000\n",
            "2/2 [==============================] - 1s 703ms/step - loss: 0.0214 - mse: 0.0167 - val_loss: 0.1251 - val_mse: 0.0657 - lr: 5.0000e-04\n",
            "Epoch 284/1000\n",
            "2/2 [==============================] - 1s 707ms/step - loss: 0.0274 - mse: 0.0212 - val_loss: 0.1237 - val_mse: 0.0626 - lr: 5.0000e-04\n",
            "Epoch 285/1000\n",
            "2/2 [==============================] - 1s 699ms/step - loss: 0.0228 - mse: 0.0179 - val_loss: 0.1384 - val_mse: 0.0662 - lr: 5.0000e-04\n",
            "Epoch 286/1000\n",
            "2/2 [==============================] - 1s 702ms/step - loss: 0.0215 - mse: 0.0164 - val_loss: 0.1324 - val_mse: 0.0641 - lr: 5.0000e-04\n",
            "Epoch 287/1000\n",
            "2/2 [==============================] - 1s 707ms/step - loss: 0.0198 - mse: 0.0146 - val_loss: 0.1259 - val_mse: 0.0627 - lr: 5.0000e-04\n",
            "Epoch 288/1000\n",
            "2/2 [==============================] - 1s 701ms/step - loss: 0.0200 - mse: 0.0159 - val_loss: 0.1404 - val_mse: 0.0695 - lr: 5.0000e-04\n",
            "Epoch 289/1000\n",
            "2/2 [==============================] - 1s 706ms/step - loss: 0.0202 - mse: 0.0161 - val_loss: 0.1331 - val_mse: 0.0646 - lr: 5.0000e-04\n",
            "Epoch 290/1000\n",
            "2/2 [==============================] - 1s 716ms/step - loss: 0.0190 - mse: 0.0142 - val_loss: 0.1278 - val_mse: 0.0644 - lr: 5.0000e-04\n",
            "Epoch 291/1000\n",
            "2/2 [==============================] - 1s 713ms/step - loss: 0.0189 - mse: 0.0151 - val_loss: 0.1371 - val_mse: 0.0670 - lr: 5.0000e-04\n",
            "Epoch 292/1000\n",
            "2/2 [==============================] - 1s 706ms/step - loss: 0.0185 - mse: 0.0145 - val_loss: 0.1309 - val_mse: 0.0634 - lr: 5.0000e-04\n",
            "Epoch 293/1000\n",
            "2/2 [==============================] - 1s 710ms/step - loss: 0.0185 - mse: 0.0142 - val_loss: 0.1296 - val_mse: 0.0659 - lr: 5.0000e-04\n",
            "Epoch 294/1000\n",
            "2/2 [==============================] - 1s 704ms/step - loss: 0.0184 - mse: 0.0148 - val_loss: 0.1361 - val_mse: 0.0658 - lr: 5.0000e-04\n",
            "Epoch 295/1000\n",
            "2/2 [==============================] - 1s 708ms/step - loss: 0.0184 - mse: 0.0146 - val_loss: 0.1355 - val_mse: 0.0660 - lr: 5.0000e-04\n",
            "Epoch 296/1000\n",
            "2/2 [==============================] - 1s 706ms/step - loss: 0.0175 - mse: 0.0136 - val_loss: 0.1281 - val_mse: 0.0643 - lr: 5.0000e-04\n",
            "Epoch 297/1000\n",
            "2/2 [==============================] - 1s 709ms/step - loss: 0.0179 - mse: 0.0143 - val_loss: 0.1322 - val_mse: 0.0644 - lr: 5.0000e-04\n",
            "Epoch 298/1000\n",
            "2/2 [==============================] - 1s 702ms/step - loss: 0.0173 - mse: 0.0138 - val_loss: 0.1380 - val_mse: 0.0675 - lr: 5.0000e-04\n",
            "Epoch 299/1000\n",
            "2/2 [==============================] - 1s 701ms/step - loss: 0.0172 - mse: 0.0135 - val_loss: 0.1282 - val_mse: 0.0638 - lr: 5.0000e-04\n",
            "Epoch 300/1000\n",
            "2/2 [==============================] - 1s 697ms/step - loss: 0.0178 - mse: 0.0139 - val_loss: 0.1307 - val_mse: 0.0648 - lr: 5.0000e-04\n",
            "Epoch 301/1000\n",
            "2/2 [==============================] - 1s 706ms/step - loss: 0.0167 - mse: 0.0137 - val_loss: 0.1365 - val_mse: 0.0664 - lr: 5.0000e-04\n",
            "Epoch 302/1000\n",
            "2/2 [==============================] - 1s 698ms/step - loss: 0.0170 - mse: 0.0136 - val_loss: 0.1344 - val_mse: 0.0658 - lr: 5.0000e-04\n",
            "Epoch 303/1000\n",
            "2/2 [==============================] - 1s 698ms/step - loss: 0.0164 - mse: 0.0130 - val_loss: 0.1297 - val_mse: 0.0642 - lr: 5.0000e-04\n",
            "Epoch 304/1000\n",
            "2/2 [==============================] - 1s 702ms/step - loss: 0.0166 - mse: 0.0133 - val_loss: 0.1326 - val_mse: 0.0650 - lr: 5.0000e-04\n",
            "Epoch 305/1000\n",
            "2/2 [==============================] - 1s 701ms/step - loss: 0.0162 - mse: 0.0131 - val_loss: 0.1359 - val_mse: 0.0661 - lr: 5.0000e-04\n",
            "Epoch 306/1000\n",
            "2/2 [==============================] - 1s 709ms/step - loss: 0.0163 - mse: 0.0129 - val_loss: 0.1328 - val_mse: 0.0651 - lr: 5.0000e-04\n",
            "Epoch 307/1000\n",
            "2/2 [==============================] - 1s 703ms/step - loss: 0.0161 - mse: 0.0128 - val_loss: 0.1303 - val_mse: 0.0643 - lr: 5.0000e-04\n",
            "Epoch 308/1000\n",
            "2/2 [==============================] - 1s 706ms/step - loss: 0.0161 - mse: 0.0130 - val_loss: 0.1331 - val_mse: 0.0651 - lr: 5.0000e-04\n",
            "Epoch 309/1000\n",
            "2/2 [==============================] - 1s 708ms/step - loss: 0.0158 - mse: 0.0128 - val_loss: 0.1351 - val_mse: 0.0659 - lr: 5.0000e-04\n",
            "Epoch 310/1000\n",
            "2/2 [==============================] - 1s 701ms/step - loss: 0.0158 - mse: 0.0127 - val_loss: 0.1340 - val_mse: 0.0655 - lr: 5.0000e-04\n",
            "Epoch 311/1000\n",
            "2/2 [==============================] - 1s 698ms/step - loss: 0.0156 - mse: 0.0125 - val_loss: 0.1314 - val_mse: 0.0647 - lr: 5.0000e-04\n",
            "Epoch 312/1000\n",
            "2/2 [==============================] - 1s 697ms/step - loss: 0.0158 - mse: 0.0127 - val_loss: 0.1305 - val_mse: 0.0644 - lr: 5.0000e-04\n",
            "Epoch 313/1000\n",
            "2/2 [==============================] - 1s 710ms/step - loss: 0.0159 - mse: 0.0129 - val_loss: 0.1319 - val_mse: 0.0650 - lr: 5.0000e-04\n",
            "Epoch 314/1000\n",
            "2/2 [==============================] - 1s 698ms/step - loss: 0.0155 - mse: 0.0126 - val_loss: 0.1360 - val_mse: 0.0662 - lr: 5.0000e-04\n",
            "Epoch 315/1000\n",
            "2/2 [==============================] - 1s 711ms/step - loss: 0.0156 - mse: 0.0126 - val_loss: 0.1359 - val_mse: 0.0658 - lr: 5.0000e-04\n",
            "Epoch 316/1000\n",
            "2/2 [==============================] - 1s 702ms/step - loss: 0.0156 - mse: 0.0125 - val_loss: 0.1342 - val_mse: 0.0657 - lr: 5.0000e-04\n",
            "Epoch 317/1000\n",
            "2/2 [==============================] - 1s 707ms/step - loss: 0.0153 - mse: 0.0123 - val_loss: 0.1300 - val_mse: 0.0641 - lr: 5.0000e-04\n",
            "Epoch 318/1000\n",
            "2/2 [==============================] - 1s 700ms/step - loss: 0.0162 - mse: 0.0131 - val_loss: 0.1281 - val_mse: 0.0643 - lr: 5.0000e-04\n",
            "Epoch 319/1000\n",
            "2/2 [==============================] - 1s 699ms/step - loss: 0.0179 - mse: 0.0141 - val_loss: 0.1285 - val_mse: 0.0633 - lr: 5.0000e-04\n",
            "Epoch 320/1000\n",
            "2/2 [==============================] - 1s 703ms/step - loss: 0.0158 - mse: 0.0131 - val_loss: 0.1382 - val_mse: 0.0669 - lr: 5.0000e-04\n",
            "Epoch 321/1000\n",
            "2/2 [==============================] - 1s 703ms/step - loss: 0.0167 - mse: 0.0135 - val_loss: 0.1363 - val_mse: 0.0650 - lr: 5.0000e-04\n",
            "Epoch 322/1000\n",
            "2/2 [==============================] - 1s 713ms/step - loss: 0.0162 - mse: 0.0127 - val_loss: 0.1297 - val_mse: 0.0648 - lr: 5.0000e-04\n",
            "Epoch 323/1000\n",
            "2/2 [==============================] - 1s 734ms/step - loss: 0.0169 - mse: 0.0132 - val_loss: 0.1276 - val_mse: 0.0631 - lr: 5.0000e-04\n",
            "Epoch 324/1000\n",
            "2/2 [==============================] - 1s 711ms/step - loss: 0.0161 - mse: 0.0132 - val_loss: 0.1346 - val_mse: 0.0659 - lr: 5.0000e-04\n",
            "Epoch 325/1000\n",
            "2/2 [==============================] - 1s 711ms/step - loss: 0.0155 - mse: 0.0127 - val_loss: 0.1377 - val_mse: 0.0662 - lr: 5.0000e-04\n",
            "Epoch 326/1000\n",
            "2/2 [==============================] - 1s 728ms/step - loss: 0.0156 - mse: 0.0123 - val_loss: 0.1304 - val_mse: 0.0643 - lr: 5.0000e-04\n"
          ]
        }
      ],
      "source": [
        "#early = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\",min_delta=0.001, patience=200, verbose=0, mode=\"auto\", baseline=None, restore_best_weights=True)\n",
        "\n",
        "early = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\",min_delta=0.0005, patience=200, verbose=0, mode=\"auto\", baseline=None, restore_best_weights=True)\n",
        "\n",
        "\n",
        "def scheduler(epoch, lr):\n",
        "  if epoch == 500:\n",
        "    return lr /5\n",
        "  else:\n",
        "    return lr\n",
        "\n",
        "\n",
        "scheduler_cb = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
        "\n",
        "\n",
        "history = model.fit(train_data, train_labels, epochs=1000, validation_data=(validation_data, validation_labels), callbacks=[early, scheduler_cb])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "UgCp0oNUAfzt",
        "outputId": "8ff5113e-46d0-4ecd-a2af-79d4118dffed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['loss', 'mse', 'val_loss', 'val_mse', 'lr'])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "print(history.history.keys())\n",
        "# # summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "#plt.ylim([0,0.2])\n",
        "plt.legend(['train', 'valid'], loc='upper right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def area_hotspot(datapoint):\n",
        "  tinc = 5\n",
        "  tnum = int(4500/tinc)\n",
        "\n",
        "  tinc = 5\n",
        "  tnum = int(4500/tinc)\n",
        "\n",
        "  bins=np.zeros((tnum,2))\n",
        "  bins[:,0] = [ tinc*(0.5 + x) for x in list(range(tnum))]\n",
        "\n",
        "  for bin_index, temp in np.ndenumerate(datapoint):\n",
        "    theta = temp * 1000\n",
        "    tind=math.floor(theta/tinc)\n",
        "    bins[:tind, 1] += 4 # ~ Roughly 2*2*1 nm^3 (volume value from Chunyu)\n",
        "  \n",
        "  return bins\n",
        "\n",
        "def datapoint_results_print(simulation_point, simulation_temperatures):\n",
        "\n",
        "  # Simulation point is the input train_data[<point order>,<dim 0>,<dim 1>,<dim 2>,<channel>]\n",
        "  # For example, simulation_point = train_data[i,:,:,:,:]\n",
        "\n",
        "\n",
        "\n",
        "  fig = make_subplots(rows=3, cols=3, specs=[[{\"type\": \"scatter3d\"},{\"type\": \"scatter3d\"},{\"type\": \"scatter3d\"}],\n",
        "                                             [{\"type\": \"scatter3d\"},{\"type\": \"scatter3d\"},{\"type\": \"scatter\"}],\n",
        "                                             [{\"type\": \"histogram\"},{\"type\": \"histogram\"},{\"type\": \"scatter\"}]],\n",
        "                      subplot_titles=[\"Input 1\",\"Input 2\",\"Input 3\",\n",
        "                                      \"Temp (Labels)\",\"Temp (Predictions)\",\"Parity Plot\",\n",
        "                                      \"Temp (Distributions)\", \"Residuals\", \"Hotspot volume\"], horizontal_spacing = 0.1, vertical_spacing = 0.1)\n",
        "  \n",
        "  fig.update_layout(autosize=False, width=1000, height=1000) \n",
        "\n",
        "  # FIRST PLOT --> INPUT 1\n",
        "\n",
        "  input_1 = simulation_point[:,:,:,0].squeeze()\n",
        "\n",
        "  X,Y,Z = np.mgrid[0:input_1.shape[2], 0:input_1.shape[1], 0:input_1.shape[0]]\n",
        "  input_1_xz = np.swapaxes(input_1, 2, 0)\n",
        "\n",
        "  trace_1 = go.Scatter3d(x = X.flatten(), y = Y.flatten(), z=Z.flatten(),\n",
        "                          mode='markers', marker=dict(colorscale='Viridis', symbol='square', color = input_1_xz.flatten(), colorbar=dict(thickness=20, len=0.3, x=0.27, y=0.9)), showlegend=False)\n",
        "  fig.add_trace(trace_1, row=1, col=1)\n",
        "\n",
        "  # SECOND PLOT --> INPUT 2\n",
        "\n",
        "  input_2 = simulation_point[:,:,:,1].squeeze()\n",
        "\n",
        "  X,Y,Z = np.mgrid[0:input_2.shape[2], 0:input_2.shape[1], 0:input_2.shape[0]]\n",
        "  input_2_xz = np.swapaxes(input_2, 2, 0)\n",
        "\n",
        "  trace_2 = go.Scatter3d(x = X.flatten(), y = Y.flatten(), z=Z.flatten(),\n",
        "                          mode='markers', marker=dict(colorscale='Viridis', symbol='square', color = input_2_xz.flatten(), colorbar=dict(thickness=20, len=0.3, x=0.62, y=0.9)), showlegend=False)\n",
        "  fig.add_trace(trace_2, row=1, col=2)\n",
        "\n",
        "\n",
        "  # THIRD PLOT --> INPUT 3\n",
        "\n",
        "  input_3 = simulation_point[:,:,:,2].squeeze()\n",
        "\n",
        "  X,Y,Z = np.mgrid[0:input_3.shape[2], 0:input_3.shape[1], 0:input_3.shape[0]]\n",
        "  input_3_xz = np.swapaxes(input_3, 2, 0)\n",
        "\n",
        "  trace_3 = go.Scatter3d(x = X.flatten(), y = Y.flatten(), z=Z.flatten(),\n",
        "                          mode='markers', marker=dict(colorscale='Viridis', symbol='square', color = input_3_xz.flatten(), colorbar=dict(thickness=20, len=0.3, x=1, y=0.9)), showlegend=False)\n",
        "  fig.add_trace(trace_3, row=1, col=3)\n",
        "\n",
        "  # FOURTH PLOT --> TEMPS (LABELS)\n",
        "\n",
        "  X,Y,Z = np.mgrid[0:simulation_temperatures.shape[2], 0:simulation_temperatures.shape[1], 0:simulation_temperatures.shape[0]]\n",
        "  simulation_temperatures_xz = np.swapaxes(simulation_temperatures, 2, 0)\n",
        "\n",
        "  trace_4 = go.Scatter3d(x = X.flatten(), y = Y.flatten(), z=Z.flatten(),\n",
        "                          mode='markers', marker=dict(colorscale='Viridis', symbol='square', color = simulation_temperatures_xz.flatten(), colorbar=dict(thickness=20,len=0.3, x=0.27, y=0.5)), showlegend=False)\n",
        "  fig.add_trace(trace_4, row=2, col=1)\n",
        "\n",
        "\n",
        "  # FIFTH PLOT --> TEMPS (PREDICTIONS)\n",
        "\n",
        "  pred = tf.expand_dims(simulation_point, axis=0)\n",
        "  prediction_point = model.predict(pred)\n",
        "  prediction_tensor = prediction_point.squeeze()\n",
        "\n",
        "  X,Y,Z = np.mgrid[0:prediction_tensor.shape[2], 0:prediction_tensor.shape[1], 0:prediction_tensor.shape[0]]\n",
        "  prediction_tensor_xz = np.swapaxes(prediction_tensor, 2, 0)\n",
        "\n",
        "  trace_5 = go.Scatter3d(x = X.flatten(), y = Y.flatten(), z=Z.flatten(),\n",
        "                          mode='markers', marker=dict(colorscale='Viridis', symbol='square', color = prediction_tensor_xz.flatten(), colorbar=dict(thickness=20, len=0.3, x=0.62, y=0.5)), showlegend=False)\n",
        "  fig.add_trace(trace_5, row=2, col=2)\n",
        "\n",
        "  # SIXTH PLOT --> PARITY PLOT\n",
        "\n",
        "  trace_6 = go.Scatter(x=simulation_temperatures.flatten(), y = prediction_tensor.flatten(), mode='markers', showlegend=False)\n",
        "  fig.add_trace(trace_6, row=2, col=3)\n",
        "  fig.update_xaxes(title=\"Scaled Temperature (K) - Labels\", row=2, col=3)\n",
        "  fig.update_yaxes(title=\"Scaled Temperature (K) - Predictions\", row=2, col=3)\n",
        "\n",
        "\n",
        "  # SEVENTH PLOT --> TEMP (Distributions)\n",
        "\n",
        "  trace_7 = go.Histogram(x=prediction_tensor.flatten(), name='Predictions', marker=dict(color='red'), showlegend=True, legendgroup = '1')\n",
        "  trace_7b = go.Histogram(x=simulation_temperatures.flatten(), name='Truth', marker=dict(color='green'), showlegend=True, legendgroup = '1')\n",
        "  fig.update_xaxes(title=\"Temperature (K) - Labels\", row=3, col=1)\n",
        "  fig.add_trace(trace_7, row=3, col=1)\n",
        "  fig.add_trace(trace_7b, row=3, col=1) \n",
        "\n",
        "\n",
        "  # EIGHTH PLOT --> RESIDUALS\n",
        "\n",
        "  diff_xz = prediction_tensor - simulation_temperatures\n",
        "  flat_diff_xz = diff_xz.flatten()\n",
        "\n",
        "  trace_8 = go.Histogram(x=flat_diff_xz, showlegend=False)\n",
        "  trace_line = go.Scatter(x=[0,0], y = [0,700], mode='lines', showlegend=False)\n",
        "\n",
        "  fig.add_trace(trace_8, row=3, col=2)\n",
        "  fig.add_trace(trace_line, row=3, col=2)\n",
        "\n",
        "  # NINTH PLOT ---> VOLUME OF HOTSPOT\n",
        "  \n",
        "  bins_labels = area_hotspot(simulation_temperatures)\n",
        "  bins_predictions = area_hotspot(prediction_tensor)\n",
        "  trace_9 = go.Scatter(x=bins_predictions[:,1], y=bins_predictions[:,0], mode='lines',  name='Predictions', marker=dict(color='red'), showlegend=True, legendgroup = '2')\n",
        "  trace_9b = go.Scatter(x=bins_labels[:,1], y=bins_labels[:,0], mode='lines', name='Truth', marker=dict(color='green'), showlegend=True, legendgroup = '2')\n",
        "  \n",
        "  fig.add_trace(trace_9, row=3, col=3)\n",
        "  fig.add_trace(trace_9b, row=3, col=3) \n",
        "  fig.update_xaxes(type=\"log\", row=3, col=3)\n",
        "  fig.update_xaxes(title=\"Hotspot Volume (nm^3)\", row=3, col=3)\n",
        "  fig.update_yaxes(title=\"Temperature (K)\", row=3, col=3)  \n",
        "\n",
        "  #### \n",
        "\n",
        "  fig.update_layout(autosize=False, width=1400, height=1000, legend_tracegroupgap = 180, legend=dict(font=dict(size=16),orientation=\"h\"))   \n",
        "\n",
        "  return fig, prediction_tensor\n",
        "\n"
      ],
      "metadata": {
        "id": "cVbLrXkBNWDI"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i,title in enumerate(paths):\n",
        "  inppoint = train_data[i,:,:,:,:].squeeze()\n",
        "  inplabel = train_labels[i,:,:,:,:].squeeze()\n",
        "  fig, prediction = datapoint_results_print(inppoint, inplabel)\n",
        "\n",
        "  ### SAVING NUMPY PREDICTIONS\n",
        "  np.save('results/'+str(title)+'.npy', prediction)\n",
        "  fig.write_image('results/'+str(title)+'.pdf')"
      ],
      "metadata": {
        "id": "-PCvavvNP6kj",
        "outputId": "9d527748-704d-4eb0-a2fb-b982b5be40ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 130ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, title in enumerate(validation_paths):\n",
        "  inppoint = validation_data[i,:,:,:,:].squeeze()\n",
        "  inplabel = validation_labels[i,:,:,:,:].squeeze()\n",
        "  fig = datapoint_results_print(inppoint, inplabel)\n",
        "  \n",
        "  ### SAVING NUMPY PREDICTIONS\n",
        "  np.save('results/'+str(title)+'.npy', prediction)\n",
        "  fig.write_image('results/'+str(title)+'.pdf')"
      ],
      "metadata": {
        "id": "ClEFHwOOjvd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cDNLoWw4mN_b"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "pbx-local",
      "language": "python",
      "name": "pbx-local"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15"
    },
    "vscode": {
      "interpreter": {
        "hash": "f806901b044f1f905f6f73d9d34ac86b2be2e087ac41c051b6f31285dd315984"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}